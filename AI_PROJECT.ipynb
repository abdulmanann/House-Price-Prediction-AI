{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ad5KNx11QIpW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "sispu3rORoxB",
        "outputId": "9a0c7e9a-e0f9-448c-a3e8-da1c7fbcfe04"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Property_ID</th>\n",
              "      <th>Property_Name</th>\n",
              "      <th>Property_Type</th>\n",
              "      <th>Property_Purpose</th>\n",
              "      <th>Baths</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>City</th>\n",
              "      <th>Property_Location</th>\n",
              "      <th>Area</th>\n",
              "      <th>Area_Name</th>\n",
              "      <th>Property_Price</th>\n",
              "      <th>Date_Added</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Flat 35683655</td>\n",
              "      <td>On Bed Apartment Possession In 1 Year For Sale...</td>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>Bahria Town, Lahore, Punjab</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Bahria Town</td>\n",
              "      <td>3900000</td>\n",
              "      <td>24/12/2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flat 43095148</td>\n",
              "      <td>Luxury Studio Apartment For Sale On Easy Insta...</td>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>1</td>\n",
              "      <td>-</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>Raiwind Road, Lahore, Punjab</td>\n",
              "      <td>2.2</td>\n",
              "      <td>Raiwind Road</td>\n",
              "      <td>6000000</td>\n",
              "      <td>24/12/2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Flat 43158444</td>\n",
              "      <td>Flat Sized 10 Marla Is Available For sale In A...</td>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>Askari, Lahore, Punjab</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Askari</td>\n",
              "      <td>30000000</td>\n",
              "      <td>28/12/2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Flat 43158439</td>\n",
              "      <td>10 Marla Flat For sale Is Available In Askari ...</td>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>Askari, Lahore, Punjab</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Askari</td>\n",
              "      <td>30000000</td>\n",
              "      <td>28/12/2022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Flat 43158422</td>\n",
              "      <td>Ideally Located Flat For sale In Askari 11 - S...</td>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>Askari, Lahore, Punjab</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Askari</td>\n",
              "      <td>30000000</td>\n",
              "      <td>28/12/2022</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Property_ID                                      Property_Name  \\\n",
              "0  Flat 35683655  On Bed Apartment Possession In 1 Year For Sale...   \n",
              "1  Flat 43095148  Luxury Studio Apartment For Sale On Easy Insta...   \n",
              "2  Flat 43158444  Flat Sized 10 Marla Is Available For sale In A...   \n",
              "3  Flat 43158439  10 Marla Flat For sale Is Available In Askari ...   \n",
              "4  Flat 43158422  Ideally Located Flat For sale In Askari 11 - S...   \n",
              "\n",
              "  Property_Type Property_Purpose Baths Bedrooms    City  \\\n",
              "0          Flat         For Sale     1        1  Lahore   \n",
              "1          Flat         For Sale     1        -  Lahore   \n",
              "2          Flat         For Sale     3        3  Lahore   \n",
              "3          Flat         For Sale     3        3  Lahore   \n",
              "4          Flat         For Sale     3        3  Lahore   \n",
              "\n",
              "              Property_Location  Area     Area_Name  Property_Price  \\\n",
              "0   Bahria Town, Lahore, Punjab   2.1   Bahria Town         3900000   \n",
              "1  Raiwind Road, Lahore, Punjab   2.2  Raiwind Road         6000000   \n",
              "2        Askari, Lahore, Punjab  10.0        Askari        30000000   \n",
              "3        Askari, Lahore, Punjab  10.0        Askari        30000000   \n",
              "4        Askari, Lahore, Punjab  10.0        Askari        30000000   \n",
              "\n",
              "   Date_Added  \n",
              "0  24/12/2022  \n",
              "1  24/12/2022  \n",
              "2  28/12/2022  \n",
              "3  28/12/2022  \n",
              "4  28/12/2022  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Removing the serial number column as the data is already indexed by pandas\n",
        "df = pd.read_csv('final_data.csv')\n",
        "df.drop(df.columns[[0]], axis=1, inplace=True) \n",
        "df.head() #data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iAdTWCdsZpm",
        "outputId": "d2fce8c7-c1f0-420e-ae6e-f536a1b06a90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12725"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZdrFZTCIVJqR"
      },
      "outputs": [],
      "source": [
        "#Changing data types\n",
        "#replace NA or inf with int max value\n",
        "df = df.replace([np.inf, -np.inf], np.iinfo(np.int64).max)\n",
        "df['Property_ID'] = df['Property_ID'].astype(str)\n",
        "df['Property_Name'] = df['Property_Name'].astype(str)\n",
        "df['Property_Type'] = df['Property_Type'].astype(str)\n",
        "df['Property_Purpose'] = df['Property_Purpose'].astype(str)\n",
        "df['Baths'] = df['Baths'].replace('-', 0)\n",
        "df['Bedrooms'] = df['Bedrooms'].replace('-', 0)\n",
        "df['Baths'] = df['Baths'].astype(int)\n",
        "df['Bedrooms'] = df['Bedrooms'].astype(int)\n",
        "df['City'] = df['City'].astype(str)\n",
        "df['Property_Location'] = df['Property_Location'].astype(str)\n",
        "df['Area'] = df['Area'].astype(float)\n",
        "df['Area_Name'] = df['Area_Name'].astype(str)\n",
        "df['Property_Price'] = df['Property_Price'].astype(int)\n",
        "df['Date_Added'] = df['Date_Added'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6enfinpKVHR2",
        "outputId": "b12acded-9f72-4104-c893-0b5f2a4b5e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12725 entries, 0 to 12724\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   Property_ID        12725 non-null  object \n",
            " 1   Property_Name      12725 non-null  object \n",
            " 2   Property_Type      12725 non-null  object \n",
            " 3   Property_Purpose   12725 non-null  object \n",
            " 4   Baths              12725 non-null  int64  \n",
            " 5   Bedrooms           12725 non-null  int64  \n",
            " 6   City               12725 non-null  object \n",
            " 7   Property_Location  12725 non-null  object \n",
            " 8   Area               12725 non-null  float64\n",
            " 9   Area_Name          12725 non-null  object \n",
            " 10  Property_Price     12725 non-null  int64  \n",
            " 11  Date_Added         12725 non-null  object \n",
            "dtypes: float64(1), int64(3), object(8)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Z-3Jj2UgeXxI",
        "outputId": "ec6652ff-06d4-41e1-fffa-89f80ede291f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Baths</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>Area</th>\n",
              "      <th>Property_Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>12725.000000</td>\n",
              "      <td>12725.000000</td>\n",
              "      <td>12725.000000</td>\n",
              "      <td>1.272500e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.618861</td>\n",
              "      <td>3.371788</td>\n",
              "      <td>11.045220</td>\n",
              "      <td>2.458487e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.072626</td>\n",
              "      <td>1.987624</td>\n",
              "      <td>10.806802</td>\n",
              "      <td>5.612490e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.700000</td>\n",
              "      <td>9.900000e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>8.033059</td>\n",
              "      <td>1.000000e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>13.950415</td>\n",
              "      <td>3.000000e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>300.000000</td>\n",
              "      <td>9.000000e+08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Baths      Bedrooms          Area  Property_Price\n",
              "count  12725.000000  12725.000000  12725.000000    1.272500e+04\n",
              "mean       3.618861      3.371788     11.045220    2.458487e+07\n",
              "std        2.072626      1.987624     10.806802    5.612490e+07\n",
              "min        0.000000      0.000000      0.200000    1.000000e+00\n",
              "25%        2.000000      2.000000      4.700000    9.900000e+04\n",
              "50%        4.000000      3.000000      8.033059    1.000000e+06\n",
              "75%        5.000000      5.000000     13.950415    3.000000e+07\n",
              "max       10.000000     16.000000    300.000000    9.000000e+08"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NKlOQXQXel2f"
      },
      "outputs": [],
      "source": [
        "# print missing values\n",
        "\n",
        "features_nan=[feature for feature in df.columns if df[feature].isnull().sum()>1 and df[feature].dtypes=='O']\n",
        "\n",
        "for feature in features_nan:\n",
        "    print(\"{}: {}% missing values\".format(feature,np.round(df[feature].isnull().mean(),4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaO11C9nguMo",
        "outputId": "bce2e431-068b-421d-8081-00fdc0699b77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Property_ID          0\n",
              "Property_Name        0\n",
              "Property_Type        0\n",
              "Property_Purpose     0\n",
              "Baths                0\n",
              "Bedrooms             0\n",
              "City                 0\n",
              "Property_Location    0\n",
              "Area                 0\n",
              "Area_Name            0\n",
              "Property_Price       0\n",
              "Date_Added           0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check if there is any null value in the data \n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "GbFNa4Bxg2Sd",
        "outputId": "b2f50acd-baae-4e1f-b3a1-25835d47e376"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_30997/721799089.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  ax = sns.heatmap(df.corr(),annot = True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'CORRELATION MATRIX')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Checking correlation between the features\n",
        "plt.figure(figsize=(15,5))\n",
        "ax = sns.heatmap(df.corr(),annot = True)\n",
        "ax.set_title('CORRELATION MATRIX', fontsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ERzYPDNwhl01"
      },
      "outputs": [],
      "source": [
        "# remove columns that contribute nothing\n",
        "# Property ID, Property Name are strings they contribute nothing to model\n",
        "# Property Location is redundant \n",
        "# The data is recent, so the Date Added is same year for all the records\n",
        "df = df.drop(['Property_ID','Property_Name','Property_Location','Date_Added'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "QcA_YokCiqxk",
        "outputId": "eeced0aa-ea48-4e7b-8a59-522e7e450509"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Property_Type</th>\n",
              "      <th>Property_Purpose</th>\n",
              "      <th>Baths</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>City</th>\n",
              "      <th>Area</th>\n",
              "      <th>Area_Name</th>\n",
              "      <th>Property_Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Bahria Town</td>\n",
              "      <td>3900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>2.2</td>\n",
              "      <td>Raiwind Road</td>\n",
              "      <td>6000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Askari</td>\n",
              "      <td>30000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Askari</td>\n",
              "      <td>30000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Askari</td>\n",
              "      <td>30000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Property_Type Property_Purpose  Baths  Bedrooms    City  Area     Area_Name  \\\n",
              "0          Flat         For Sale      1         1  Lahore   2.1   Bahria Town   \n",
              "1          Flat         For Sale      1         0  Lahore   2.2  Raiwind Road   \n",
              "2          Flat         For Sale      3         3  Lahore  10.0        Askari   \n",
              "3          Flat         For Sale      3         3  Lahore  10.0        Askari   \n",
              "4          Flat         For Sale      3         3  Lahore  10.0        Askari   \n",
              "\n",
              "   Property_Price  \n",
              "0         3900000  \n",
              "1         6000000  \n",
              "2        30000000  \n",
              "3        30000000  \n",
              "4        30000000  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd--HKbfiy3F",
        "outputId": "f8e88548-58cb-4a64-eae0-500e9134e994"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Flat', 'House'], dtype=object)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Property_Type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYDiD0sDi5nv",
        "outputId": "a97f9dfa-005b-4e80-eee0-de586c00fc7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Lahore', 'Islamabad', 'Karachi'], dtype=object)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['City'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sf1_NSejAQv",
        "outputId": "21310145-4d11-4b61-961b-0d1d41615bf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['For Sale', 'For Rent'], dtype=object)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Property_Purpose'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzvsHu2-jKH_",
        "outputId": "efd8a1e4-8444-4638-c877-2502139ab2be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Properties with 0 bedrooms:  999\n",
            "Percentage of bedrooms with value 0:  7.85\n",
            "\n",
            "Number of Properties with 0 baths:  942\n",
            "Percentage of baths with value 0:  7.3999999999999995\n"
          ]
        }
      ],
      "source": [
        "# Properties with 0 bedrooms and 0 baths\n",
        "c = df.Bedrooms[df.Bedrooms==0].count()\n",
        "print(\"Number of Properties with 0 bedrooms: \", c)\n",
        "print('Percentage of bedrooms with value 0: ', 100*np.round(c/df.shape[0],4))\n",
        "\n",
        "c = df.Baths[df.Baths == 0].count()\n",
        "print(\"\\nNumber of Properties with 0 baths: \", c)\n",
        "print('Percentage of baths with value 0: ', 100*np.round(c/df.shape[0],4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "EixMXe__jUk5",
        "outputId": "f9d4336a-fd98-46d8-b861-3b7e08dbb7f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Bedrooms VS Price')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Plotting relation between bedrooms and price\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "ax = sns.barplot(x=df['Bedrooms'], y=df['Property_Price'])\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "ax.set_title('Bedrooms VS Price', fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "ZHaYp1qGmqS7",
        "outputId": "bed285b5-684e-47fc-a623-79d3487c0c36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Bedrooms VS Area')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Plotting relation between bedrooms and area\n",
        "plt.figure(figsize=(15,6))\n",
        "ax = sns.barplot(x=df['Bedrooms'], y=df['Area'])\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "ax.set_title('Bedrooms VS Area', fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Il72XFKdnCD8"
      },
      "outputs": [],
      "source": [
        "# Since about 7.85% of properties are without entered data for bedrooms and baths\n",
        "# resulting in 0 values in data. By analyzing Bedrooms against Area and Price\n",
        "# and its correlation with both features\n",
        "# We can replace 0 bedrooms with 5 bedrooms \n",
        "\n",
        "df['Bedrooms'].replace(to_replace = 0, value = 5, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiIBbQliwqZU",
        "outputId": "64bee575-5b20-4526-a88a-bb9d785d359a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.Bedrooms[df.Bedrooms==16].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5CHHpOvvyHeu"
      },
      "outputs": [],
      "source": [
        "#Removing row with 16 bedrooms, its an outlier\n",
        "df = df[df.Bedrooms != 16]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "tP6JrEgfoVeM",
        "outputId": "3155a26c-ad98-4fb3-cefa-8d8431a585c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Bedrooms VS Price')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "ax = sns.barplot(x=df['Bedrooms'], y=df['Property_Price'])\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "ax.set_title('Bedrooms VS Price', fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "fLX_oviH1q5d",
        "outputId": "11b4343b-dcb8-4d1a-817b-f53e87909c07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Baths VS Price')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "ax = sns.barplot(x=df['Baths'], y=df['Property_Price'])\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "ax.set_title('Baths VS Price', fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "7QYpNLfM2G6x",
        "outputId": "5ba845e2-573c-425e-d0e7-bacb09c2643e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Baths VS Area')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "ax = sns.barplot(x=df['Baths'], y=df['Property_Price'])\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "ax.set_title('Baths VS Area', fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "82LVnmym126W"
      },
      "outputs": [],
      "source": [
        "# Replacing 0 baths by 5 as Bedrooms and Baths are correlated\n",
        "df['Baths'].replace(to_replace = 0, value = 5, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "Az9oUJdC2RmP",
        "outputId": "62c16fa8-bb48-4774-ebb3-3df8a9cd9464"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Bedrooms VS Price')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABNEAAAIkCAYAAAAwI73uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNcElEQVR4nO3debhVZcE+4OcwHZDRgVkUxBJHBEw+1BxJRCP98jM+h0Q0y4JCUb+gUjQHtNLEckgLzNS0QdHUQCPBVJxAzMpZFFI5HjMZlens3x/l+XniwAY8sA9y39e1r/Z+97vWfjbrSPCw3rXKCoVCIQAAAADAajUodQAAAAAAqO+UaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARSjRAAAAAKAIJRoAAAAAFKFEAwAAAIAilGgAAP/hwAMPTFlZWaljsJGUlZXlwAMPLHUMAKCeU6IBAPXKq6++mrKyslUezZs3zx577JHzzz8/ixYtKnXMzVJVVVW6dOmShg0b5vXXX1/j3Pvvvz9lZWU59NBDq8dWrFiRH//4x+nXr19at26dJk2apGPHjunbt2/OOOOMPPXUU2uV44Ybbljl56NZs2bp0aNHRo4cmbfffvsjfU8AgNo0KnUAAIDadO/ePSeccEKSpFAopLKyMr///e9z3nnnZdKkSXnooYfSsGHDEqfcvDRo0CAnnXRSLrzwwtxwww359re/vdq548ePT5KccsopSZKVK1dm4MCB+cMf/pBOnTrlmGOOSfv27fPuu+9m5syZufLKK9O8efP06tVrrfMccsgh2W+//ZIklZWVmTx5cn74wx/m9ttvz4wZM7L11luv1X6effbZbLHFFmv9uQDA5kmJBgDUSzvuuGPOO++8GmNLly5Nv3798uijj2batGk5+OCDSxNuMzZ06NBcdNFFayzR/vnPf2bixInZaqutctRRRyVJbrnllvzhD3/IYYcdlrvuuiuNGzeusc28efPyxhtvrFOW/v37Z9SoUdWvly9fngEDBuSBBx7Ij370o1V+flanR48e6/S5AMDmyXJOAGCTUV5enoMOOihJal2y99Zbb+WMM87IjjvumPLy8myzzTY5+uij85e//KXW/T300EM54IAD0rx582y99dYZPHhw5s6dW+vc8847L2VlZZk6dWpuuOGG9O7dO1tssUWNa2m99tprOeWUU9K5c+c0adIk2267bU455ZTMmTOn1n2uy/wPrtO2dOnSfOtb38p2222XZs2apU+fPvnDH/6QJJk/f36GDRuWTp06pWnTpunXr18ef/zxVfb14osvZujQoenWrVvKy8uz1VZbpWfPnjn99NNTKBRqzfqBHXbYIQcddFBeeumlTJs2rdY5t9xyS95///2ccMIJKS8vT5JMnz49SfKVr3xllQItSTp06JDevXuv8bOLady4cb7yla8kSZ544okkydSpU1NWVpbzzjsvjzzySA499NC0adOmxjXvVndNtGXLluWHP/xhPvWpT6Vly5Zp0aJFdtlll4wcOTL//Oc/a8xd1589AGDT40w0AGCTsWzZsupSZM8996zx3ssvv5wDDzwwf//733PooYfmqKOOyltvvZXf/va3mTx5cqZMmZK+fftWz58yZUoGDhyYBg0aZPDgwenUqVOmTJmSfffdN1tuueVqM3z/+9/PAw88kCOPPDKHHnpo9ZLSF154Ifvtt18qKyszaNCg7LrrrvnLX/6S8ePH53e/+10eeuihfPKTn6zez7rO/8DgwYPzzDPP5HOf+1zee++93HzzzfnsZz+bhx9+OF/+8pezbNmyHHPMMamsrMxtt92Www47LLNnz07r1q2TJG+88Ub23nvvLF68OEcccUQGDx6cxYsX58UXX8zVV1+dH/zgB2nUaM1/RDzllFPyxz/+MePHj88BBxywyvsTJkyonveBD5ZWvvDCC2vcd135zxtDPPLII7n44otz0EEH5ctf/vJqi80PvPfee/nMZz6Thx9+OJ/4xCcydOjQlJeX58UXX8xPfvKTnHjiidU/J+v6swcAbKIKAAD1yOzZswtJCt27dy+MGTOmMGbMmMK5555b+NrXvlbo3r17oWnTpoXvf//7q2y3zz77FBo2bFiYNGlSjfHnn3++0LJly8Luu+9ePbZy5crCDjvsUCgrKyv86U9/qh6vqqoqHHfccYUkhf/8Y9KYMWMKSQrNmzcv/PnPf17l8w866KBCksJPfvKTGuNXXXVVIUnh4IMP/kjzDzjggEKSwn777VdYtGhR9fhtt91WSFJo06ZN4ZhjjiksX768+r1LL720kKRw2WWXVY9deeWVhSSFK664YpXv8I9//GOVsdq89957hTZt2hS22GKLwoIFC2q89/TTTxeSFPbaa68a4zNmzCg0atSo0KRJk8JXvvKVwl133VV444031urz/tOECRMKSQpjx46tMb58+fLCwQcfXEhSOP/88wuFQqHwwAMPVB/P8ePH17q/JIUDDjigxtiZZ55ZSFL44he/WFixYkWN9959993CwoULq1+vy88eALDpUqIBAPXKByXa6h6f/exnC0899VSNbWbOnFlIUjj55JNr3efIkSMLSQrPPPNMoVAoFKZNm1ZIUhg0aNAqc1999dVCw4YNV1uinXHGGats89prrxWSFHbZZZdCVVVVjfdWrlxZ6NGjRyFJYc6cOes1v1D4/yXatGnTVpnfuHHjQpLCa6+9VuO9OXPmFJIUTjzxxOqxD0q0/yzv1tWwYcMKSQrXXXddjfERI0YUkhSuvvrqVba5+eabC9tss02N47ntttsWTjrppMKTTz651p/9QYl2yCGHVBetw4cPL3ziE58oJCl069atuhD8oETr3bv3avf3nyXa8uXLCy1btiy0bt268M4776wxy7r+7AEAmy7LOQGAemnAgAGZNGlS9et//OMfefjhhzNixIjsu++++eMf/1i9RO7RRx9NklRUVNR6Mfnnnnuu+n932223PP3000mST3/606vM3X777dOlS5e8+uqrtebae++9VxmbNWtWkuSAAw5YZRlhgwYNsv/+++e5557LrFmz0qVLl3We/2H/uYy1QYMGadeuXZYsWZLtttuuxnsdO3ZMkhoX7B80aFBGjx6dYcOGZcqUKTnssMNywAEHZIcddqj1+67Ol770pVx11VUZP358Tj311CT/Wm578803p1mzZjnuuONW2ea4447L5z//+dx///156KGHMmPGjDzyyCO54YYbcuONN+aqq67KaaedttYZpkyZkilTpiT51/XyunbtmpEjR2b06NHZaqutasz91Kc+tdb7fe6557Jw4cL0799/jUt7k3X/2QMANl2bdYn24IMP5vvf/35mzJiRN998M3fccUf1HaTW1uTJkzNmzJj89a9/TdOmTbP//vvnsssuS9euXTdIZgDYXG299db53Oc+ly222CKf+cxn8p3vfCf3339/kuSdd95Jktxzzz255557VruPxYsXJ/nXBfiTpF27drXOa9++/WpLtPbt268ytmDBgtW+l/z/MuuDees6/8NatWq1ylijRo1WO578666VH+jatWseffTRnHfeebn33nvzq1/9Ksm/7lD53e9+N8ccc0ytmf7Tnnvumd69e+fRRx/Ns88+m5133jl33XVX3n777ZxwwgnV12D7T02bNs2gQYMyaNCgJMn777+fH/zgBznnnHMyYsSIHHXUUenQocNaZRg7dmyNu3Ouyep+rWvzwc9H586di85d1589AGDTtVnfnXPx4sXp2bNnrrrqqvXafvbs2TnyyCNz8MEHZ9asWZk8eXLefvvtfP7zn6/jpADABz44++yDuy8m/79Y+tGPfpTCvy5XUetjyJAhSVJd8Lz11lu1fkZFRcVqP/8/zxz78Oevbrt58+bVmLeu8+vabrvtlt/85jd55513Mn369Jx77rmZN29eBg8enIcffnit9/PBjQN+9rOfJan9hgLFNG3aNN/5zney//77Z9myZev0+euituO2Om3atEmSvP7660XnruvPHgCw6dqsS7SBAwfmwgsvzH//93/X+v7SpUtz1llnpXPnzmnevHn69u2bqVOnVr8/Y8aMrFy5MhdeeGG6d++e3r1756yzzsqsWbNq/IsvAFB3/vnPfyZJqqqqqsc+KNamT5++Vvvo2bNnkuRPf/rTKu+99tprmTt37jpl+mCJ5YMPPphCoVDjvUKhkAcffLDGvHWdv6E0btw4//Vf/5Xzzz8/V155ZQqFQu6+++613v64445L06ZNc9NNN+W1117L5MmT071791rv2FlMixYt1nmbDWWnnXZKq1at8sQTT1T/vK3Ouv7sAQCbrs26RCtm+PDhmT59em699db8+c9/zjHHHJPDDjssL774YpKkT58+adCgQSZMmJCVK1dm/vz5+cUvfpH+/funcePGJU4PAB9Pl19+eZJk//33rx7be++907dv3/zyl7/Mbbfdtso2VVVVmTZtWvXr/fbbL926dcvdd9+dhx56qHq8UCjkW9/6VlauXLlOmbbbbrscdNBB+etf/5rx48fXeO+6667Ls88+m4MPPrj6+mbrOr8uzZgxo9Zloh+cFde0adO13lebNm1y9NFHp6KiIscff3xWrlyZk08+udazvm699db88Y9/XKU0TP51XbEHHnggjRo1yn/913+tw7fZMBo1apSvfOUrmT9/fkaMGLHKz8P8+fOzaNGiJOv+swcAbLo262uircmcOXMyYcKEzJkzJ506dUqSnHXWWZk0aVImTJiQiy++ON26dct9992XL3zhC/nKV76SlStXpl+/frn33ntLnB4ANn0vvfRSjQu1v/POO3n44Yczc+bMbLnllrn00ktrzP/lL3+Zgw46KP/7v/+bK664Ir17906zZs0yZ86cTJ8+PZWVlXn//feT/Oti/Nddd10OP/zw9O/fP4MHD06nTp3yxz/+MW+++Wb22GOP/PnPf16nvNdcc03222+/nHrqqfnd736XXXbZJX/9619z1113pW3btrnmmms+0vy68otf/CI/+clPsv/++6d79+5p1apV/va3v+Xee+/NVlttlaFDh67T/k455ZTcfPPNefjhh9OwYcOcdNJJtc579NFHM27cuHTu3Dn7779/tttuuyxbtizPPvts7rvvvlRVVeWSSy5Zq+uQbQzf/e538+ijj+YXv/hFHn300QwcODDl5eV55ZVXMmnSpDz00EPVZwquy88eALDpUqKtxjPPPJOVK1fmk5/8ZI3xpUuXZuutt07yr+uVnHrqqRkyZEiOPfbYLFy4MOeee27+53/+J/fff/86XXsDAKjp5Zdfzvnnn1/9ury8PNtuu22++tWvZtSoUavcibJbt2556qmncvnll2fixImZMGFCGjZsmI4dO2b//ffP//zP/9SY379//0yZMiXf+c538utf/zrNmjXLIYcckl//+tc58cQT1znvTjvtlCeffDLnn39+Jk2alHvuuSdt27bN0KFDM2bMmGy//fYfaX5dOfbYY/P+++/n4YcfzuOPP56lS5dW/7qeffbZq/y6FnPggQeme/fuefnllzNgwIDqf3z8T2eeeWZ23HHH3HfffXniiSdy1113Zfny5enQoUOOPvronHbaaTn44IPr4ivWiaZNm+b+++/Pj3/849x00025/vrr07Bhw2y33XY57bTTatxEal1/9gCATVNZobZz6jdDZWVlNe7Oedttt+X444/PX//61zRs2LDG3BYtWqRDhw4555xzMmnSpBoXNv773/+eLl26ZPr06fViOQIAAAAAH50z0VajV69eWblyZd566618+tOfrnXOkiVL0qBBzcvKfVC4ffhixwAAAABs2jbrGwssWrQos2bNyqxZs5Iks2fPzqxZszJnzpx88pOfzPHHH58TTzwxt99+e2bPnp3HH388Y8eOzT333JMkOeKII/LEE0/ku9/9bl588cXMnDkzQ4cOzfbbb59evXqV8JsBAAAAUJc26+WcU6dOzUEHHbTK+JAhQ3LDDTdk+fLlufDCC3PjjTfm9ddfzzbbbFN9G/jdd989yb/uNPW9730vL7zwQrbYYov069cvl156aXr06LGxvw4AAAAAG8hmXaIBAAAAwNrYrJdzAgAAAMDaUKIBAAAAQBGb3d05q6qq8sYbb6Rly5YpKysrdRwAAAAASqhQKGThwoXp1KlTGjRY/flmm12J9sYbb6RLly6ljgEAAABAPTJ37txsu+22q31/syvRWrZsmeRfvzCtWrUqcRoAAAAASmnBggXp0qVLdWe0OptdifbBEs5WrVop0QAAAABIkqKX/XJjAQAAAAAooqQl2oMPPphBgwalU6dOKSsry8SJE9c4//bbb89nPvOZtG3bNq1atUq/fv0yefLkjRMWAAAAgM1WSUu0xYsXp2fPnrnqqqvWav6DDz6Yz3zmM7n33nszY8aMHHTQQRk0aFCeeuqpDZwUAAAAgM1ZWaFQKJQ6RPKvdad33HFHjjrqqHXabtddd83gwYNz7rnnrtX8BQsWpHXr1pk/f75rogEAAABs5ta2K9qkbyxQVVWVhQsXZquttlrtnKVLl2bp0qXVrxcsWLAxogEAAADwMbJJ31jgBz/4QRYtWpQvfOELq50zduzYtG7duvrRpUuXjZgQAAAAgI+DTbZEu+WWW3L++efnV7/6Vdq1a7faeaNHj878+fOrH3Pnzt2IKQEAAAD4ONgkl3Peeuut+dKXvpRf//rX6d+//xrnlpeXp7y8fCMlAwAAAODjaJM7E+2Xv/xlhg4dml/+8pc54ogjSh0HAAAAgM1ASc9EW7RoUV566aXq17Nnz86sWbOy1VZbZbvttsvo0aPz+uuv58Ybb0zyryWcQ4YMybhx49K3b9/MmzcvSdKsWbO0bt26JN8BAAAAgI+/kp6J9uSTT6ZXr17p1atXkmTkyJHp1atXzj333CTJm2++mTlz5lTPv+6667JixYoMGzYsHTt2rH6MGDGiJPkBAAAA2DyUFQqFQqlDbEwLFixI69atM3/+/LRq1arUcQAAAAAoobXtija5a6IBAAAAwMamRAMAAACAIpRoAAAAAFCEEg0AAAAAilCiAQAAAEARjUodAAAAANi8jRgxIpWVlUmStm3bZty4cSVOBKtSogEAAAAlVVlZmYqKilLHgDWynBMAAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARSjRAAAAAKAIJRoAAAAAFKFEAwAAAIAilGgAAAAAUIQSDQAAAACKUKIBAAAAQBFKNAAAAAAoQokGAAAAAEUo0QAAAACgCCUaAAAAABShRAMAAACAIpRoAAAAAFCEEg0AAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARTQqdQAAAADYkEaMGJHKysokSdu2bTNu3LgSJwI2RUo0AAAAPtYqKytTUVFR6hjAJs5yTgAAAAAoQokGAAAAAEUo0QAAAACgCCUaAAAAABShRAMAAACAIpRoAAAAAFCEEg0AAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARSjRAAAAAKAIJRoAAAAAFFHSEu3BBx/MoEGD0qlTp5SVlWXixIlFt5k6dWp69+6d8vLy7Ljjjrnhhhs2eE4AAAAANm8lLdEWL16cnj175qqrrlqr+bNnz84RRxyRgw46KLNmzcrpp5+eL33pS5k8efIGTgoAAADA5qxRKT984MCBGThw4FrPv/baa9OtW7dcdtllSZKdd945Dz30UH74wx9mwIABtW6zdOnSLF26tPr1ggULPlpoAAAAADY7m9Q10aZPn57+/fvXGBswYECmT5++2m3Gjh2b1q1bVz+6dOmyoWMCAAAA8DGzSZVo8+bNS/v27WuMtW/fPgsWLMh7771X6zajR4/O/Pnzqx9z587dGFEBAAAA+Bgp6XLOjaG8vDzl5eWljgEAAADAJmyTOhOtQ4cOqaioqDFWUVGRVq1apVmzZiVKBQAAAMDH3SZVovXr1y9TpkypMXb//fenX79+JUoEAAAAwOagpCXaokWLMmvWrMyaNStJMnv27MyaNStz5sxJ8q/rmZ144onV80877bS88sor+b//+78899xzufrqq/OrX/0qZ5xxRiniAwAAALCZKGmJ9uSTT6ZXr17p1atXkmTkyJHp1atXzj333CTJm2++WV2oJUm3bt1yzz335P7770/Pnj1z2WWX5ac//WkGDBhQkvwAAAAAbB5KemOBAw88MIVCYbXv33DDDbVu89RTT23AVAAAAABQ0yZ1TTQAAAAAKAUlGgAAAAAUUdLlnAAAAJuyESNGpLKyMknStm3bjBs3rsSJANhQlGgAAADrqbKyMhUVFaWOAcBGYDknAAAAABShRAMAAACAIpRoAAAAAFCEEg0AAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARSjRAAAAAKAIJRoAAAAAFKFEAwAAAIAilGgAAAAAUIQSDQAAAACKUKIBAAAAQBFKNAAAAAAoQokGAAAAAEUo0QAAAACgCCUaAAAAABShRAMAAACAIpRoAAAAAFCEEg0AAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARSjRAAAAAKAIJRoAAAAAFKFEAwAAAIAilGgAAAAAUIQSDQAAAACKUKIBAAAAQBFKNAAAAAAoQokGAAAAAEUo0QAAAACgCCUaAAAAABShRAMAAACAIpRoAAAAAFCEEg0AAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIqoFyXaVVddla5du6Zp06bp27dvHn/88TXOv+KKK7LTTjulWbNm6dKlS84444y8//77GyktAAAAAJubkpdot912W0aOHJkxY8Zk5syZ6dmzZwYMGJC33nqr1vm33HJLRo0alTFjxuTZZ5/Nz372s9x222351re+tZGTAwAAALC5aFTqAJdffnlOPfXUDB06NEly7bXX5p577sn48eMzatSoVeY/8sgj2XfffXPcccclSbp27Zpjjz02jz322EbNDQAAG9KIESNSWVmZJGnbtm3GjRtX4kQAsHkr6Zloy5Yty4wZM9K/f//qsQYNGqR///6ZPn16rdvss88+mTFjRvWSz1deeSX33ntvDj/88FrnL126NAsWLKjxAACA+q6ysjIVFRWpqKioLtMAgNIp6Zlob7/9dlauXJn27dvXGG/fvn2ee+65Wrc57rjj8vbbb2e//fZLoVDIihUrctppp612OefYsWNz/vnn13l2AAAAADYfJb8m2rqaOnVqLr744lx99dWZOXNmbr/99txzzz254IILap0/evTozJ8/v/oxd+7cjZwYAAAAgE1dSc9E22abbdKwYcNUVFTUGK+oqEiHDh1q3eacc87JF7/4xXzpS19Kkuy+++5ZvHhxvvzlL+fb3/52GjSo2QuWl5envLx8w3wBAAAAADYLJT0TrUmTJunTp0+mTJlSPVZVVZUpU6akX79+tW6zZMmSVYqyhg0bJkkKhcKGCwsAAADAZqvkd+ccOXJkhgwZkr322it77713rrjiiixevLj6bp0nnnhiOnfunLFjxyZJBg0alMsvvzy9evVK375989JLL+Wcc87JoEGDqss0AAAAAKhLJS/RBg8enMrKypx77rmZN29e9txzz0yaNKn6ZgNz5sypcebZd77znZSVleU73/lOXn/99bRt2zaDBg3KRRddVKqvAAAAsNn61W/fLnWEohYvqarxvL5n/sLR25Q6AlCLkpdoSTJ8+PAMHz681vemTp1a43WjRo0yZsyYjBkzZiMkAwAAAIBN8O6cAAAAALCxKdEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARSjRAAAAAKAIJRoAAAAAFKFEAwAAAIAilGgAAAAAUIQSDQAAAACKUKIBAAAAQBFKNAAAAAAoQokGAAAAAEUo0QAAAACgiI9coi1btizPP/98VqxYURd5AAAAAKDeWe8SbcmSJTnllFOyxRZbZNddd82cOXOSJF//+tdzySWX1FlAAAAAACi1Ruu74ejRo/P0009n6tSpOeyww6rH+/fvn/POOy+jRo2qk4AAAMDm65jf/qXUEdbo3SXLq59XLlle7/P++ujdSh0BYJO13iXaxIkTc9ttt+W//uu/UlZWVj2+66675uWXX66TcAAAAABQH6z3cs7Kysq0a9dulfHFixfXKNUAAAAAYFO33iXaXnvtlXvuuaf69QfF2U9/+tP069fvoycDAAAAgHpivZdzXnzxxRk4cGD+9re/ZcWKFRk3blz+9re/5ZFHHsm0adPqMiMAAAAAlNR6n4m23377ZdasWVmxYkV233333HfffWnXrl2mT5+ePn361GVGAAAAACip9T4TLUm6d++e66+/vq6yAAAAAEC9tN4l2r333puGDRtmwIABNcYnT56cqqqqDBw48COHAwBgwxgxYkQqKyuTJG3bts24ceNKnAgAoH5b7xJt1KhRueSSS1YZLxQKGTVqlBINAKAeq6ysTEVFRaljALCRPHd1/f49f/nClTWe1/e8SdLja+1LHYGNbL2vifbiiy9ml112WWW8R48eeemllz5SKAAAAACoT9a7RGvdunVeeeWVVcZfeumlNG/e/COFAgAAAID6ZL1LtCOPPDKnn356Xn755eqxl156KWeeeWY+97nP1Uk4AAAAAKgP1rtE+973vpfmzZunR48e6datW7p165add945W2+9dX7wgx/UZUYAAAAAKKn1vrFA69at88gjj+T+++/P008/nWbNmmWPPfbI/vvvX5f5AAAAAKDk1rtES5KysrIceuihOfTQQ+sqDwAAAADUO+tUol155ZX58pe/nKZNm+bKK69c49xvfOMbHykYAAAAANQX61Si/fCHP8zxxx+fpk2b5oc//OFq55WVlSnRAAAAAPjYWKcSbfbs2bU+BwAAAICPs/W6O+fy5cvTvXv3PPvss3WdBwAAAADqnfUq0Ro3bpz333+/rrMAAAAAQL20XiVakgwbNiyXXnppVqxYUZd5AAAAAKDeWadron3YE088kSlTpuS+++7L7rvvnubNm9d4//bbb//I4QAAAACgPljvEq1NmzY5+uij6zILAAAAANRL612iTZgwoS5zAAAAAEC9tc7XRKuqqsqll16afffdN5/61KcyatSovPfeexsiGwAAAADUC+t8JtpFF12U8847L/3790+zZs0ybty4vPXWWxk/fvyGyAcAABvE535zZ6kjrNGSJUuqn7+1ZEm9z5skd/3PkaWOAAAbzDqfiXbjjTfm6quvzuTJkzNx4sT87ne/y80335yqqqoNkQ8AAAAASm6dS7Q5c+bk8MMPr37dv3//lJWV5Y033qjTYAAAAABQX6xzibZixYo0bdq0xljjxo2zfPnyOgsFAAAAAPXJOl8TrVAo5KSTTkp5eXn12Pvvv5/TTjstzZs3rx67/fbb6yYhAAAAAJTYOpdoQ4YMWWXshBNOqJMwAAAAAFAfrXOJNmHChHWa//e//z2dOnVKgwbrvHIUAAAAAOqFdS7R1tUuu+ySWbNmZYcddtjQHwUAUC8c8duflDpCUe8vWVj9vGLJwnqf+Z6jv1LqCADAZm6Dnx5WKBQ29EcAAAAAwAZljSUAAAAAFKFEAwAAAIAilGgAAAAAUMQGL9HKyso29EcAAAAAwAblxgIAAAAAUMR6l2gTJkzIkiVLis7729/+lu233359PwYAAAAASm69S7RRo0alQ4cOOeWUU/LII4+sdl6XLl3SsGHD9f0YAAAAACi59S7RXn/99fz85z/P22+/nQMPPDA9evTIpZdemnnz5tVlPgAAAAAoufUu0Ro1apT//u//zp133pm5c+fm1FNPzc0335ztttsun/vc53LnnXemqqqqLrMCAAAAQEnUyY0F2rdvn/322y/9+vVLgwYN8swzz2TIkCHp3r17pk6dWhcfAQAAAAAl85FKtIqKivzgBz/IrrvumgMPPDALFizI3XffndmzZ+f111/PF77whQwZMqSusgIAAABASax3iTZo0KB06dIlN9xwQ0499dS8/vrr+eUvf5n+/fsnSZo3b54zzzwzc+fOLbqvq666Kl27dk3Tpk3Tt2/fPP7442uc/+6772bYsGHp2LFjysvL88lPfjL33nvv+n4VAAAAAFijRuu7Ybt27TJt2rT069dvtXPatm2b2bNnr3E/t912W0aOHJlrr702ffv2zRVXXJEBAwbk+eefT7t27VaZv2zZsnzmM59Ju3bt8pvf/CadO3fOa6+9ljZt2qzvVwEAAACANVrvM9EOOOCA9O7de5XxZcuW5cYbb0ySlJWVZfvtt1/jfi6//PKceuqpGTp0aHbZZZdce+212WKLLTJ+/Pha548fPz7vvPNOJk6cmH333Tddu3bNAQcckJ49e67vVwEAAACANVrvEm3o0KGZP3/+KuMLFy7M0KFD12ofy5Yty4wZM6qXgCZJgwYN0r9//0yfPr3Wbe66667069cvw4YNS/v27bPbbrvl4osvzsqVK2udv3Tp0ixYsKDGAwAAAADWxXqXaIVCIWVlZauM//3vf0/r1q3Xah9vv/12Vq5cmfbt29cYb9++febNm1frNq+88kp+85vfZOXKlbn33ntzzjnn5LLLLsuFF15Y6/yxY8emdevW1Y8uXbqsVTYAAAAA+MA6XxOtV69eKSsrS1lZWQ455JA0avT/d7Fy5crMnj07hx12WJ2G/LCqqqq0a9cu1113XRo2bJg+ffrk9ddfz/e///2MGTNmlfmjR4/OyJEjq18vWLBAkQYAAADAOlnnEu2oo45KksyaNSsDBgxIixYtqt9r0qRJunbtmqOPPnqt9rXNNtukYcOGqaioqDFeUVGRDh061LpNx44d07hx4zRs2LB6bOedd868efOybNmyNGnSpMb88vLylJeXr1UeAAAAAKjNOpdoY8aMycqVK9O1a9cceuih6dix43p/eJMmTdKnT59MmTKlupyrqqrKlClTMnz48Fq32XfffXPLLbekqqoqDRr8azXqCy+8kI4dO65SoAEAAABAXViva6I1bNgwX/nKV/L+++9/5AAjR47M9ddfn5///Od59tln89WvfjWLFy+uvjnBiSeemNGjR1fP/+pXv5p33nknI0aMyAsvvJB77rknF198cYYNG/aRswAAAABAbdb5TLQP7LbbbnnllVfSrVu3jxRg8ODBqayszLnnnpt58+Zlzz33zKRJk6pvNjBnzpzqM86SpEuXLpk8eXLOOOOM7LHHHuncuXNGjBiRb37zmx8pBwAAAACsznqXaBdeeGHOOuusXHDBBenTp0+aN29e4/1WrVqt9b6GDx++2uWbU6dOXWWsX79+efTRR9cpLwAA/19Z8y1S+NBzAADWbL1LtMMPPzxJ8rnPfS5lZWXV44VCIWVlZVm5cuVHTwcAwAZR/vmDSx0BAGCTst4l2gMPPFCXOQAAAACg3lrvEu2AAw6oyxwAAAAAUG+t1905P/CnP/0pJ5xwQvbZZ5+8/vrrSZJf/OIXeeihh+okHAAAAADUB+tdov32t7/NgAED0qxZs8ycOTNLly5NksyfPz8XX3xxnQUEAAAAgFJb7xLtwgsvzLXXXpvrr78+jRs3rh7fd999M3PmzDoJBwAAAAD1wXqXaM8//3z233//VcZbt26dd99996NkAgAAAIB6Zb1LtA4dOuSll15aZfyhhx7KDjvs8JFCAQAAAEB9st4l2qmnnpoRI0bkscceS1lZWd54443cfPPNOeuss/LVr361LjMCAAAAQEk1Wt8NR40alaqqqhxyyCFZsmRJ9t9//5SXl+ess87K17/+9brMCAAAAAAltd4lWllZWb797W/n7LPPzksvvZRFixZll112SYsWLeoyHwAAAACU3HqXaB9o0qRJWrZsmZYtWyrQAAAAqHdaNN+q1ucA62K9S7QVK1bk/PPPz5VXXplFixYlSVq0aJGvf/3rGTNmTBo3blxnIQEAAGB9Hfn575Y6AvAxsN4l2te//vXcfvvt+d73vpd+/folSaZPn57zzjsv//jHP3LNNdfUWUgAAAAAKKX1LtFuueWW3HrrrRk4cGD12B577JEuXbrk2GOPVaIBAAAA8LHRYH03LC8vT9euXVcZ79atW5o0afJRMgEAAABAvbLeJdrw4cNzwQUXZOnSpdVjS5cuzUUXXZThw4fXSTgAAAAAqA/WeznnU089lSlTpmTbbbdNz549kyRPP/10li1blkMOOSSf//znq+fefvvtHz0pAAAAAJTIepdobdq0ydFHH11jrEuXLh85EAAAkJQ1b1HrcwCgNNa7RJswYUJd5gAAAD6k2X8fU+oIAMCHrHeJ9oHKyso8//zzSZKddtopbdu2/cihAAAAAKA+We8bCyxevDgnn3xyOnbsmP333z/7779/OnXqlFNOOSVLliypy4wAAAAAUFLrXaKNHDky06ZNy+9+97u8++67effdd3PnnXdm2rRpOfPMM+syIwAAAACU1Hov5/ztb3+b3/zmNznwwAOrxw4//PA0a9YsX/jCF3LNNdfURT4AAAAAKLn1LtGWLFmS9u3brzLerl07yzkBAIDNQoPmrWt9DsDHz3qXaP369cuYMWNy4403pmnTpkmS9957L+eff3769etXZwEBAADqq1afP73UEQDYSNa7RLviiity2GGHZdttt03Pnj2TJE8//XSaNm2ayZMn11lAAAAAACi19S7Rdt9997z44ou5+eab89xzzyVJjj322Bx//PFp1qxZnQUEAAAAgFJbrxJt+fLl6dGjR+6+++6ceuqpdZ0JAAAAAOqVBuuzUePGjfP+++/XdRYAAAAAqJfWq0RLkmHDhuXSSy/NihUr6jIPAAAAANQ7631NtCeeeCJTpkzJfffdl9133z3Nmzev8f7tt9/+kcMBAAAAQH2w3iVamzZtcvTRR9dlFgAAAACol9a5RKuqqsr3v//9vPDCC1m2bFkOPvjgnHfeee7ICQAAAMDH1jpfE+2iiy7Kt771rbRo0SKdO3fOlVdemWHDhm2IbAAAAABQL6xziXbjjTfm6quvzuTJkzNx4sT87ne/y80335yqqqoNkQ8AAAAASm6dS7Q5c+bk8MMPr37dv3//lJWV5Y033qjTYAAAAABQX6xzibZixYo0bdq0xljjxo2zfPnyOgsFAAAAAPXJOt9YoFAo5KSTTkp5eXn12Pvvv5/TTjstzZs3rx67/fbb6yYhAAAAAJTYOpdoQ4YMWWXshBNOqJMwAAAAAFAfrXOJNmHChA2RAwAAAADqrXW+JhoAAAAAbG6UaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARSjRAAAAAKAIJRoAAAAAFKFEAwAAAIAilGgAAAAAUIQSDQAAAACKUKIBAAAAQBFKNAAAAAAoQokGAAAAAEUo0QAAAACgCCUaAAAAABShRAMAAACAIpRoAAAAAFCEEg0AAAAAimhU6gAAAAAA1H8jRoxIZWVlkqRt27YZN25ciRNtXEo0AAAAAIqqrKxMRUVFqWOUjOWcAAAAAFCEEg0AAAAAilCiAQAAAEAR9aJEu+qqq9K1a9c0bdo0ffv2zeOPP75W2916660pKyvLUUcdtWEDAgAAALBZK3mJdtttt2XkyJEZM2ZMZs6cmZ49e2bAgAF566231rjdq6++mrPOOiuf/vSnN1JSAAAAADZXJS/RLr/88px66qkZOnRodtlll1x77bXZYostMn78+NVus3Llyhx//PE5//zzs8MOO6xx/0uXLs2CBQtqPAAAAABgXZS0RFu2bFlmzJiR/v37V481aNAg/fv3z/Tp01e73Xe/+920a9cup5xyStHPGDt2bFq3bl396NKlS51kBwAAAGDzUdIS7e23387KlSvTvn37GuPt27fPvHnzat3moYceys9+9rNcf/31a/UZo0ePzvz586sfc+fO/ci5AQAAANi8NCp1gHWxcOHCfPGLX8z111+fbbbZZq22KS8vT3l5+QZOBgAAAMDHWUlLtG222SYNGzZMRUVFjfGKiop06NBhlfkvv/xyXn311QwaNKh6rKqqKknSqFGjPP/88+nevfuGDQ0AAADAZqekyzmbNGmSPn36ZMqUKdVjVVVVmTJlSvr167fK/B49euSZZ57JrFmzqh+f+9znctBBB2XWrFmudwYAAADABlHy5ZwjR47MkCFDstdee2XvvffOFVdckcWLF2fo0KFJkhNPPDGdO3fO2LFj07Rp0+y22241tm/Tpk2SrDIOAAAAAHWl5CXa4MGDU1lZmXPPPTfz5s3LnnvumUmTJlXfbGDOnDlp0KCkJ8wBAAAAsJkreYmWJMOHD8/w4cNrfW/q1Klr3PaGG26o+0AAAAAA8CFO8QIAAACAIpRoAAAAAFCEEg0AAAAAiqgX10QDAD4+RowYkcrKyiRJ27ZtM27cuBInAgCAj06JBgDUqcrKylRUVJQ6BgAA1CnLOQEAAACgCCUaAAAAABShRAMAAACAIpRoAAAAAFCEEg0AAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFNCp1AABg3Qy887RSR1ijZUv+Uf28Ysk/6n3e3x95bakjAACwCXAmGgAAAAAUoUQDAAAAgCIs5wQAAABKqk2zrWp9DvWJEg0AAAAoqTMPPK/UEaAoyzkBAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBsLAAB1qqxFoxQ+9BwAAD4O/MkWAKhTjY/pUuoIAABQ5yznBAAAAIAilGgAAAAAUIQSDQAAAACKUKIBAAAAQBFKNAAAAAAoQokGAAAAAEUo0QAAAACgCCUaAAAAABShRAMAAACAIpRoAAAAAFCEEg0AAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARSjRAAAAAKAIJRoAAAAAFKFEAwAAAIAilGgAAAAAUIQSDQAAAACKUKIBAAAAQBFKNAAAAAAoQokGAAAAAEUo0QAAAACgCCUaAAAAABShRAMAAACAIpRoAAAAAFBEo1IHAIC1NWLEiFRWViZJ2rZtm3HjxpU4EQAAsLlQogGwyaisrExFRUWpYwAAAJshyzkBAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQRL0o0a666qp07do1TZs2Td++ffP444+vdu7111+fT3/609lyyy2z5ZZbpn///mucDwAAAAAfVclLtNtuuy0jR47MmDFjMnPmzPTs2TMDBgzIW2+9Vev8qVOn5thjj80DDzyQ6dOnp0uXLjn00EPz+uuvb+TkAAAAAGwuSl6iXX755Tn11FMzdOjQ7LLLLrn22muzxRZbZPz48bXOv/nmm/O1r30te+65Z3r06JGf/vSnqaqqypQpUzZycgAAAAA2F41K+eHLli3LjBkzMnr06OqxBg0apH///pk+ffpa7WPJkiVZvnx5ttpqq1rfX7p0aZYuXVr9esGCBR8tNMDH2A9vGVDqCGu0YPGKDz2vqPd5k+SM4yaXOgIAAFAHSnom2ttvv52VK1emffv2Ncbbt2+fefPmrdU+vvnNb6ZTp07p379/re+PHTs2rVu3rn506dLlI+cGAAAAYPNS8uWcH8Ull1ySW2+9NXfccUeaNm1a65zRo0dn/vz51Y+5c+du5JQAAAAAbOpKupxzm222ScOGDVNRUVFjvKKiIh06dFjjtj/4wQ9yySWX5A9/+EP22GOP1c4rLy9PeXl5neQFAAAA2FAqrni81BHWaOWCpTWe1/e87U/fu073V9Iz0Zo0aZI+ffrUuCnABzcJ6Nev32q3+973vpcLLrggkyZNyl577bUxogIAAACwGSvpmWhJMnLkyAwZMiR77bVX9t5771xxxRVZvHhxhg4dmiQ58cQT07lz54wdOzZJcumll+bcc8/NLbfckq5du1ZfO61FixZp0aJFyb4HAAAAAB9fJS/RBg8enMrKypx77rmZN29e9txzz0yaNKn6ZgNz5sxJgwb//4S5a665JsuWLcv//M//1NjPmDFjct55523M6AAAAABsJkpeoiXJ8OHDM3z48Frfmzp1ao3Xr7766oYPBEC91LR57c8BAAA2tHpRogHA2tj3s/5vCwAAKI2S3lgAAAAAADYFSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIpQogEAAABAEUo0AAAAAChCiQYAAAAARSjRAAAAAKAIJRoAAAAAFKFEAwAAAIAilGgAAAAAUIQSDQAAAACKUKIBAAAAQBGNSh0AoL4YMWJEKisrkyRt27bNuHHjSpwIAACA+kKJBvBvlZWVqaioKHUMAAAA6iHLOQEAAACgCCUaAAAAABRhOSewUdw9fmCpIxT13qJlH3peUe8zf/bk35c6AgAAwGbDmWgAAAAAUIQSDQAAAACKsJwT4N9abFGWpPCh5wAAAPAvSjSAfxt8aONSRwAAAKCespwTAAAAAIpQogEAAABAEUo0AAAAACjCNdFgIxgxYkQqKyuTJG3bts24ceNKnAgAAABYF0o02AgqKytTUVFR6hgAAADAerKcEwAAAACKUKIBAAAAQBFKNAAAAAAowjXR+Fh4+UdHljrCGq1YsPhDz9+q93m7f/3OUkcAAACAesWZaAAAAABQhBINAAAAAIqwnBM2gjbNymp9DgAAAGwalGiwEXxz/y1KHQEAAAD4CCznBAAAAIAilGgAAAAAUIQSDQAAAACKUKIBAAAAQBFuLLCJGzFiRCorK5Mkbdu2zbhx40qcCAAAAODjR4m2iausrExFRUWpYwAAAAB8rFnOCQAAAABFKNEAAAAAoAjLOYuovOamUkdYo5ULF9d4Xt/zJknbr55Q6ggAAAAA68SZaAAAAABQhBINAAAAAIqwnHMTt3WzLWp9DgAAAEDdUaJt4s4/6IhSRwAAAAD42LOcEwAAAACKUKIBAAAAQBGWcwIAAABQ1FZNW9f6fHOhRAMAAACgqPP2O63UEUrKck4AAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAAABQhBINAAAAAIpQogEAAABAEfWiRLvqqqvStWvXNG3aNH379s3jjz++xvm//vWv06NHjzRt2jS777577r333o2UFAAAAIDNUclLtNtuuy0jR47MmDFjMnPmzPTs2TMDBgzIW2+9Vev8Rx55JMcee2xOOeWUPPXUUznqqKNy1FFH5S9/+ctGTg4AAADA5qLkJdrll1+eU089NUOHDs0uu+ySa6+9NltssUXGjx9f6/xx48blsMMOy9lnn52dd945F1xwQXr37p0f//jHGzk5AAAAAJuLRqX88GXLlmXGjBkZPXp09ViDBg3Sv3//TJ8+vdZtpk+fnpEjR9YYGzBgQCZOnFjr/KVLl2bp0qXVr+fPn58kWbBgwVplXPjee2s1j7VXvpa/9uti4XvL63yfm7O1/e9jXSx5b0Wd73NztyGO0/tLHKe6tiGO04oly+p8n5uzDXGMli/x54e6tmGO05I63+fmbsMcp0V1vs/N2Qb5c96ShXW+z83dggVN6nyfi95znOraggXN6nyfC9/3e15daraWv+d98HtjoVBY47ySlmhvv/12Vq5cmfbt29cYb9++fZ577rlat5k3b16t8+fNm1fr/LFjx+b8889fZbxLly7rmZqP7MwvlzoBxXyzdakTsDaGO06bgm+d6jjVd60zodQRWAutc0apI7AW/I5X/zlGm4ahpQ7A2jmr1AEoanTxKR+2cOHCtG69+t8pS1qibQyjR4+uceZaVVVV3nnnnWy99dYpKysrYbK6s2DBgnTp0iVz585Nq1atSh2H1XCcNg2OU/3nGG0aHKdNg+NU/zlGmwbHadPgONV/jtGm4eN4nAqFQhYuXJhOnTqtcV5JS7RtttkmDRs2TEVFRY3xioqKdOjQodZtOnTosE7zy8vLU15eXmOsTZs26x+6HmvVqtXH5gf448xx2jQ4TvWfY7RpcJw2DY5T/ecYbRocp02D41T/OUabho/bcVrTGWgfKOmNBZo0aZI+ffpkypQp1WNVVVWZMmVK+vXrV+s2/fr1qzE/Se6///7VzgcAAACAj6rkyzlHjhyZIUOGZK+99sree++dK664IosXL87Qof9aBX7iiSemc+fOGTt2bJJkxIgROeCAA3LZZZfliCOOyK233ponn3wy1113XSm/BgAAAAAfYyUv0QYPHpzKysqce+65mTdvXvbcc89MmjSp+uYBc+bMSYMG//+EuX322Se33HJLvvOd7+Rb3/pWPvGJT2TixInZbbfdSvUVSq68vDxjxoxZZdkq9YvjtGlwnOo/x2jT4DhtGhyn+s8x2jQ4TpsGx6n+c4w2DZvzcSorFLt/JwAAAABs5kp6TTQAAAAA2BQo0QAAAACgCCUaAAAAABShRAMAAACAIpRom7AHH3wwgwYNSqdOnVJWVpaJEyeWOhK1GDt2bD71qU+lZcuWadeuXY466qg8//zzpY7Fh1xzzTXZY4890qpVq7Rq1Sr9+vXL73//+1LHoohLLrkkZWVlOf3000sdhQ8577zzUlZWVuPRo0ePUsfiP7z++us54YQTsvXWW6dZs2bZfffd8+STT5Y6Fh/StWvXVf5bKisry7Bhw0odjQ9ZuXJlzjnnnHTr1i3NmjVL9+7dc8EFF8S92+qXhQsX5vTTT8/222+fZs2aZZ999skTTzxR6libtWJ/ly0UCjn33HPTsWPHNGvWLP3798+LL75YmrCbsWLH6fbbb8+hhx6arbfeOmVlZZk1a1ZJcm5MSrRN2OLFi9OzZ89cddVVpY7CGkybNi3Dhg3Lo48+mvvvvz/Lly/PoYcemsWLF5c6Gv+27bbb5pJLLsmMGTPy5JNP5uCDD86RRx6Zv/71r6WOxmo88cQT+clPfpI99tij1FGoxa677po333yz+vHQQw+VOhIf8s9//jP77rtvGjdunN///vf529/+lssuuyxbbrllqaPxIU888USN/47uv//+JMkxxxxT4mR82KWXXpprrrkmP/7xj/Pss8/m0ksvzfe+97386Ec/KnU0PuRLX/pS7r///vziF7/IM888k0MPPTT9+/fP66+/Xupom61if5f93ve+lyuvvDLXXnttHnvssTRv3jwDBgzI+++/v5GTbt6KHafFixdnv/32y6WXXrqRk5VOWcE/k3wslJWV5Y477shRRx1V6igUUVlZmXbt2mXatGnZf//9Sx2H1dhqq63y/e9/P6ecckqpo/AfFi1alN69e+fqq6/OhRdemD333DNXXHFFqWPxb+edd14mTpy4WfxL5KZq1KhRefjhh/OnP/2p1FFYB6effnruvvvuvPjiiykrKyt1HP7ts5/9bNq3b5+f/exn1WNHH310mjVrlptuuqmEyfjAe++9l5YtW+bOO+/MEUccUT3ep0+fDBw4MBdeeGEJ05Gs+nfZQqGQTp065cwzz8xZZ52VJJk/f37at2+fG264If/7v/9bwrSbrzV1Dq+++mq6deuWp556KnvuuedGz7YxORMNNrL58+cn+VdJQ/2zcuXK3HrrrVm8eHH69etX6jjUYtiwYTniiCPSv3//UkdhNV588cV06tQpO+ywQ44//vjMmTOn1JH4kLvuuit77bVXjjnmmLRr1y69evXK9ddfX+pYrMGyZcty00035eSTT1ag1TP77LNPpkyZkhdeeCFJ8vTTT+ehhx7KwIEDS5yMD6xYsSIrV65M06ZNa4w3a9bMmdL11OzZszNv3rwaf9Zr3bp1+vbtm+nTp5cwGSSNSh0ANidVVVU5/fTTs++++2a33XYrdRw+5Jlnnkm/fv3y/vvvp0WLFrnjjjuyyy67lDoW/+HWW2/NzJkzXcekHuvbt29uuOGG7LTTTnnzzTdz/vnn59Of/nT+8pe/pGXLlqWOR5JXXnkl11xzTUaOHJlvfetbeeKJJ/KNb3wjTZo0yZAhQ0odj1pMnDgx7777bk466aRSR+E/jBo1KgsWLEiPHj3SsGHDrFy5MhdddFGOP/74Ukfj31q2bJl+/frlggsuyM4775z27dvnl7/8ZaZPn54dd9yx1PGoxbx585Ik7du3rzHevn376vegVJRosBENGzYsf/nLX/yrVz200047ZdasWZk/f35+85vfZMiQIZk2bZoirR6ZO3duRowYkfvvv3+Vf02m/vjw2Rd77LFH+vbtm+233z6/+tWvLI+uJ6qqqrLXXnvl4osvTpL06tUrf/nLX3Lttdcq0eqpn/3sZxk4cGA6depU6ij8h1/96le5+eabc8stt2TXXXfNrFmzcvrpp6dTp07+e6pHfvGLX+Tkk09O586d07Bhw/Tu3TvHHntsZsyYUepowCbGck7YSIYPH5677747DzzwQLbddttSx+E/NGnSJDvuuGP69OmTsWPHpmfPnhk3blypY/EhM2bMyFtvvZXevXunUaNGadSoUaZNm5Yrr7wyjRo1ysqVK0sdkVq0adMmn/zkJ/PSSy+VOgr/1rFjx1X+gWDnnXe27Laeeu211/KHP/whX/rSl0odhVqcffbZGTVqVP73f/83u+++e774xS/mjDPOyNixY0sdjQ/p3r17pk2blkWLFmXu3Ll5/PHHs3z58uywww6ljkYtOnTokCSpqKioMV5RUVH9HpSKEg02sEKhkOHDh+eOO+7IH//4x3Tr1q3UkVgLVVVVWbp0aalj8CGHHHJInnnmmcyaNav6sddee+X444/PrFmz0rBhw1JHpBaLFi3Kyy+/nI4dO5Y6Cv+277775vnnn68x9sILL2T77bcvUSLWZMKECWnXrl2NC6JTfyxZsiQNGtT8K1XDhg1TVVVVokSsSfPmzdOxY8f885//zOTJk3PkkUeWOhK16NatWzp06JApU6ZUjy1YsCCPPfaYaxZTcpZzbsIWLVpU41/2Z8+enVmzZmWrrbbKdtttV8JkfNiwYcNyyy235M4770zLli2r1/G3bt06zZo1K3E6kmT06NEZOHBgtttuuyxcuDC33HJLpk6dmsmTJ5c6Gh/SsmXLVa4l2Lx582y99dauMViPnHXWWRk0aFC23377vPHGGxkzZkwaNmyYY489ttTR+Lczzjgj++yzTy6++OJ84QtfyOOPP57rrrsu1113Xamj8R+qqqoyYcKEDBkyJI0a+WN7fTRo0KBcdNFF2W677bLrrrvmqaeeyuWXX56TTz651NH4kMmTJ6dQKGSnnXbKSy+9lLPPPjs9evTI0KFDSx1ts1Xs77Knn356LrzwwnziE59It27dcs4556RTp0613hmSDafYcXrnnXcyZ86cvPHGG0lS/Y90HTp0+PieNVhgk/XAAw8UkqzyGDJkSKmj8SG1HaMkhQkTJpQ6Gv928sknF7bffvtCkyZNCm3bti0ccsghhfvuu6/UsVgLBxxwQGHEiBGljsGHDB48uNCxY8dCkyZNCp07dy4MHjy48NJLL5U6Fv/hd7/7XWG33XYrlJeXF3r06FG47rrrSh2JWkyePLmQpPD888+XOgqrsWDBgsKIESMK2223XaFp06aFHXbYofDtb3+7sHTp0lJH40Nuu+22wg477FBo0qRJoUOHDoVhw4YV3n333VLH2qwV+7tsVVVV4Zxzzim0b9++UF5eXjjkkEP8XlgCxY7ThAkTan1/zJgxJc29IZUVCoXCRurrAAAAAGCT5JpoAAAAAFCEEg0AAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCKUaAAAHyMnnXRSjjrqqFLHAAD42FGiAQCUwEknnZSysrLqx9Zbb53DDjssf/7zn0sdDQCAWijRAABK5LDDDsubb76ZN998M1OmTEmjRo3y2c9+doN+5rJlyzbo/gEAPq6UaAAAJVJeXp4OHTqkQ4cO2XPPPTNq1KjMnTs3lZWVSZK5c+fmC1/4Qtq0aZOtttoqRx55ZF599dXq7VeuXJmRI0emTZs22XrrrfN///d/KRQKNT7jwAMPzPDhw3P66adnm222yYABA5Ik06ZNy957753y8vJ07Ngxo0aNyooVK6q3W7p0ab7xjW+kXbt2adq0afbbb7888cQT1e9PnTo1ZWVlmTx5cnr16pVmzZrl4IMPzltvvZXf//732XnnndOqVascd9xxWbJkSfV2v/nNb7L77runWbNm2XrrrdO/f/8sXrx4Q/zyAgDUKSUaAEA9sGjRotx0003Zcccds/XWW2f58uUZMGBAWrZsmT/96U95+OGH06JFixx22GHVZ5NddtllueGGGzJ+/Pg89NBDeeedd3LHHXessu+f//znadKkSR5++OFce+21ef3113P44YfnU5/6VJ5++ulcc801+dnPfpYLL7ywepv/+7//y29/+9v8/Oc/z8yZM7PjjjtmwIABeeedd2rs+7zzzsuPf/zjPPLII9Wl3xVXXJFbbrkl99xzT+6777786Ec/SpK8+eabOfbYY3PyySfn2WefzdSpU/P5z39+leIPAKA+Kiv4UwsAwEZ30kkn5aabbkrTpk2TJIsXL07Hjh1z9913p3fv3rnpppty4YUX5tlnn01ZWVmSfy3FbNOmTSZOnJhDDz00nTp1yhlnnJGzzz47SbJixYp069Ytffr0ycSJE5P860y0BQsWZObMmdWf/e1vfzu//e1va+z76quvzje/+c3Mnz8/7733XrbccsvccMMNOe6445Iky5cvT9euXXP66afn7LPPztSpU3PQQQflD3/4Qw455JAkySWXXJLRo0fn5Zdfzg477JAkOe200/Lqq69m0qRJmTlzZvr06ZNXX30122+//Yb/RQYAqEPORAMAKJGDDjoos2bNyqxZs/L4449nwIABGThwYF577bU8/fTTeemll9KyZcu0aNEiLVq0yFZbbZX3338/L7/8cubPn58333wzffv2rd5fo0aNstdee63yOX369Knx+tlnn02/fv2qC7Qk2XfffbNo0aL8/e9/z8svv5zly5dn3333rX6/cePG2XvvvfPss8/W2Ncee+xR/bx9+/bZYostqgu0D8beeuutJEnPnj1zyCGHZPfdd88xxxyT66+/Pv/85z/X81cPAGDjalTqAAAAm6vmzZtnxx13rH7905/+NK1bt87111+fRYsWpU+fPrn55ptX2a5t27br/DkbSuPGjaufl5WV1Xj9wVhVVVWSpGHDhrn//vvzyCOPVC/z/Pa3v53HHnss3bp122AZAQDqgjPRAADqibKysjRo0CDvvfdeevfunRdffDHt2rXLjjvuWOPRunXrtG7dOh07dsxjjz1Wvf2KFSsyY8aMop+z8847Z/r06TWuRfbwww+nZcuW2XbbbdO9e/fqa6h9YPny5XniiSeyyy67fOTvuO++++b888/PU089lSZNmtR6HTcAgPpGiQYAUCJLly7NvHnzMm/evDz77LP5+te/nkWLFmXQoEE5/vjjs8022+TII4/Mn/70p8yePTtTp07NN77xjfz9739PkowYMSKXXHJJJk6cmOeeey5f+9rX8u677xb93K997WuZO3duvv71r+e5557LnXfemTFjxmTkyJFp0KBBmjdvnq9+9as5++yzM2nSpPztb3/LqaeemiVLluSUU05Z7+/72GOP5eKLL86TTz6ZOXPm5Pbbb09lZWV23nnn9d4nAMDGYjknAECJTJo0KR07dkyStGzZMj169Mivf/3rHHjggUmSBx98MN/85jfz+c9/PgsXLkznzp1zyCGHpFWrVkmSM888M2+++WaGDBmSBg0a5OSTT85///d/Z/78+Wv83M6dO+fee+/N2WefnZ49e2arrbbKKaecku985zvVcy655JJUVVXli1/8YhYuXJi99torkydPzpZbbrne37dVq1Z58MEHc8UVV2TBggXZfvvtc9lll2XgwIHrvU8AgI3F3TkBAAAAoAjLOQEAAACgCCUaAAAAABShRAMAAACAIpRoAAAAAFCEEg0AAAAAilCiAQAAAEARSjQAAAAAKEKJBgAAAABFKNEAAAAAoAglGgAAAAAUoUQDAAAAgCL+H0v0j65co4RPAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1500x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "ax = sns.barplot(x=df['Bedrooms'], y=df['Property_Price'])\n",
        "ax.set_xticklabels(ax.get_xticklabels())\n",
        "ax.set_title('Bedrooms VS Price', fontsize=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "nC4EaV0l2XMv",
        "outputId": "983070f1-150e-4f24-cdb4-ee1b11b6813e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>City</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Islamabad</th>\n",
              "      <td>4778</td>\n",
              "      <td>11000</td>\n",
              "      <td>900000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Karachi</th>\n",
              "      <td>3226</td>\n",
              "      <td>12000</td>\n",
              "      <td>850000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lahore</th>\n",
              "      <td>4715</td>\n",
              "      <td>11000</td>\n",
              "      <td>650000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            len    min        max\n",
              "City                             \n",
              "Islamabad  4778  11000  900000000\n",
              "Karachi    3226  12000  850000000\n",
              "Lahore     4715  11000  650000000"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Displaying total records, min and max price from each city\n",
        "# Dropping the row with odd property price observed\n",
        "df = df[df.Property_Price != 1]\n",
        "df = df[df.Property_Price != 12]\n",
        "df = df[df.Property_Price != 15]\n",
        "df = df[df.Property_Price != 125]\n",
        "city = df.groupby(['City']).Property_Price.agg([len, min, max])\n",
        "city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "4BWnDLcV3er_",
        "outputId": "44d0683d-9835-418f-b645-fb4b790d914d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "ax = sns.barplot(x=\"City\", y=\"Property_Price\", data=df)\n",
        "ax.set_xticklabels(ax.get_xticklabels());"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>mean</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>City</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Islamabad</th>\n",
              "      <td>4778</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>300.0</td>\n",
              "      <td>12.569527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Karachi</th>\n",
              "      <td>3226</td>\n",
              "      <td>1.090909</td>\n",
              "      <td>158.0</td>\n",
              "      <td>9.661230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lahore</th>\n",
              "      <td>4715</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>10.363966</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            len       min    max       mean\n",
              "City                                       \n",
              "Islamabad  4778  0.200000  300.0  12.569527\n",
              "Karachi    3226  1.090909  158.0   9.661230\n",
              "Lahore     4715  1.000000  100.0  10.363966"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "city = df.groupby(['City']).Area.agg([len, min, max, np.mean])\n",
        "city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "Y7ahgBqI30jX",
        "outputId": "59a6e8cd-e17e-4de1-fe83-70e4ba06672a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_30997/654995354.py:4: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  ax = sns.heatmap(df.corr(),annot = True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'CORRELATION MATRIX')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Checking correlation after cleaning\n",
        "# Checking correlation between the features\n",
        "plt.figure(figsize=(15,5))\n",
        "ax = sns.heatmap(df.corr(),annot = True)\n",
        "ax.set_title('CORRELATION MATRIX', fontsize=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kCOuiQ34FeC",
        "outputId": "61252792-e814-49b4-91a0-d81626ea822c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "House    6638\n",
              "Flat     6081\n",
              "Name: Property_Type, dtype: int64"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Property_Type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yBDwAwY-Q7M",
        "outputId": "92a02971-96d2-40f8-afc5-580e5fa03f7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "For Rent    6600\n",
              "For Sale    6119\n",
              "Name: Property_Purpose, dtype: int64"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Property_Purpose'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ait7Reo-W-z",
        "outputId": "e232cf0d-6784-4f4c-daab-6b1bff49f1bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Islamabad    4778\n",
              "Lahore       4715\n",
              "Karachi      3226\n",
              "Name: City, dtype: int64"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['City'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "P2khbjvm-cWS",
        "outputId": "9d4fa58d-4553-4e62-93b9-0b01a6e43490"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Property_Type</th>\n",
              "      <th>Property_Purpose</th>\n",
              "      <th>Baths</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>City</th>\n",
              "      <th>Area</th>\n",
              "      <th>Area_Name</th>\n",
              "      <th>Property_Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>2.1</td>\n",
              "      <td>Bahria Town</td>\n",
              "      <td>3900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>2.2</td>\n",
              "      <td>Raiwind Road</td>\n",
              "      <td>6000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Askari</td>\n",
              "      <td>30000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Askari</td>\n",
              "      <td>30000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Flat</td>\n",
              "      <td>For Sale</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Lahore</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Askari</td>\n",
              "      <td>30000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Property_Type Property_Purpose  Baths  Bedrooms    City  Area     Area_Name  \\\n",
              "0          Flat         For Sale      1         1  Lahore   2.1   Bahria Town   \n",
              "1          Flat         For Sale      1         5  Lahore   2.2  Raiwind Road   \n",
              "2          Flat         For Sale      3         3  Lahore  10.0        Askari   \n",
              "3          Flat         For Sale      3         3  Lahore  10.0        Askari   \n",
              "4          Flat         For Sale      3         3  Lahore  10.0        Askari   \n",
              "\n",
              "   Property_Price  \n",
              "0         3900000  \n",
              "1         6000000  \n",
              "2        30000000  \n",
              "3        30000000  \n",
              "4        30000000  "
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "qIyORez3-nBD",
        "outputId": "b0a4a593-74d8-432b-e34c-4405a24482b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot: >"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 8000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Checking outliers in Price\n",
        "df[\"Property_Price\"].plot.box(vert = 0, figsize=(80,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8C8QGWR_PrM",
        "outputId": "ed330bf2-6828-41a2-e1a3-585a7512ccd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.Property_Price[df.Property_Price > 400000000].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "kbPAO8ZQ_u7C",
        "outputId": "da532734-6b27-49ac-ffc9-35175c803237"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot: >"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 8000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df[\"Area\"].plot.box(vert = 0, figsize=(80,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "gWw5O-KO_8Hq",
        "outputId": "57a016aa-2dd4-488e-e0a0-62acc5be6af8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>len</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>City</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Islamabad</th>\n",
              "      <td>4778</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Karachi</th>\n",
              "      <td>3226</td>\n",
              "      <td>1.090909</td>\n",
              "      <td>158.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lahore</th>\n",
              "      <td>4715</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            len       min    max\n",
              "City                            \n",
              "Islamabad  4778  0.200000  300.0\n",
              "Karachi    3226  1.090909  158.0\n",
              "Lahore     4715  1.000000  100.0"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "city = df.groupby(['City']).Area.agg([len, min, max])\n",
        "city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qwse0csTAdko",
        "outputId": "91fc410d-9822-42a1-e1fd-cefc0646a585"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.Area[df.Area > 100].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ub83iXidA1ux"
      },
      "outputs": [],
      "source": [
        "#Removing rows with area greater than 100 and price greater than 80 Crores\n",
        "df = df[df.Area < 100]\n",
        "df = df[df.Property_Price < 800000000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "kVGBo-CFAqIY",
        "outputId": "8cbf2a42-572d-4927-d29c-e62b1036de13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot: >"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 8000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df[\"Area\"].plot.box(vert = 0, figsize=(80,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "3huvt8OUBQ6r",
        "outputId": "d7617677-3277-41c4-d0eb-bf37ef26ae4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot: >"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 8000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df[\"Property_Price\"].plot.box(vert = 0, figsize=(80,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_s79aWbcBt8l"
      },
      "outputs": [],
      "source": [
        "df.to_csv('final_data_processed.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pefbviDFCMT",
        "outputId": "37241edf-b50c-4f95-f7ba-fa4fe9b82327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4277 1833\n"
          ]
        }
      ],
      "source": [
        "# Running the model on 'For Sale' properties only as the prices for rental property are lesser than sale property\n",
        "# and can impact the accuracy of the model\n",
        "df_for_sale = pd.read_csv('final_data_processed.csv')\n",
        "df_for_sale = df_for_sale[df_for_sale[\"Property_Purpose\"]==\"For Sale\"]\n",
        "df_for_sale.drop(df_for_sale.columns[[1]], axis=1, inplace=True)\n",
        "df_for_sale = pd.get_dummies(df_for_sale, columns = ['Property_Type','City', 'Area_Name'])\n",
        "labels = df_for_sale[\"Property_Price\"]\n",
        "train = df_for_sale.drop(columns=[\"Property_Price\"])\n",
        "x_train, x_test, y_train, y_test = train_test_split(train, labels, test_size=0.3, random_state = 12)\n",
        "print(len(x_train),len(x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "sYsKNkDpHKEf",
        "outputId": "2e60f6bc-9832-4cea-8f15-7a616b9b62c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Baths</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>Area</th>\n",
              "      <th>Property_Price</th>\n",
              "      <th>Property_Type_Flat</th>\n",
              "      <th>Property_Type_House</th>\n",
              "      <th>City_Islamabad</th>\n",
              "      <th>City_Karachi</th>\n",
              "      <th>City_Lahore</th>\n",
              "      <th>Area_Name_9th Avenue</th>\n",
              "      <th>...</th>\n",
              "      <th>Area_Name_Top City 1</th>\n",
              "      <th>Area_Name_University Road</th>\n",
              "      <th>Area_Name_University Town</th>\n",
              "      <th>Area_Name_Valencia Housing Society</th>\n",
              "      <th>Area_Name_Venus Housing Scheme</th>\n",
              "      <th>Area_Name_Vital Homes Housing Scheme</th>\n",
              "      <th>Area_Name_Walton Road</th>\n",
              "      <th>Area_Name_Wapda Town</th>\n",
              "      <th>Area_Name_Zamzama</th>\n",
              "      <th>Area_Name_Zaraj Housing Scheme</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.1</td>\n",
              "      <td>3900000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2.2</td>\n",
              "      <td>6000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>10.0</td>\n",
              "      <td>30000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>10.0</td>\n",
              "      <td>30000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>10.0</td>\n",
              "      <td>30000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 253 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Baths  Bedrooms  Area  Property_Price  Property_Type_Flat  \\\n",
              "0      1         1   2.1         3900000                   1   \n",
              "1      1         5   2.2         6000000                   1   \n",
              "2      3         3  10.0        30000000                   1   \n",
              "3      3         3  10.0        30000000                   1   \n",
              "4      3         3  10.0        30000000                   1   \n",
              "\n",
              "   Property_Type_House  City_Islamabad  City_Karachi  City_Lahore  \\\n",
              "0                    0               0             0            1   \n",
              "1                    0               0             0            1   \n",
              "2                    0               0             0            1   \n",
              "3                    0               0             0            1   \n",
              "4                    0               0             0            1   \n",
              "\n",
              "   Area_Name_9th Avenue  ...  Area_Name_Top City 1  Area_Name_University Road  \\\n",
              "0                     0  ...                     0                          0   \n",
              "1                     0  ...                     0                          0   \n",
              "2                     0  ...                     0                          0   \n",
              "3                     0  ...                     0                          0   \n",
              "4                     0  ...                     0                          0   \n",
              "\n",
              "   Area_Name_University Town  Area_Name_Valencia Housing Society  \\\n",
              "0                          0                                   0   \n",
              "1                          0                                   0   \n",
              "2                          0                                   0   \n",
              "3                          0                                   0   \n",
              "4                          0                                   0   \n",
              "\n",
              "   Area_Name_Venus Housing Scheme  Area_Name_Vital Homes Housing Scheme  \\\n",
              "0                               0                                     0   \n",
              "1                               0                                     0   \n",
              "2                               0                                     0   \n",
              "3                               0                                     0   \n",
              "4                               0                                     0   \n",
              "\n",
              "   Area_Name_Walton Road  Area_Name_Wapda Town  Area_Name_Zamzama  \\\n",
              "0                      0                     0                  0   \n",
              "1                      0                     0                  0   \n",
              "2                      0                     0                  0   \n",
              "3                      0                     0                  0   \n",
              "4                      0                     0                  0   \n",
              "\n",
              "   Area_Name_Zaraj Housing Scheme  \n",
              "0                               0  \n",
              "1                               0  \n",
              "2                               0  \n",
              "3                               0  \n",
              "4                               0  \n",
              "\n",
              "[5 rows x 253 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_for_sale.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 300)               75900     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 200)               60200     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 156,301\n",
            "Trainable params: 156,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from tensorflow_addons.metrics import RSquare\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(300, input_shape=(x_train.shape[1],), activation='relu')) # (features,)\n",
        "# model.add(Dropout(0.5)) # specify a percentage between 0 and 0.5, or larger\n",
        "\n",
        "model.add(Dense(200, activation='relu'))\n",
        "# model.add(Dropout(0.5)) # specify a percentage between 0 and 0.5, or larger\n",
        "\n",
        "model.add(Dense(100, activation='relu'))\n",
        "# model.add(Dropout(0.5)) # specify a percentage between 0 and 0.5, or larger\n",
        "\n",
        "model.add(Dense(1, activation='linear')) # output node\n",
        "\n",
        "model.summary() # see what your model looks like\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_logarithmic_error', metrics=[RSquare()])\n",
        "\n",
        "# early stopping callback\n",
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   mode='min',\n",
        "                   patience=500,\n",
        "                   restore_best_weights=True)\n",
        "#model checkpoint callback\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-16 23:24:59.261357: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1db71cd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2023-05-16 23:24:59.261386: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
            "2023-05-16 23:24:59.265728: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2023-05-16 23:24:59.383354: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56/67 [========================>.....] - ETA: 0s - loss: 177.8246 - r_square: -0.5404\n",
            "Epoch 1: val_loss improved from inf to 116.99869, saving model to best_model.h5\n",
            "67/67 [==============================] - 3s 7ms/step - loss: 168.7727 - r_square: -0.5540 - val_loss: 116.9987 - val_r_square: -0.5485\n",
            "Epoch 2/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 99.8702 - r_square: -0.5468 \n",
            "Epoch 2: val_loss improved from 116.99869 to 80.84578, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 97.0343 - r_square: -0.5539 - val_loss: 80.8458 - val_r_square: -0.5484\n",
            "Epoch 3/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 73.2617 - r_square: -0.5270\n",
            "Epoch 3: val_loss improved from 80.84578 to 62.91483, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 71.6249 - r_square: -0.5537 - val_loss: 62.9148 - val_r_square: -0.5481\n",
            "Epoch 4/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 58.2432 - r_square: -0.5491\n",
            "Epoch 4: val_loss improved from 62.91483 to 51.73275, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 57.3698 - r_square: -0.5533 - val_loss: 51.7327 - val_r_square: -0.5477\n",
            "Epoch 5/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 48.6371 - r_square: -0.5603\n",
            "Epoch 5: val_loss improved from 51.73275 to 43.65741, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 47.8893 - r_square: -0.5528 - val_loss: 43.6574 - val_r_square: -0.5469\n",
            "Epoch 6/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 40.9118 - r_square: -0.5492\n",
            "Epoch 6: val_loss improved from 43.65741 to 37.44419, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 40.7362 - r_square: -0.5519 - val_loss: 37.4442 - val_r_square: -0.5459\n",
            "Epoch 7/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 35.7158 - r_square: -0.5693\n",
            "Epoch 7: val_loss improved from 37.44419 to 32.68897, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 35.2650 - r_square: -0.5507 - val_loss: 32.6890 - val_r_square: -0.5446\n",
            "Epoch 8/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 31.3292 - r_square: -0.5522\n",
            "Epoch 8: val_loss improved from 32.68897 to 28.91159, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 30.9964 - r_square: -0.5493 - val_loss: 28.9116 - val_r_square: -0.5429\n",
            "Epoch 9/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 27.8249 - r_square: -0.5358\n",
            "Epoch 9: val_loss improved from 28.91159 to 25.87248, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 27.5852 - r_square: -0.5476 - val_loss: 25.8725 - val_r_square: -0.5410\n",
            "Epoch 10/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 25.0650 - r_square: -0.5423\n",
            "Epoch 10: val_loss improved from 25.87248 to 23.36501, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 24.8111 - r_square: -0.5456 - val_loss: 23.3650 - val_r_square: -0.5389\n",
            "Epoch 11/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 22.7241 - r_square: -0.5609\n",
            "Epoch 11: val_loss improved from 23.36501 to 21.22600, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 22.4874 - r_square: -0.5433 - val_loss: 21.2260 - val_r_square: -0.5364\n",
            "Epoch 12/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 20.6731 - r_square: -0.5417\n",
            "Epoch 12: val_loss improved from 21.22600 to 19.29415, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 20.4474 - r_square: -0.5406 - val_loss: 19.2941 - val_r_square: -0.5334\n",
            "Epoch 13/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 18.7668 - r_square: -0.5482\n",
            "Epoch 13: val_loss improved from 19.29415 to 17.58463, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 18.6124 - r_square: -0.5375 - val_loss: 17.5846 - val_r_square: -0.5300\n",
            "Epoch 14/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 17.1451 - r_square: -0.5158\n",
            "Epoch 14: val_loss improved from 17.58463 to 16.07820, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 16.9980 - r_square: -0.5340 - val_loss: 16.0782 - val_r_square: -0.5262\n",
            "Epoch 15/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 15.6703 - r_square: -0.5245\n",
            "Epoch 15: val_loss improved from 16.07820 to 14.71997, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 15.5578 - r_square: -0.5301 - val_loss: 14.7200 - val_r_square: -0.5220\n",
            "Epoch 16/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 14.3250 - r_square: -0.5026\n",
            "Epoch 16: val_loss improved from 14.71997 to 13.50403, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 14.2620 - r_square: -0.5256 - val_loss: 13.5040 - val_r_square: -0.5173\n",
            "Epoch 17/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 13.2209 - r_square: -0.5143\n",
            "Epoch 17: val_loss improved from 13.50403 to 12.38761, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 13.0952 - r_square: -0.5208 - val_loss: 12.3876 - val_r_square: -0.5119\n",
            "Epoch 18/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 12.0512 - r_square: -0.4925\n",
            "Epoch 18: val_loss improved from 12.38761 to 11.36035, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 12.0101 - r_square: -0.5152 - val_loss: 11.3603 - val_r_square: -0.5060\n",
            "Epoch 19/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 11.0828 - r_square: -0.5059\n",
            "Epoch 19: val_loss improved from 11.36035 to 10.43952, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 11.0285 - r_square: -0.5091 - val_loss: 10.4395 - val_r_square: -0.4995\n",
            "Epoch 20/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 10.2359 - r_square: -0.5295\n",
            "Epoch 20: val_loss improved from 10.43952 to 9.61896, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 10.1523 - r_square: -0.5026 - val_loss: 9.6190 - val_r_square: -0.4927\n",
            "Epoch 21/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 9.4512 - r_square: -0.5044\n",
            "Epoch 21: val_loss improved from 9.61896 to 8.88516, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 9.3697 - r_square: -0.4956 - val_loss: 8.8852 - val_r_square: -0.4854\n",
            "Epoch 22/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 8.7164 - r_square: -0.4748\n",
            "Epoch 22: val_loss improved from 8.88516 to 8.22555, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 8.6677 - r_square: -0.4884 - val_loss: 8.2256 - val_r_square: -0.4778\n",
            "Epoch 23/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 8.1097 - r_square: -0.4765\n",
            "Epoch 23: val_loss improved from 8.22555 to 7.62967, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 8.0353 - r_square: -0.4807 - val_loss: 7.6297 - val_r_square: -0.4698\n",
            "Epoch 24/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 7.5165 - r_square: -0.4776\n",
            "Epoch 24: val_loss improved from 7.62967 to 7.09012, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 7.4629 - r_square: -0.4728 - val_loss: 7.0901 - val_r_square: -0.4615\n",
            "Epoch 25/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 6.9780 - r_square: -0.4765\n",
            "Epoch 25: val_loss improved from 7.09012 to 6.59895, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 6.9426 - r_square: -0.4642 - val_loss: 6.5989 - val_r_square: -0.4528\n",
            "Epoch 26/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 6.4964 - r_square: -0.4594\n",
            "Epoch 26: val_loss improved from 6.59895 to 6.14963, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 6.4681 - r_square: -0.4558 - val_loss: 6.1496 - val_r_square: -0.4438\n",
            "Epoch 27/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 6.0688 - r_square: -0.4573\n",
            "Epoch 27: val_loss improved from 6.14963 to 5.73769, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 6.0338 - r_square: -0.4468 - val_loss: 5.7377 - val_r_square: -0.4344\n",
            "Epoch 28/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 5.6691 - r_square: -0.4285\n",
            "Epoch 28: val_loss improved from 5.73769 to 5.35969, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 5.6349 - r_square: -0.4375 - val_loss: 5.3597 - val_r_square: -0.4248\n",
            "Epoch 29/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 5.2956 - r_square: -0.4110\n",
            "Epoch 29: val_loss improved from 5.35969 to 5.01099, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 5.2677 - r_square: -0.4279 - val_loss: 5.0110 - val_r_square: -0.4148\n",
            "Epoch 30/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 4.9509 - r_square: -0.4224\n",
            "Epoch 30: val_loss improved from 5.01099 to 4.68911, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 4.9289 - r_square: -0.4179 - val_loss: 4.6891 - val_r_square: -0.4045\n",
            "Epoch 31/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 4.6709 - r_square: -0.4057\n",
            "Epoch 31: val_loss improved from 4.68911 to 4.39082, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 4.6155 - r_square: -0.4077 - val_loss: 4.3908 - val_r_square: -0.3939\n",
            "Epoch 32/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 4.3519 - r_square: -0.3743\n",
            "Epoch 32: val_loss improved from 4.39082 to 4.11503, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 4.3251 - r_square: -0.3971 - val_loss: 4.1150 - val_r_square: -0.3831\n",
            "Epoch 33/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 4.0839 - r_square: -0.3806\n",
            "Epoch 33: val_loss improved from 4.11503 to 3.85836, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 4.0555 - r_square: -0.3862 - val_loss: 3.8584 - val_r_square: -0.3720\n",
            "Epoch 34/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 3.8477 - r_square: -0.3671\n",
            "Epoch 34: val_loss improved from 3.85836 to 3.61962, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 3.8050 - r_square: -0.3755 - val_loss: 3.6196 - val_r_square: -0.3605\n",
            "Epoch 35/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 3.5894 - r_square: -0.3678\n",
            "Epoch 35: val_loss improved from 3.61962 to 3.39793, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 3.5717 - r_square: -0.3640 - val_loss: 3.3979 - val_r_square: -0.3489\n",
            "Epoch 36/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 3.3713 - r_square: -0.3467\n",
            "Epoch 36: val_loss improved from 3.39793 to 3.19061, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 3.3545 - r_square: -0.3524 - val_loss: 3.1906 - val_r_square: -0.3370\n",
            "Epoch 37/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 3.1498 - r_square: -0.3366\n",
            "Epoch 37: val_loss improved from 3.19061 to 2.99805, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 3.1518 - r_square: -0.3408 - val_loss: 2.9981 - val_r_square: -0.3248\n",
            "Epoch 38/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 2.9670 - r_square: -0.3286\n",
            "Epoch 38: val_loss improved from 2.99805 to 2.81775, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 2.9626 - r_square: -0.3286 - val_loss: 2.8178 - val_r_square: -0.3125\n",
            "Epoch 39/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 2.7926 - r_square: -0.3090\n",
            "Epoch 39: val_loss improved from 2.81775 to 2.64922, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 2.7857 - r_square: -0.3165 - val_loss: 2.6492 - val_r_square: -0.2999\n",
            "Epoch 40/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 2.6464 - r_square: -0.3095\n",
            "Epoch 40: val_loss improved from 2.64922 to 2.49156, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 2.6202 - r_square: -0.3040 - val_loss: 2.4916 - val_r_square: -0.2870\n",
            "Epoch 41/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 2.4796 - r_square: -0.2890\n",
            "Epoch 41: val_loss improved from 2.49156 to 2.34414, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 2.4652 - r_square: -0.2914 - val_loss: 2.3441 - val_r_square: -0.2740\n",
            "Epoch 42/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 2.3232 - r_square: -0.2764\n",
            "Epoch 42: val_loss improved from 2.34414 to 2.20594, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 2.3201 - r_square: -0.2784 - val_loss: 2.2059 - val_r_square: -0.2608\n",
            "Epoch 43/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 2.1908 - r_square: -0.2634\n",
            "Epoch 43: val_loss improved from 2.20594 to 2.07321, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 2.1831 - r_square: -0.2651 - val_loss: 2.0732 - val_r_square: -0.2471\n",
            "Epoch 44/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 2.0534 - r_square: -0.2531\n",
            "Epoch 44: val_loss improved from 2.07321 to 1.93709, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 2.0474 - r_square: -0.2511 - val_loss: 1.9371 - val_r_square: -0.2317\n",
            "Epoch 45/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 1.9205 - r_square: -0.2397\n",
            "Epoch 45: val_loss improved from 1.93709 to 1.78059, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 1.8988 - r_square: -0.2344 - val_loss: 1.7806 - val_r_square: -0.2123\n",
            "Epoch 46/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 1.7527 - r_square: -0.2132\n",
            "Epoch 46: val_loss improved from 1.78059 to 1.62622, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 1.7382 - r_square: -0.2140 - val_loss: 1.6262 - val_r_square: -0.1909\n",
            "Epoch 47/1500\n",
            "49/67 [====================>.........] - ETA: 0s - loss: 1.5951 - r_square: -0.2107\n",
            "Epoch 47: val_loss improved from 1.62622 to 1.48696, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 1.5879 - r_square: -0.1927 - val_loss: 1.4870 - val_r_square: -0.1693\n",
            "Epoch 48/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 1.4806 - r_square: -0.1775\n",
            "Epoch 48: val_loss improved from 1.48696 to 1.36255, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 1.4531 - r_square: -0.1714 - val_loss: 1.3625 - val_r_square: -0.1478\n",
            "Epoch 49/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 1.3405 - r_square: -0.1461\n",
            "Epoch 49: val_loss improved from 1.36255 to 1.25225, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 1.3331 - r_square: -0.1509 - val_loss: 1.2523 - val_r_square: -0.1266\n",
            "Epoch 50/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 1.2278 - r_square: -0.1410\n",
            "Epoch 50: val_loss improved from 1.25225 to 1.15473, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 1.2266 - r_square: -0.1297 - val_loss: 1.1547 - val_r_square: -0.1058\n",
            "Epoch 51/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 1.1406 - r_square: -0.1095\n",
            "Epoch 51: val_loss improved from 1.15473 to 1.06782, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 1.1321 - r_square: -0.1100 - val_loss: 1.0678 - val_r_square: -0.0853\n",
            "Epoch 52/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 1.0410 - r_square: -0.1056\n",
            "Epoch 52: val_loss improved from 1.06782 to 0.99090, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 1.0479 - r_square: -0.0894 - val_loss: 0.9909 - val_r_square: -0.0654\n",
            "Epoch 53/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.9712 - r_square: -0.0794\n",
            "Epoch 53: val_loss improved from 0.99090 to 0.92252, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.9731 - r_square: -0.0707 - val_loss: 0.9225 - val_r_square: -0.0459\n",
            "Epoch 54/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.9241 - r_square: -0.0582\n",
            "Epoch 54: val_loss improved from 0.92252 to 0.86122, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.9063 - r_square: -0.0517 - val_loss: 0.8612 - val_r_square: -0.0267\n",
            "Epoch 55/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.8410 - r_square: -0.0293\n",
            "Epoch 55: val_loss improved from 0.86122 to 0.80698, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.8467 - r_square: -0.0331 - val_loss: 0.8070 - val_r_square: -0.0082\n",
            "Epoch 56/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.8033 - r_square: -0.0199\n",
            "Epoch 56: val_loss improved from 0.80698 to 0.75851, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.7936 - r_square: -0.0151 - val_loss: 0.7585 - val_r_square: 0.0099\n",
            "Epoch 57/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.7570 - r_square: 1.2439e-04\n",
            "Epoch 57: val_loss improved from 0.75851 to 0.71561, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.7461 - r_square: 0.0018 - val_loss: 0.7156 - val_r_square: 0.0274\n",
            "Epoch 58/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.7153 - r_square: 0.0149\n",
            "Epoch 58: val_loss improved from 0.71561 to 0.67715, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.7038 - r_square: 0.0190 - val_loss: 0.6771 - val_r_square: 0.0445\n",
            "Epoch 59/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.6527 - r_square: 0.0362\n",
            "Epoch 59: val_loss improved from 0.67715 to 0.64282, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.6659 - r_square: 0.0358 - val_loss: 0.6428 - val_r_square: 0.0611\n",
            "Epoch 60/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.6286 - r_square: 0.0525\n",
            "Epoch 60: val_loss improved from 0.64282 to 0.61262, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.6320 - r_square: 0.0518 - val_loss: 0.6126 - val_r_square: 0.0770\n",
            "Epoch 61/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.5921 - r_square: 0.0734\n",
            "Epoch 61: val_loss improved from 0.61262 to 0.58532, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.6019 - r_square: 0.0670 - val_loss: 0.5853 - val_r_square: 0.0926\n",
            "Epoch 62/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.5764 - r_square: 0.0808\n",
            "Epoch 62: val_loss improved from 0.58532 to 0.56098, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.5750 - r_square: 0.0820 - val_loss: 0.5610 - val_r_square: 0.1077\n",
            "Epoch 63/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.5606 - r_square: 0.0954\n",
            "Epoch 63: val_loss improved from 0.56098 to 0.53980, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.5509 - r_square: 0.0964 - val_loss: 0.5398 - val_r_square: 0.1220\n",
            "Epoch 64/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.5335 - r_square: 0.1018\n",
            "Epoch 64: val_loss improved from 0.53980 to 0.52094, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.5295 - r_square: 0.1104 - val_loss: 0.5209 - val_r_square: 0.1358\n",
            "Epoch 65/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.5214 - r_square: 0.1145\n",
            "Epoch 65: val_loss improved from 0.52094 to 0.50392, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.5106 - r_square: 0.1238 - val_loss: 0.5039 - val_r_square: 0.1494\n",
            "Epoch 66/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.4993 - r_square: 0.1332\n",
            "Epoch 66: val_loss improved from 0.50392 to 0.48922, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.4936 - r_square: 0.1364 - val_loss: 0.4892 - val_r_square: 0.1621\n",
            "Epoch 67/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.4788 - r_square: 0.1446\n",
            "Epoch 67: val_loss improved from 0.48922 to 0.47622, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.4786 - r_square: 0.1489 - val_loss: 0.4762 - val_r_square: 0.1743\n",
            "Epoch 68/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.4597 - r_square: 0.1646\n",
            "Epoch 68: val_loss improved from 0.47622 to 0.46457, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.4653 - r_square: 0.1607 - val_loss: 0.4646 - val_r_square: 0.1862\n",
            "Epoch 69/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.4551 - r_square: 0.1678\n",
            "Epoch 69: val_loss improved from 0.46457 to 0.45433, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.4535 - r_square: 0.1721 - val_loss: 0.4543 - val_r_square: 0.1975\n",
            "Epoch 70/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.4387 - r_square: 0.1823\n",
            "Epoch 70: val_loss improved from 0.45433 to 0.44554, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.4431 - r_square: 0.1829 - val_loss: 0.4455 - val_r_square: 0.2081\n",
            "Epoch 71/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.4314 - r_square: 0.1989\n",
            "Epoch 71: val_loss improved from 0.44554 to 0.43776, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.4339 - r_square: 0.1930 - val_loss: 0.4378 - val_r_square: 0.2183\n",
            "Epoch 72/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.4211 - r_square: 0.2024\n",
            "Epoch 72: val_loss improved from 0.43776 to 0.43078, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.4258 - r_square: 0.2030 - val_loss: 0.4308 - val_r_square: 0.2283\n",
            "Epoch 73/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.4201 - r_square: 0.2094\n",
            "Epoch 73: val_loss improved from 0.43078 to 0.42480, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.4186 - r_square: 0.2123 - val_loss: 0.4248 - val_r_square: 0.2376\n",
            "Epoch 74/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.4112 - r_square: 0.2270\n",
            "Epoch 74: val_loss improved from 0.42480 to 0.41965, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.4123 - r_square: 0.2212 - val_loss: 0.4196 - val_r_square: 0.2462\n",
            "Epoch 75/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.4069 - r_square: 0.2281\n",
            "Epoch 75: val_loss improved from 0.41965 to 0.41492, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.4067 - r_square: 0.2296 - val_loss: 0.4149 - val_r_square: 0.2548\n",
            "Epoch 76/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.4084 - r_square: 0.2304\n",
            "Epoch 76: val_loss improved from 0.41492 to 0.41092, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.4018 - r_square: 0.2375 - val_loss: 0.4109 - val_r_square: 0.2627\n",
            "Epoch 77/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3968 - r_square: 0.2426\n",
            "Epoch 77: val_loss improved from 0.41092 to 0.40745, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3975 - r_square: 0.2450 - val_loss: 0.4074 - val_r_square: 0.2701\n",
            "Epoch 78/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.3978 - r_square: 0.2541\n",
            "Epoch 78: val_loss improved from 0.40745 to 0.40438, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3937 - r_square: 0.2523 - val_loss: 0.4044 - val_r_square: 0.2771\n",
            "Epoch 79/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3942 - r_square: 0.2512\n",
            "Epoch 79: val_loss improved from 0.40438 to 0.40159, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3903 - r_square: 0.2588 - val_loss: 0.4016 - val_r_square: 0.2839\n",
            "Epoch 80/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3893 - r_square: 0.2642\n",
            "Epoch 80: val_loss improved from 0.40159 to 0.39924, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3873 - r_square: 0.2653 - val_loss: 0.3992 - val_r_square: 0.2899\n",
            "Epoch 81/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3881 - r_square: 0.2655\n",
            "Epoch 81: val_loss improved from 0.39924 to 0.39706, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3846 - r_square: 0.2711 - val_loss: 0.3971 - val_r_square: 0.2959\n",
            "Epoch 82/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.3851 - r_square: 0.2706\n",
            "Epoch 82: val_loss improved from 0.39706 to 0.39513, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3821 - r_square: 0.2764 - val_loss: 0.3951 - val_r_square: 0.3013\n",
            "Epoch 83/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.3829 - r_square: 0.2769\n",
            "Epoch 83: val_loss improved from 0.39513 to 0.39335, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3799 - r_square: 0.2819 - val_loss: 0.3933 - val_r_square: 0.3065\n",
            "Epoch 84/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.3800 - r_square: 0.2861\n",
            "Epoch 84: val_loss improved from 0.39335 to 0.39172, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3779 - r_square: 0.2869 - val_loss: 0.3917 - val_r_square: 0.3113\n",
            "Epoch 85/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.3808 - r_square: 0.2961\n",
            "Epoch 85: val_loss improved from 0.39172 to 0.39022, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3760 - r_square: 0.2911 - val_loss: 0.3902 - val_r_square: 0.3156\n",
            "Epoch 86/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.3742 - r_square: 0.2953\n",
            "Epoch 86: val_loss improved from 0.39022 to 0.38875, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3742 - r_square: 0.2953 - val_loss: 0.3887 - val_r_square: 0.3200\n",
            "Epoch 87/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3762 - r_square: 0.2933\n",
            "Epoch 87: val_loss improved from 0.38875 to 0.38737, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3726 - r_square: 0.2997 - val_loss: 0.3874 - val_r_square: 0.3242\n",
            "Epoch 88/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3708 - r_square: 0.3022\n",
            "Epoch 88: val_loss improved from 0.38737 to 0.38604, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3710 - r_square: 0.3032 - val_loss: 0.3860 - val_r_square: 0.3276\n",
            "Epoch 89/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3695 - r_square: 0.3144\n",
            "Epoch 89: val_loss improved from 0.38604 to 0.38472, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3695 - r_square: 0.3066 - val_loss: 0.3847 - val_r_square: 0.3311\n",
            "Epoch 90/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3611 - r_square: 0.3154\n",
            "Epoch 90: val_loss improved from 0.38472 to 0.38342, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3680 - r_square: 0.3101 - val_loss: 0.3834 - val_r_square: 0.3344\n",
            "Epoch 91/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3639 - r_square: 0.3142\n",
            "Epoch 91: val_loss improved from 0.38342 to 0.38210, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3665 - r_square: 0.3132 - val_loss: 0.3821 - val_r_square: 0.3378\n",
            "Epoch 92/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3574 - r_square: 0.3234\n",
            "Epoch 92: val_loss improved from 0.38210 to 0.38079, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3650 - r_square: 0.3160 - val_loss: 0.3808 - val_r_square: 0.3404\n",
            "Epoch 93/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.3641 - r_square: 0.3151\n",
            "Epoch 93: val_loss improved from 0.38079 to 0.37945, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3636 - r_square: 0.3189 - val_loss: 0.3795 - val_r_square: 0.3433\n",
            "Epoch 94/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.3578 - r_square: 0.3192\n",
            "Epoch 94: val_loss improved from 0.37945 to 0.37810, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3621 - r_square: 0.3216 - val_loss: 0.3781 - val_r_square: 0.3459\n",
            "Epoch 95/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3702 - r_square: 0.3244\n",
            "Epoch 95: val_loss improved from 0.37810 to 0.37669, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3606 - r_square: 0.3238 - val_loss: 0.3767 - val_r_square: 0.3482\n",
            "Epoch 96/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3639 - r_square: 0.3168\n",
            "Epoch 96: val_loss improved from 0.37669 to 0.37530, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3591 - r_square: 0.3262 - val_loss: 0.3753 - val_r_square: 0.3508\n",
            "Epoch 97/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.3576 - r_square: 0.3286\n",
            "Epoch 97: val_loss improved from 0.37530 to 0.37382, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3576 - r_square: 0.3286 - val_loss: 0.3738 - val_r_square: 0.3528\n",
            "Epoch 98/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3518 - r_square: 0.3345\n",
            "Epoch 98: val_loss improved from 0.37382 to 0.37234, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3561 - r_square: 0.3305 - val_loss: 0.3723 - val_r_square: 0.3551\n",
            "Epoch 99/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.3593 - r_square: 0.3208\n",
            "Epoch 99: val_loss improved from 0.37234 to 0.37080, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3545 - r_square: 0.3327 - val_loss: 0.3708 - val_r_square: 0.3571\n",
            "Epoch 100/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.3538 - r_square: 0.3320\n",
            "Epoch 100: val_loss improved from 0.37080 to 0.36922, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3529 - r_square: 0.3346 - val_loss: 0.3692 - val_r_square: 0.3592\n",
            "Epoch 101/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.3512 - r_square: 0.3365\n",
            "Epoch 101: val_loss improved from 0.36922 to 0.36760, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3512 - r_square: 0.3365 - val_loss: 0.3676 - val_r_square: 0.3613\n",
            "Epoch 102/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.3427 - r_square: 0.3431\n",
            "Epoch 102: val_loss improved from 0.36760 to 0.36597, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3496 - r_square: 0.3387 - val_loss: 0.3660 - val_r_square: 0.3634\n",
            "Epoch 103/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3526 - r_square: 0.3315\n",
            "Epoch 103: val_loss improved from 0.36597 to 0.36419, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3478 - r_square: 0.3399 - val_loss: 0.3642 - val_r_square: 0.3650\n",
            "Epoch 104/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3382 - r_square: 0.3456\n",
            "Epoch 104: val_loss improved from 0.36419 to 0.36247, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3461 - r_square: 0.3422 - val_loss: 0.3625 - val_r_square: 0.3668\n",
            "Epoch 105/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3486 - r_square: 0.3380\n",
            "Epoch 105: val_loss improved from 0.36247 to 0.36065, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3443 - r_square: 0.3439 - val_loss: 0.3606 - val_r_square: 0.3687\n",
            "Epoch 106/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.3369 - r_square: 0.3472\n",
            "Epoch 106: val_loss improved from 0.36065 to 0.35889, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3424 - r_square: 0.3458 - val_loss: 0.3589 - val_r_square: 0.3712\n",
            "Epoch 107/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3419 - r_square: 0.3495\n",
            "Epoch 107: val_loss improved from 0.35889 to 0.35697, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3405 - r_square: 0.3480 - val_loss: 0.3570 - val_r_square: 0.3730\n",
            "Epoch 108/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.3390 - r_square: 0.3521\n",
            "Epoch 108: val_loss improved from 0.35697 to 0.35502, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3386 - r_square: 0.3498 - val_loss: 0.3550 - val_r_square: 0.3748\n",
            "Epoch 109/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3370 - r_square: 0.3529\n",
            "Epoch 109: val_loss improved from 0.35502 to 0.35305, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3366 - r_square: 0.3515 - val_loss: 0.3531 - val_r_square: 0.3767\n",
            "Epoch 110/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.3324 - r_square: 0.3585\n",
            "Epoch 110: val_loss improved from 0.35305 to 0.35102, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3346 - r_square: 0.3534 - val_loss: 0.3510 - val_r_square: 0.3786\n",
            "Epoch 111/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.3382 - r_square: 0.3526\n",
            "Epoch 111: val_loss improved from 0.35102 to 0.34894, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3325 - r_square: 0.3557 - val_loss: 0.3489 - val_r_square: 0.3807\n",
            "Epoch 112/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.3331 - r_square: 0.3554\n",
            "Epoch 112: val_loss improved from 0.34894 to 0.34683, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3304 - r_square: 0.3573 - val_loss: 0.3468 - val_r_square: 0.3828\n",
            "Epoch 113/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.3293 - r_square: 0.3618\n",
            "Epoch 113: val_loss improved from 0.34683 to 0.34470, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3283 - r_square: 0.3602 - val_loss: 0.3447 - val_r_square: 0.3853\n",
            "Epoch 114/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3236 - r_square: 0.3753\n",
            "Epoch 114: val_loss improved from 0.34470 to 0.34248, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3260 - r_square: 0.3613 - val_loss: 0.3425 - val_r_square: 0.3871\n",
            "Epoch 115/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3174 - r_square: 0.3730\n",
            "Epoch 115: val_loss improved from 0.34248 to 0.34026, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3238 - r_square: 0.3642 - val_loss: 0.3403 - val_r_square: 0.3900\n",
            "Epoch 116/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.3215 - r_square: 0.3659\n",
            "Epoch 116: val_loss improved from 0.34026 to 0.33789, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3215 - r_square: 0.3659 - val_loss: 0.3379 - val_r_square: 0.3917\n",
            "Epoch 117/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3165 - r_square: 0.3666\n",
            "Epoch 117: val_loss improved from 0.33789 to 0.33561, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3191 - r_square: 0.3682 - val_loss: 0.3356 - val_r_square: 0.3947\n",
            "Epoch 118/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3099 - r_square: 0.3800\n",
            "Epoch 118: val_loss improved from 0.33561 to 0.33322, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3167 - r_square: 0.3711 - val_loss: 0.3332 - val_r_square: 0.3970\n",
            "Epoch 119/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.3143 - r_square: 0.3734\n",
            "Epoch 119: val_loss improved from 0.33322 to 0.33070, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3143 - r_square: 0.3734 - val_loss: 0.3307 - val_r_square: 0.3990\n",
            "Epoch 120/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.3237 - r_square: 0.3706\n",
            "Epoch 120: val_loss improved from 0.33070 to 0.32826, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3118 - r_square: 0.3757 - val_loss: 0.3283 - val_r_square: 0.4021\n",
            "Epoch 121/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.3029 - r_square: 0.3766\n",
            "Epoch 121: val_loss improved from 0.32826 to 0.32573, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3092 - r_square: 0.3776 - val_loss: 0.3257 - val_r_square: 0.4042\n",
            "Epoch 122/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.3104 - r_square: 0.3835\n",
            "Epoch 122: val_loss improved from 0.32573 to 0.32314, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.3066 - r_square: 0.3802 - val_loss: 0.3231 - val_r_square: 0.4075\n",
            "Epoch 123/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.3045 - r_square: 0.3830\n",
            "Epoch 123: val_loss improved from 0.32314 to 0.32051, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3039 - r_square: 0.3838 - val_loss: 0.3205 - val_r_square: 0.4107\n",
            "Epoch 124/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.2991 - r_square: 0.4011\n",
            "Epoch 124: val_loss improved from 0.32051 to 0.31787, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.3013 - r_square: 0.3871 - val_loss: 0.3179 - val_r_square: 0.4136\n",
            "Epoch 125/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.2981 - r_square: 0.3890\n",
            "Epoch 125: val_loss improved from 0.31787 to 0.31509, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2985 - r_square: 0.3890 - val_loss: 0.3151 - val_r_square: 0.4158\n",
            "Epoch 126/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.2950 - r_square: 0.3913\n",
            "Epoch 126: val_loss improved from 0.31509 to 0.31225, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2957 - r_square: 0.3914 - val_loss: 0.3123 - val_r_square: 0.4189\n",
            "Epoch 127/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.2912 - r_square: 0.4016\n",
            "Epoch 127: val_loss improved from 0.31225 to 0.30944, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2928 - r_square: 0.3942 - val_loss: 0.3094 - val_r_square: 0.4223\n",
            "Epoch 128/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.2891 - r_square: 0.3976\n",
            "Epoch 128: val_loss improved from 0.30944 to 0.30665, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2899 - r_square: 0.3984 - val_loss: 0.3067 - val_r_square: 0.4263\n",
            "Epoch 129/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.2869 - r_square: 0.4011\n",
            "Epoch 129: val_loss improved from 0.30665 to 0.30363, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2869 - r_square: 0.4011 - val_loss: 0.3036 - val_r_square: 0.4285\n",
            "Epoch 130/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.2864 - r_square: 0.3951\n",
            "Epoch 130: val_loss improved from 0.30363 to 0.30069, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2839 - r_square: 0.4040 - val_loss: 0.3007 - val_r_square: 0.4324\n",
            "Epoch 131/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.2816 - r_square: 0.4139\n",
            "Epoch 131: val_loss improved from 0.30069 to 0.29768, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2808 - r_square: 0.4077 - val_loss: 0.2977 - val_r_square: 0.4359\n",
            "Epoch 132/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.2717 - r_square: 0.4126\n",
            "Epoch 132: val_loss improved from 0.29768 to 0.29460, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2777 - r_square: 0.4111 - val_loss: 0.2946 - val_r_square: 0.4391\n",
            "Epoch 133/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.2698 - r_square: 0.4294\n",
            "Epoch 133: val_loss improved from 0.29460 to 0.29153, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2745 - r_square: 0.4145 - val_loss: 0.2915 - val_r_square: 0.4432\n",
            "Epoch 134/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.2722 - r_square: 0.4135\n",
            "Epoch 134: val_loss improved from 0.29153 to 0.28834, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2713 - r_square: 0.4188 - val_loss: 0.2883 - val_r_square: 0.4467\n",
            "Epoch 135/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.2727 - r_square: 0.4121\n",
            "Epoch 135: val_loss improved from 0.28834 to 0.28521, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2681 - r_square: 0.4214 - val_loss: 0.2852 - val_r_square: 0.4508\n",
            "Epoch 136/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.2672 - r_square: 0.4268\n",
            "Epoch 136: val_loss improved from 0.28521 to 0.28203, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2648 - r_square: 0.4256 - val_loss: 0.2820 - val_r_square: 0.4548\n",
            "Epoch 137/1500\n",
            "49/67 [====================>.........] - ETA: 0s - loss: 0.2608 - r_square: 0.4120\n",
            "Epoch 137: val_loss improved from 0.28203 to 0.27885, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2615 - r_square: 0.4293 - val_loss: 0.2789 - val_r_square: 0.4594\n",
            "Epoch 138/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.2581 - r_square: 0.4329\n",
            "Epoch 138: val_loss improved from 0.27885 to 0.27557, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2581 - r_square: 0.4329 - val_loss: 0.2756 - val_r_square: 0.4629\n",
            "Epoch 139/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.2578 - r_square: 0.4351\n",
            "Epoch 139: val_loss improved from 0.27557 to 0.27231, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2548 - r_square: 0.4377 - val_loss: 0.2723 - val_r_square: 0.4673\n",
            "Epoch 140/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.2506 - r_square: 0.4444\n",
            "Epoch 140: val_loss improved from 0.27231 to 0.26911, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2514 - r_square: 0.4426 - val_loss: 0.2691 - val_r_square: 0.4720\n",
            "Epoch 141/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.2426 - r_square: 0.4464\n",
            "Epoch 141: val_loss improved from 0.26911 to 0.26574, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2479 - r_square: 0.4457 - val_loss: 0.2657 - val_r_square: 0.4762\n",
            "Epoch 142/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.2448 - r_square: 0.4495\n",
            "Epoch 142: val_loss improved from 0.26574 to 0.26236, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2445 - r_square: 0.4494 - val_loss: 0.2624 - val_r_square: 0.4800\n",
            "Epoch 143/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.2429 - r_square: 0.4494\n",
            "Epoch 143: val_loss improved from 0.26236 to 0.25918, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2411 - r_square: 0.4546 - val_loss: 0.2592 - val_r_square: 0.4862\n",
            "Epoch 144/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.2401 - r_square: 0.4495\n",
            "Epoch 144: val_loss improved from 0.25918 to 0.25590, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2377 - r_square: 0.4590 - val_loss: 0.2559 - val_r_square: 0.4908\n",
            "Epoch 145/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.2364 - r_square: 0.4618\n",
            "Epoch 145: val_loss improved from 0.25590 to 0.25265, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2342 - r_square: 0.4647 - val_loss: 0.2527 - val_r_square: 0.4957\n",
            "Epoch 146/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.2386 - r_square: 0.4561\n",
            "Epoch 146: val_loss improved from 0.25265 to 0.24934, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.2309 - r_square: 0.4683 - val_loss: 0.2493 - val_r_square: 0.5004\n",
            "Epoch 147/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.2261 - r_square: 0.4717\n",
            "Epoch 147: val_loss improved from 0.24934 to 0.24624, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2274 - r_square: 0.4747 - val_loss: 0.2462 - val_r_square: 0.5061\n",
            "Epoch 148/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.2241 - r_square: 0.4787\n",
            "Epoch 148: val_loss improved from 0.24624 to 0.24299, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2241 - r_square: 0.4787 - val_loss: 0.2430 - val_r_square: 0.5102\n",
            "Epoch 149/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.2179 - r_square: 0.4782\n",
            "Epoch 149: val_loss improved from 0.24299 to 0.23994, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2208 - r_square: 0.4830 - val_loss: 0.2399 - val_r_square: 0.5157\n",
            "Epoch 150/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.2174 - r_square: 0.4893\n",
            "Epoch 150: val_loss improved from 0.23994 to 0.23693, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2176 - r_square: 0.4893 - val_loss: 0.2369 - val_r_square: 0.5214\n",
            "Epoch 151/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.2143 - r_square: 0.4917\n",
            "Epoch 151: val_loss improved from 0.23693 to 0.23390, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.2145 - r_square: 0.4928 - val_loss: 0.2339 - val_r_square: 0.5269\n",
            "Epoch 152/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.2084 - r_square: 0.5087\n",
            "Epoch 152: val_loss improved from 0.23390 to 0.23103, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.2114 - r_square: 0.4997 - val_loss: 0.2310 - val_r_square: 0.5323\n",
            "Epoch 153/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.2093 - r_square: 0.5047\n",
            "Epoch 153: val_loss improved from 0.23103 to 0.22820, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.2084 - r_square: 0.5040 - val_loss: 0.2282 - val_r_square: 0.5380\n",
            "Epoch 154/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.2057 - r_square: 0.5084\n",
            "Epoch 154: val_loss improved from 0.22820 to 0.22550, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.2055 - r_square: 0.5101 - val_loss: 0.2255 - val_r_square: 0.5435\n",
            "Epoch 155/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.2027 - r_square: 0.5135\n",
            "Epoch 155: val_loss improved from 0.22550 to 0.22279, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.2027 - r_square: 0.5147 - val_loss: 0.2228 - val_r_square: 0.5475\n",
            "Epoch 156/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.2018 - r_square: 0.5145\n",
            "Epoch 156: val_loss improved from 0.22279 to 0.22018, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.2000 - r_square: 0.5183 - val_loss: 0.2202 - val_r_square: 0.5528\n",
            "Epoch 157/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1995 - r_square: 0.5209\n",
            "Epoch 157: val_loss improved from 0.22018 to 0.21781, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.1975 - r_square: 0.5239 - val_loss: 0.2178 - val_r_square: 0.5593\n",
            "Epoch 158/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.1954 - r_square: 0.5265\n",
            "Epoch 158: val_loss improved from 0.21781 to 0.21555, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1950 - r_square: 0.5283 - val_loss: 0.2156 - val_r_square: 0.5642\n",
            "Epoch 159/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.1924 - r_square: 0.5306\n",
            "Epoch 159: val_loss improved from 0.21555 to 0.21330, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1927 - r_square: 0.5343 - val_loss: 0.2133 - val_r_square: 0.5694\n",
            "Epoch 160/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.1908 - r_square: 0.5393\n",
            "Epoch 160: val_loss improved from 0.21330 to 0.21116, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1905 - r_square: 0.5405 - val_loss: 0.2112 - val_r_square: 0.5745\n",
            "Epoch 161/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1852 - r_square: 0.5518\n",
            "Epoch 161: val_loss improved from 0.21116 to 0.20913, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1884 - r_square: 0.5432 - val_loss: 0.2091 - val_r_square: 0.5790\n",
            "Epoch 162/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.1866 - r_square: 0.5470\n",
            "Epoch 162: val_loss improved from 0.20913 to 0.20715, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.1864 - r_square: 0.5474 - val_loss: 0.2072 - val_r_square: 0.5839\n",
            "Epoch 163/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1846 - r_square: 0.5548\n",
            "Epoch 163: val_loss improved from 0.20715 to 0.20529, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.1845 - r_square: 0.5529 - val_loss: 0.2053 - val_r_square: 0.5878\n",
            "Epoch 164/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1834 - r_square: 0.5555\n",
            "Epoch 164: val_loss improved from 0.20529 to 0.20352, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1826 - r_square: 0.5574 - val_loss: 0.2035 - val_r_square: 0.5920\n",
            "Epoch 165/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.1809 - r_square: 0.5614\n",
            "Epoch 165: val_loss improved from 0.20352 to 0.20174, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.1809 - r_square: 0.5614 - val_loss: 0.2017 - val_r_square: 0.5950\n",
            "Epoch 166/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1801 - r_square: 0.5620\n",
            "Epoch 166: val_loss improved from 0.20174 to 0.20015, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1792 - r_square: 0.5635 - val_loss: 0.2002 - val_r_square: 0.5999\n",
            "Epoch 167/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.1804 - r_square: 0.5666\n",
            "Epoch 167: val_loss improved from 0.20015 to 0.19861, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1776 - r_square: 0.5669 - val_loss: 0.1986 - val_r_square: 0.6039\n",
            "Epoch 168/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1781 - r_square: 0.5674\n",
            "Epoch 168: val_loss improved from 0.19861 to 0.19710, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1760 - r_square: 0.5715 - val_loss: 0.1971 - val_r_square: 0.6085\n",
            "Epoch 169/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1746 - r_square: 0.5797\n",
            "Epoch 169: val_loss improved from 0.19710 to 0.19563, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1745 - r_square: 0.5744 - val_loss: 0.1956 - val_r_square: 0.6117\n",
            "Epoch 170/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.1730 - r_square: 0.5760\n",
            "Epoch 170: val_loss improved from 0.19563 to 0.19422, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1731 - r_square: 0.5784 - val_loss: 0.1942 - val_r_square: 0.6155\n",
            "Epoch 171/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1704 - r_square: 0.5957\n",
            "Epoch 171: val_loss improved from 0.19422 to 0.19274, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1717 - r_square: 0.5830 - val_loss: 0.1927 - val_r_square: 0.6167\n",
            "Epoch 172/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1718 - r_square: 0.5844\n",
            "Epoch 172: val_loss improved from 0.19274 to 0.19139, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.1703 - r_square: 0.5837 - val_loss: 0.1914 - val_r_square: 0.6199\n",
            "Epoch 173/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.1689 - r_square: 0.5958\n",
            "Epoch 173: val_loss improved from 0.19139 to 0.19017, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1690 - r_square: 0.5867 - val_loss: 0.1902 - val_r_square: 0.6241\n",
            "Epoch 174/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.1677 - r_square: 0.5904\n",
            "Epoch 174: val_loss improved from 0.19017 to 0.18895, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1677 - r_square: 0.5915 - val_loss: 0.1890 - val_r_square: 0.6276\n",
            "Epoch 175/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.1664 - r_square: 0.5924\n",
            "Epoch 175: val_loss improved from 0.18895 to 0.18765, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.1664 - r_square: 0.5924 - val_loss: 0.1876 - val_r_square: 0.6303\n",
            "Epoch 176/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.1677 - r_square: 0.5876\n",
            "Epoch 176: val_loss improved from 0.18765 to 0.18646, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1652 - r_square: 0.5959 - val_loss: 0.1865 - val_r_square: 0.6334\n",
            "Epoch 177/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1652 - r_square: 0.5934\n",
            "Epoch 177: val_loss improved from 0.18646 to 0.18527, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1640 - r_square: 0.5981 - val_loss: 0.1853 - val_r_square: 0.6355\n",
            "Epoch 178/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1628 - r_square: 0.5992\n",
            "Epoch 178: val_loss improved from 0.18527 to 0.18418, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1628 - r_square: 0.6008 - val_loss: 0.1842 - val_r_square: 0.6388\n",
            "Epoch 179/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.1619 - r_square: 0.6032\n",
            "Epoch 179: val_loss improved from 0.18418 to 0.18299, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.1617 - r_square: 0.6034 - val_loss: 0.1830 - val_r_square: 0.6397\n",
            "Epoch 180/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1588 - r_square: 0.6007\n",
            "Epoch 180: val_loss improved from 0.18299 to 0.18191, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.1605 - r_square: 0.6057 - val_loss: 0.1819 - val_r_square: 0.6425\n",
            "Epoch 181/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.1608 - r_square: 0.6004\n",
            "Epoch 181: val_loss improved from 0.18191 to 0.18085, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1594 - r_square: 0.6075 - val_loss: 0.1808 - val_r_square: 0.6464\n",
            "Epoch 182/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1589 - r_square: 0.6046\n",
            "Epoch 182: val_loss improved from 0.18085 to 0.17973, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1583 - r_square: 0.6100 - val_loss: 0.1797 - val_r_square: 0.6473\n",
            "Epoch 183/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1568 - r_square: 0.6112\n",
            "Epoch 183: val_loss improved from 0.17973 to 0.17868, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1573 - r_square: 0.6123 - val_loss: 0.1787 - val_r_square: 0.6494\n",
            "Epoch 184/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1566 - r_square: 0.6107\n",
            "Epoch 184: val_loss improved from 0.17868 to 0.17766, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1562 - r_square: 0.6138 - val_loss: 0.1777 - val_r_square: 0.6524\n",
            "Epoch 185/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.1547 - r_square: 0.6168\n",
            "Epoch 185: val_loss improved from 0.17766 to 0.17665, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.1552 - r_square: 0.6158 - val_loss: 0.1767 - val_r_square: 0.6552\n",
            "Epoch 186/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.1533 - r_square: 0.6130\n",
            "Epoch 186: val_loss improved from 0.17665 to 0.17571, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1541 - r_square: 0.6171 - val_loss: 0.1757 - val_r_square: 0.6579\n",
            "Epoch 187/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.1496 - r_square: 0.6236\n",
            "Epoch 187: val_loss improved from 0.17571 to 0.17451, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1531 - r_square: 0.6210 - val_loss: 0.1745 - val_r_square: 0.6567\n",
            "Epoch 188/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.1533 - r_square: 0.6235\n",
            "Epoch 188: val_loss improved from 0.17451 to 0.17347, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1520 - r_square: 0.6223 - val_loss: 0.1735 - val_r_square: 0.6593\n",
            "Epoch 189/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1509 - r_square: 0.6164\n",
            "Epoch 189: val_loss improved from 0.17347 to 0.17245, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1510 - r_square: 0.6228 - val_loss: 0.1724 - val_r_square: 0.6616\n",
            "Epoch 190/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.1502 - r_square: 0.6244\n",
            "Epoch 190: val_loss improved from 0.17245 to 0.17141, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1499 - r_square: 0.6258 - val_loss: 0.1714 - val_r_square: 0.6633\n",
            "Epoch 191/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1498 - r_square: 0.6291\n",
            "Epoch 191: val_loss improved from 0.17141 to 0.17050, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1489 - r_square: 0.6258 - val_loss: 0.1705 - val_r_square: 0.6672\n",
            "Epoch 192/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1501 - r_square: 0.6252\n",
            "Epoch 192: val_loss improved from 0.17050 to 0.16937, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1478 - r_square: 0.6293 - val_loss: 0.1694 - val_r_square: 0.6673\n",
            "Epoch 193/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.1471 - r_square: 0.6419\n",
            "Epoch 193: val_loss improved from 0.16937 to 0.16834, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1468 - r_square: 0.6301 - val_loss: 0.1683 - val_r_square: 0.6690\n",
            "Epoch 194/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.1451 - r_square: 0.6213\n",
            "Epoch 194: val_loss improved from 0.16834 to 0.16733, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1458 - r_square: 0.6326 - val_loss: 0.1673 - val_r_square: 0.6683\n",
            "Epoch 195/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.1456 - r_square: 0.6406\n",
            "Epoch 195: val_loss improved from 0.16733 to 0.16636, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1448 - r_square: 0.6316 - val_loss: 0.1664 - val_r_square: 0.6735\n",
            "Epoch 196/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1428 - r_square: 0.6361\n",
            "Epoch 196: val_loss improved from 0.16636 to 0.16531, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1437 - r_square: 0.6356 - val_loss: 0.1653 - val_r_square: 0.6748\n",
            "Epoch 197/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1426 - r_square: 0.6386\n",
            "Epoch 197: val_loss improved from 0.16531 to 0.16422, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1427 - r_square: 0.6366 - val_loss: 0.1642 - val_r_square: 0.6748\n",
            "Epoch 198/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.1398 - r_square: 0.6323\n",
            "Epoch 198: val_loss improved from 0.16422 to 0.16332, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1417 - r_square: 0.6376 - val_loss: 0.1633 - val_r_square: 0.6788\n",
            "Epoch 199/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.1378 - r_square: 0.6428\n",
            "Epoch 199: val_loss improved from 0.16332 to 0.16229, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1407 - r_square: 0.6390 - val_loss: 0.1623 - val_r_square: 0.6804\n",
            "Epoch 200/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.1410 - r_square: 0.6458\n",
            "Epoch 200: val_loss improved from 0.16229 to 0.16131, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1397 - r_square: 0.6411 - val_loss: 0.1613 - val_r_square: 0.6828\n",
            "Epoch 201/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.1379 - r_square: 0.6363\n",
            "Epoch 201: val_loss improved from 0.16131 to 0.16027, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1387 - r_square: 0.6428 - val_loss: 0.1603 - val_r_square: 0.6835\n",
            "Epoch 202/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.1402 - r_square: 0.6480\n",
            "Epoch 202: val_loss improved from 0.16027 to 0.15925, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1377 - r_square: 0.6434 - val_loss: 0.1592 - val_r_square: 0.6853\n",
            "Epoch 203/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.1363 - r_square: 0.6471\n",
            "Epoch 203: val_loss improved from 0.15925 to 0.15820, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1368 - r_square: 0.6466 - val_loss: 0.1582 - val_r_square: 0.6852\n",
            "Epoch 204/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.1349 - r_square: 0.6509\n",
            "Epoch 204: val_loss improved from 0.15820 to 0.15722, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1357 - r_square: 0.6484 - val_loss: 0.1572 - val_r_square: 0.6872\n",
            "Epoch 205/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.1330 - r_square: 0.6496\n",
            "Epoch 205: val_loss improved from 0.15722 to 0.15634, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1349 - r_square: 0.6477 - val_loss: 0.1563 - val_r_square: 0.6904\n",
            "Epoch 206/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.1362 - r_square: 0.6533\n",
            "Epoch 206: val_loss improved from 0.15634 to 0.15530, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.1339 - r_square: 0.6506 - val_loss: 0.1553 - val_r_square: 0.6879\n",
            "Epoch 207/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.1336 - r_square: 0.6505\n",
            "Epoch 207: val_loss improved from 0.15530 to 0.15453, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1329 - r_square: 0.6498 - val_loss: 0.1545 - val_r_square: 0.6941\n",
            "Epoch 208/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.1302 - r_square: 0.6664\n",
            "Epoch 208: val_loss improved from 0.15453 to 0.15349, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.1320 - r_square: 0.6530 - val_loss: 0.1535 - val_r_square: 0.6940\n",
            "Epoch 209/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.1291 - r_square: 0.6556\n",
            "Epoch 209: val_loss improved from 0.15349 to 0.15269, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1312 - r_square: 0.6540 - val_loss: 0.1527 - val_r_square: 0.6966\n",
            "Epoch 210/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.1307 - r_square: 0.6610\n",
            "Epoch 210: val_loss improved from 0.15269 to 0.15170, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1304 - r_square: 0.6555 - val_loss: 0.1517 - val_r_square: 0.6972\n",
            "Epoch 211/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.1278 - r_square: 0.6637\n",
            "Epoch 211: val_loss improved from 0.15170 to 0.15090, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.1294 - r_square: 0.6561 - val_loss: 0.1509 - val_r_square: 0.6991\n",
            "Epoch 212/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1278 - r_square: 0.6583\n",
            "Epoch 212: val_loss improved from 0.15090 to 0.14988, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1286 - r_square: 0.6590 - val_loss: 0.1499 - val_r_square: 0.6986\n",
            "Epoch 213/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1274 - r_square: 0.6567\n",
            "Epoch 213: val_loss improved from 0.14988 to 0.14905, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1277 - r_square: 0.6595 - val_loss: 0.1491 - val_r_square: 0.7007\n",
            "Epoch 214/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1256 - r_square: 0.6614\n",
            "Epoch 214: val_loss improved from 0.14905 to 0.14840, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1269 - r_square: 0.6604 - val_loss: 0.1484 - val_r_square: 0.7043\n",
            "Epoch 215/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.1261 - r_square: 0.6628\n",
            "Epoch 215: val_loss improved from 0.14840 to 0.14736, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1261 - r_square: 0.6628 - val_loss: 0.1474 - val_r_square: 0.7016\n",
            "Epoch 216/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.1253 - r_square: 0.6624\n",
            "Epoch 216: val_loss improved from 0.14736 to 0.14678, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1253 - r_square: 0.6624 - val_loss: 0.1468 - val_r_square: 0.7070\n",
            "Epoch 217/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.1245 - r_square: 0.6627\n",
            "Epoch 217: val_loss improved from 0.14678 to 0.14577, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1246 - r_square: 0.6652 - val_loss: 0.1458 - val_r_square: 0.7057\n",
            "Epoch 218/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.1232 - r_square: 0.6718\n",
            "Epoch 218: val_loss improved from 0.14577 to 0.14502, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1237 - r_square: 0.6661 - val_loss: 0.1450 - val_r_square: 0.7078\n",
            "Epoch 219/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1248 - r_square: 0.6641\n",
            "Epoch 219: val_loss improved from 0.14502 to 0.14415, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1229 - r_square: 0.6671 - val_loss: 0.1442 - val_r_square: 0.7068\n",
            "Epoch 220/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.1198 - r_square: 0.6635\n",
            "Epoch 220: val_loss improved from 0.14415 to 0.14345, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1221 - r_square: 0.6675 - val_loss: 0.1434 - val_r_square: 0.7108\n",
            "Epoch 221/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1221 - r_square: 0.6628\n",
            "Epoch 221: val_loss improved from 0.14345 to 0.14267, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.1214 - r_square: 0.6680 - val_loss: 0.1427 - val_r_square: 0.7108\n",
            "Epoch 222/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.1229 - r_square: 0.6692\n",
            "Epoch 222: val_loss improved from 0.14267 to 0.14189, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1206 - r_square: 0.6706 - val_loss: 0.1419 - val_r_square: 0.7118\n",
            "Epoch 223/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.1178 - r_square: 0.6871\n",
            "Epoch 223: val_loss improved from 0.14189 to 0.14119, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.1198 - r_square: 0.6722 - val_loss: 0.1412 - val_r_square: 0.7106\n",
            "Epoch 224/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1201 - r_square: 0.6695\n",
            "Epoch 224: val_loss improved from 0.14119 to 0.14057, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.1191 - r_square: 0.6716 - val_loss: 0.1406 - val_r_square: 0.7164\n",
            "Epoch 225/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1185 - r_square: 0.6819\n",
            "Epoch 225: val_loss improved from 0.14057 to 0.13977, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.1184 - r_square: 0.6747 - val_loss: 0.1398 - val_r_square: 0.7140\n",
            "Epoch 226/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1174 - r_square: 0.6772\n",
            "Epoch 226: val_loss improved from 0.13977 to 0.13906, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.1176 - r_square: 0.6747 - val_loss: 0.1391 - val_r_square: 0.7167\n",
            "Epoch 227/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.1158 - r_square: 0.6876\n",
            "Epoch 227: val_loss improved from 0.13906 to 0.13842, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1169 - r_square: 0.6756 - val_loss: 0.1384 - val_r_square: 0.7187\n",
            "Epoch 228/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.1166 - r_square: 0.6761\n",
            "Epoch 228: val_loss improved from 0.13842 to 0.13767, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1162 - r_square: 0.6774 - val_loss: 0.1377 - val_r_square: 0.7181\n",
            "Epoch 229/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.1157 - r_square: 0.6761\n",
            "Epoch 229: val_loss improved from 0.13767 to 0.13706, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.1155 - r_square: 0.6782 - val_loss: 0.1371 - val_r_square: 0.7201\n",
            "Epoch 230/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.1148 - r_square: 0.6785\n",
            "Epoch 230: val_loss improved from 0.13706 to 0.13646, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 8ms/step - loss: 0.1149 - r_square: 0.6797 - val_loss: 0.1365 - val_r_square: 0.7223\n",
            "Epoch 231/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.1111 - r_square: 0.6756\n",
            "Epoch 231: val_loss improved from 0.13646 to 0.13579, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1142 - r_square: 0.6798 - val_loss: 0.1358 - val_r_square: 0.7227\n",
            "Epoch 232/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.1147 - r_square: 0.6699\n",
            "Epoch 232: val_loss improved from 0.13579 to 0.13512, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1134 - r_square: 0.6807 - val_loss: 0.1351 - val_r_square: 0.7225\n",
            "Epoch 233/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1130 - r_square: 0.6792\n",
            "Epoch 233: val_loss improved from 0.13512 to 0.13467, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1128 - r_square: 0.6819 - val_loss: 0.1347 - val_r_square: 0.7266\n",
            "Epoch 234/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.1119 - r_square: 0.6767\n",
            "Epoch 234: val_loss improved from 0.13467 to 0.13395, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1123 - r_square: 0.6841 - val_loss: 0.1340 - val_r_square: 0.7244\n",
            "Epoch 235/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.1134 - r_square: 0.6794\n",
            "Epoch 235: val_loss improved from 0.13395 to 0.13393, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1116 - r_square: 0.6821 - val_loss: 0.1339 - val_r_square: 0.7316\n",
            "Epoch 236/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.1097 - r_square: 0.6945\n",
            "Epoch 236: val_loss improved from 0.13393 to 0.13278, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1110 - r_square: 0.6870 - val_loss: 0.1328 - val_r_square: 0.7281\n",
            "Epoch 237/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.1098 - r_square: 0.6919\n",
            "Epoch 237: val_loss improved from 0.13278 to 0.13218, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1103 - r_square: 0.6865 - val_loss: 0.1322 - val_r_square: 0.7280\n",
            "Epoch 238/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.1099 - r_square: 0.6864\n",
            "Epoch 238: val_loss improved from 0.13218 to 0.13166, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1099 - r_square: 0.6876 - val_loss: 0.1317 - val_r_square: 0.7283\n",
            "Epoch 239/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.1108 - r_square: 0.6735\n",
            "Epoch 239: val_loss improved from 0.13166 to 0.13115, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1091 - r_square: 0.6878 - val_loss: 0.1311 - val_r_square: 0.7319\n",
            "Epoch 240/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.1114 - r_square: 0.6902\n",
            "Epoch 240: val_loss improved from 0.13115 to 0.13058, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1087 - r_square: 0.6900 - val_loss: 0.1306 - val_r_square: 0.7318\n",
            "Epoch 241/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.1080 - r_square: 0.6904\n",
            "Epoch 241: val_loss improved from 0.13058 to 0.13012, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1080 - r_square: 0.6904 - val_loss: 0.1301 - val_r_square: 0.7336\n",
            "Epoch 242/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.1089 - r_square: 0.6946\n",
            "Epoch 242: val_loss improved from 0.13012 to 0.12968, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1074 - r_square: 0.6912 - val_loss: 0.1297 - val_r_square: 0.7363\n",
            "Epoch 243/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.1103 - r_square: 0.6921\n",
            "Epoch 243: val_loss improved from 0.12968 to 0.12902, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1068 - r_square: 0.6927 - val_loss: 0.1290 - val_r_square: 0.7355\n",
            "Epoch 244/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.1049 - r_square: 0.6977\n",
            "Epoch 244: val_loss improved from 0.12902 to 0.12860, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1064 - r_square: 0.6941 - val_loss: 0.1286 - val_r_square: 0.7374\n",
            "Epoch 245/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.1057 - r_square: 0.6844\n",
            "Epoch 245: val_loss improved from 0.12860 to 0.12830, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.1057 - r_square: 0.6944 - val_loss: 0.1283 - val_r_square: 0.7410\n",
            "Epoch 246/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.1040 - r_square: 0.6850\n",
            "Epoch 246: val_loss improved from 0.12830 to 0.12754, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1053 - r_square: 0.6958 - val_loss: 0.1275 - val_r_square: 0.7385\n",
            "Epoch 247/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.1047 - r_square: 0.6968\n",
            "Epoch 247: val_loss improved from 0.12754 to 0.12726, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1047 - r_square: 0.6968 - val_loss: 0.1273 - val_r_square: 0.7425\n",
            "Epoch 248/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1052 - r_square: 0.6958\n",
            "Epoch 248: val_loss improved from 0.12726 to 0.12670, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.1041 - r_square: 0.6973 - val_loss: 0.1267 - val_r_square: 0.7428\n",
            "Epoch 249/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.1016 - r_square: 0.7008\n",
            "Epoch 249: val_loss improved from 0.12670 to 0.12618, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1037 - r_square: 0.7007 - val_loss: 0.1262 - val_r_square: 0.7405\n",
            "Epoch 250/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.1039 - r_square: 0.7023\n",
            "Epoch 250: val_loss improved from 0.12618 to 0.12569, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1033 - r_square: 0.6986 - val_loss: 0.1257 - val_r_square: 0.7449\n",
            "Epoch 251/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.1013 - r_square: 0.7018\n",
            "Epoch 251: val_loss improved from 0.12569 to 0.12521, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1026 - r_square: 0.7019 - val_loss: 0.1252 - val_r_square: 0.7425\n",
            "Epoch 252/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.1026 - r_square: 0.7071\n",
            "Epoch 252: val_loss improved from 0.12521 to 0.12494, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1021 - r_square: 0.7020 - val_loss: 0.1249 - val_r_square: 0.7486\n",
            "Epoch 253/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.1018 - r_square: 0.7092\n",
            "Epoch 253: val_loss improved from 0.12494 to 0.12450, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1016 - r_square: 0.7030 - val_loss: 0.1245 - val_r_square: 0.7488\n",
            "Epoch 254/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.1016 - r_square: 0.7009\n",
            "Epoch 254: val_loss improved from 0.12450 to 0.12399, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1011 - r_square: 0.7047 - val_loss: 0.1240 - val_r_square: 0.7492\n",
            "Epoch 255/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1007 - r_square: 0.6991\n",
            "Epoch 255: val_loss improved from 0.12399 to 0.12362, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1006 - r_square: 0.7046 - val_loss: 0.1236 - val_r_square: 0.7506\n",
            "Epoch 256/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.1006 - r_square: 0.7064\n",
            "Epoch 256: val_loss improved from 0.12362 to 0.12309, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.1002 - r_square: 0.7074 - val_loss: 0.1231 - val_r_square: 0.7495\n",
            "Epoch 257/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0989 - r_square: 0.7049\n",
            "Epoch 257: val_loss improved from 0.12309 to 0.12266, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0997 - r_square: 0.7075 - val_loss: 0.1227 - val_r_square: 0.7501\n",
            "Epoch 258/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0996 - r_square: 0.7090\n",
            "Epoch 258: val_loss improved from 0.12266 to 0.12228, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0992 - r_square: 0.7088 - val_loss: 0.1223 - val_r_square: 0.7530\n",
            "Epoch 259/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0996 - r_square: 0.7057\n",
            "Epoch 259: val_loss improved from 0.12228 to 0.12208, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0989 - r_square: 0.7106 - val_loss: 0.1221 - val_r_square: 0.7563\n",
            "Epoch 260/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0989 - r_square: 0.7102\n",
            "Epoch 260: val_loss improved from 0.12208 to 0.12159, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0984 - r_square: 0.7114 - val_loss: 0.1216 - val_r_square: 0.7511\n",
            "Epoch 261/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0978 - r_square: 0.7089\n",
            "Epoch 261: val_loss improved from 0.12159 to 0.12127, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0979 - r_square: 0.7106 - val_loss: 0.1213 - val_r_square: 0.7583\n",
            "Epoch 262/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0999 - r_square: 0.7098\n",
            "Epoch 262: val_loss improved from 0.12127 to 0.12075, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0978 - r_square: 0.7150 - val_loss: 0.1207 - val_r_square: 0.7571\n",
            "Epoch 263/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0971 - r_square: 0.7134\n",
            "Epoch 263: val_loss improved from 0.12075 to 0.12032, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0970 - r_square: 0.7141 - val_loss: 0.1203 - val_r_square: 0.7560\n",
            "Epoch 264/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0960 - r_square: 0.7249\n",
            "Epoch 264: val_loss improved from 0.12032 to 0.12005, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0966 - r_square: 0.7150 - val_loss: 0.1201 - val_r_square: 0.7613\n",
            "Epoch 265/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0944 - r_square: 0.7208\n",
            "Epoch 265: val_loss improved from 0.12005 to 0.11958, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0962 - r_square: 0.7157 - val_loss: 0.1196 - val_r_square: 0.7595\n",
            "Epoch 266/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0961 - r_square: 0.7178\n",
            "Epoch 266: val_loss improved from 0.11958 to 0.11928, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0958 - r_square: 0.7180 - val_loss: 0.1193 - val_r_square: 0.7634\n",
            "Epoch 267/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0953 - r_square: 0.7232\n",
            "Epoch 267: val_loss improved from 0.11928 to 0.11884, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0955 - r_square: 0.7215 - val_loss: 0.1188 - val_r_square: 0.7615\n",
            "Epoch 268/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0957 - r_square: 0.7230\n",
            "Epoch 268: val_loss improved from 0.11884 to 0.11846, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0951 - r_square: 0.7204 - val_loss: 0.1185 - val_r_square: 0.7638\n",
            "Epoch 269/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0940 - r_square: 0.7251\n",
            "Epoch 269: val_loss improved from 0.11846 to 0.11815, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0946 - r_square: 0.7208 - val_loss: 0.1181 - val_r_square: 0.7664\n",
            "Epoch 270/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0955 - r_square: 0.7195\n",
            "Epoch 270: val_loss improved from 0.11815 to 0.11777, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0943 - r_square: 0.7210 - val_loss: 0.1178 - val_r_square: 0.7677\n",
            "Epoch 271/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0931 - r_square: 0.7142\n",
            "Epoch 271: val_loss improved from 0.11777 to 0.11742, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0938 - r_square: 0.7243 - val_loss: 0.1174 - val_r_square: 0.7675\n",
            "Epoch 272/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0938 - r_square: 0.7228\n",
            "Epoch 272: val_loss improved from 0.11742 to 0.11715, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0934 - r_square: 0.7252 - val_loss: 0.1172 - val_r_square: 0.7700\n",
            "Epoch 273/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0935 - r_square: 0.7166\n",
            "Epoch 273: val_loss improved from 0.11715 to 0.11675, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0931 - r_square: 0.7266 - val_loss: 0.1168 - val_r_square: 0.7702\n",
            "Epoch 274/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0934 - r_square: 0.7282\n",
            "Epoch 274: val_loss improved from 0.11675 to 0.11650, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0928 - r_square: 0.7275 - val_loss: 0.1165 - val_r_square: 0.7735\n",
            "Epoch 275/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0921 - r_square: 0.7303\n",
            "Epoch 275: val_loss improved from 0.11650 to 0.11611, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0924 - r_square: 0.7275 - val_loss: 0.1161 - val_r_square: 0.7714\n",
            "Epoch 276/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0925 - r_square: 0.7290\n",
            "Epoch 276: val_loss improved from 0.11611 to 0.11572, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0920 - r_square: 0.7305 - val_loss: 0.1157 - val_r_square: 0.7739\n",
            "Epoch 277/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0917 - r_square: 0.7307\n",
            "Epoch 277: val_loss improved from 0.11572 to 0.11548, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0917 - r_square: 0.7317 - val_loss: 0.1155 - val_r_square: 0.7767\n",
            "Epoch 278/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0904 - r_square: 0.7391\n",
            "Epoch 278: val_loss improved from 0.11548 to 0.11541, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0914 - r_square: 0.7317 - val_loss: 0.1154 - val_r_square: 0.7789\n",
            "Epoch 279/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0912 - r_square: 0.7348\n",
            "Epoch 279: val_loss improved from 0.11541 to 0.11480, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0910 - r_square: 0.7322 - val_loss: 0.1148 - val_r_square: 0.7770\n",
            "Epoch 280/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0907 - r_square: 0.7349\n",
            "Epoch 280: val_loss improved from 0.11480 to 0.11469, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0907 - r_square: 0.7349 - val_loss: 0.1147 - val_r_square: 0.7760\n",
            "Epoch 281/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0898 - r_square: 0.7356\n",
            "Epoch 281: val_loss improved from 0.11469 to 0.11424, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0903 - r_square: 0.7351 - val_loss: 0.1142 - val_r_square: 0.7802\n",
            "Epoch 282/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0893 - r_square: 0.7392\n",
            "Epoch 282: val_loss improved from 0.11424 to 0.11390, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0900 - r_square: 0.7369 - val_loss: 0.1139 - val_r_square: 0.7816\n",
            "Epoch 283/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0898 - r_square: 0.7360\n",
            "Epoch 283: val_loss improved from 0.11390 to 0.11386, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0897 - r_square: 0.7393 - val_loss: 0.1139 - val_r_square: 0.7792\n",
            "Epoch 284/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0895 - r_square: 0.7379\n",
            "Epoch 284: val_loss improved from 0.11386 to 0.11356, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0893 - r_square: 0.7387 - val_loss: 0.1136 - val_r_square: 0.7815\n",
            "Epoch 285/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0896 - r_square: 0.7395\n",
            "Epoch 285: val_loss improved from 0.11356 to 0.11317, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0891 - r_square: 0.7408 - val_loss: 0.1132 - val_r_square: 0.7836\n",
            "Epoch 286/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0880 - r_square: 0.7422\n",
            "Epoch 286: val_loss did not improve from 0.11317\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0886 - r_square: 0.7411 - val_loss: 0.1142 - val_r_square: 0.7933\n",
            "Epoch 287/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0883 - r_square: 0.7456\n",
            "Epoch 287: val_loss improved from 0.11317 to 0.11285, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0886 - r_square: 0.7442 - val_loss: 0.1129 - val_r_square: 0.7913\n",
            "Epoch 288/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0881 - r_square: 0.7436\n",
            "Epoch 288: val_loss improved from 0.11285 to 0.11218, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0883 - r_square: 0.7435 - val_loss: 0.1122 - val_r_square: 0.7897\n",
            "Epoch 289/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0877 - r_square: 0.7469\n",
            "Epoch 289: val_loss improved from 0.11218 to 0.11200, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0878 - r_square: 0.7462 - val_loss: 0.1120 - val_r_square: 0.7890\n",
            "Epoch 290/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0877 - r_square: 0.7454\n",
            "Epoch 290: val_loss improved from 0.11200 to 0.11174, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0875 - r_square: 0.7456 - val_loss: 0.1117 - val_r_square: 0.7925\n",
            "Epoch 291/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0874 - r_square: 0.7430\n",
            "Epoch 291: val_loss improved from 0.11174 to 0.11148, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0872 - r_square: 0.7477 - val_loss: 0.1115 - val_r_square: 0.7914\n",
            "Epoch 292/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0857 - r_square: 0.7494\n",
            "Epoch 292: val_loss improved from 0.11148 to 0.11147, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0870 - r_square: 0.7492 - val_loss: 0.1115 - val_r_square: 0.7897\n",
            "Epoch 293/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0868 - r_square: 0.7507\n",
            "Epoch 293: val_loss improved from 0.11147 to 0.11097, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0867 - r_square: 0.7484 - val_loss: 0.1110 - val_r_square: 0.7951\n",
            "Epoch 294/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0860 - r_square: 0.7525\n",
            "Epoch 294: val_loss improved from 0.11097 to 0.11084, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0865 - r_square: 0.7523 - val_loss: 0.1108 - val_r_square: 0.7933\n",
            "Epoch 295/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0872 - r_square: 0.7572\n",
            "Epoch 295: val_loss improved from 0.11084 to 0.11044, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0860 - r_square: 0.7514 - val_loss: 0.1104 - val_r_square: 0.7960\n",
            "Epoch 296/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0861 - r_square: 0.7529\n",
            "Epoch 296: val_loss improved from 0.11044 to 0.11018, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0859 - r_square: 0.7536 - val_loss: 0.1102 - val_r_square: 0.7984\n",
            "Epoch 297/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0866 - r_square: 0.7507\n",
            "Epoch 297: val_loss did not improve from 0.11018\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0861 - r_square: 0.7522 - val_loss: 0.1106 - val_r_square: 0.8034\n",
            "Epoch 298/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0858 - r_square: 0.7541\n",
            "Epoch 298: val_loss improved from 0.11018 to 0.10969, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0853 - r_square: 0.7566 - val_loss: 0.1097 - val_r_square: 0.7999\n",
            "Epoch 299/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0857 - r_square: 0.7545\n",
            "Epoch 299: val_loss improved from 0.10969 to 0.10949, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0852 - r_square: 0.7559 - val_loss: 0.1095 - val_r_square: 0.8023\n",
            "Epoch 300/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0840 - r_square: 0.7605\n",
            "Epoch 300: val_loss improved from 0.10949 to 0.10937, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0848 - r_square: 0.7573 - val_loss: 0.1094 - val_r_square: 0.8049\n",
            "Epoch 301/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0838 - r_square: 0.7553\n",
            "Epoch 301: val_loss did not improve from 0.10937\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0846 - r_square: 0.7588 - val_loss: 0.1095 - val_r_square: 0.7996\n",
            "Epoch 302/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0842 - r_square: 0.7593\n",
            "Epoch 302: val_loss improved from 0.10937 to 0.10885, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0842 - r_square: 0.7593 - val_loss: 0.1088 - val_r_square: 0.8039\n",
            "Epoch 303/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0825 - r_square: 0.7636\n",
            "Epoch 303: val_loss improved from 0.10885 to 0.10869, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0840 - r_square: 0.7608 - val_loss: 0.1087 - val_r_square: 0.8082\n",
            "Epoch 304/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0833 - r_square: 0.7612\n",
            "Epoch 304: val_loss improved from 0.10869 to 0.10842, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0836 - r_square: 0.7619 - val_loss: 0.1084 - val_r_square: 0.8070\n",
            "Epoch 305/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0839 - r_square: 0.7633\n",
            "Epoch 305: val_loss improved from 0.10842 to 0.10832, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0835 - r_square: 0.7628 - val_loss: 0.1083 - val_r_square: 0.8108\n",
            "Epoch 306/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0825 - r_square: 0.7616\n",
            "Epoch 306: val_loss improved from 0.10832 to 0.10812, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0832 - r_square: 0.7645 - val_loss: 0.1081 - val_r_square: 0.8071\n",
            "Epoch 307/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0827 - r_square: 0.7718\n",
            "Epoch 307: val_loss did not improve from 0.10812\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0830 - r_square: 0.7652 - val_loss: 0.1084 - val_r_square: 0.8052\n",
            "Epoch 308/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0828 - r_square: 0.7663\n",
            "Epoch 308: val_loss did not improve from 0.10812\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0828 - r_square: 0.7663 - val_loss: 0.1083 - val_r_square: 0.8167\n",
            "Epoch 309/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0825 - r_square: 0.7797\n",
            "Epoch 309: val_loss improved from 0.10812 to 0.10753, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0826 - r_square: 0.7678 - val_loss: 0.1075 - val_r_square: 0.8163\n",
            "Epoch 310/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0827 - r_square: 0.7626\n",
            "Epoch 310: val_loss improved from 0.10753 to 0.10741, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0822 - r_square: 0.7695 - val_loss: 0.1074 - val_r_square: 0.8114\n",
            "Epoch 311/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0818 - r_square: 0.7682\n",
            "Epoch 311: val_loss improved from 0.10741 to 0.10694, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0820 - r_square: 0.7702 - val_loss: 0.1069 - val_r_square: 0.8149\n",
            "Epoch 312/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0799 - r_square: 0.7836\n",
            "Epoch 312: val_loss improved from 0.10694 to 0.10690, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0820 - r_square: 0.7714 - val_loss: 0.1069 - val_r_square: 0.8186\n",
            "Epoch 313/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0814 - r_square: 0.7843\n",
            "Epoch 313: val_loss improved from 0.10690 to 0.10679, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0817 - r_square: 0.7725 - val_loss: 0.1068 - val_r_square: 0.8143\n",
            "Epoch 314/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0813 - r_square: 0.7748\n",
            "Epoch 314: val_loss improved from 0.10679 to 0.10640, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0814 - r_square: 0.7722 - val_loss: 0.1064 - val_r_square: 0.8206\n",
            "Epoch 315/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0812 - r_square: 0.7784\n",
            "Epoch 315: val_loss improved from 0.10640 to 0.10619, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0810 - r_square: 0.7746 - val_loss: 0.1062 - val_r_square: 0.8208\n",
            "Epoch 316/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0817 - r_square: 0.7786\n",
            "Epoch 316: val_loss did not improve from 0.10619\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0810 - r_square: 0.7761 - val_loss: 0.1062 - val_r_square: 0.8184\n",
            "Epoch 317/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0801 - r_square: 0.7766\n",
            "Epoch 317: val_loss improved from 0.10619 to 0.10593, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0808 - r_square: 0.7771 - val_loss: 0.1059 - val_r_square: 0.8197\n",
            "Epoch 318/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0804 - r_square: 0.7774\n",
            "Epoch 318: val_loss improved from 0.10593 to 0.10570, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0806 - r_square: 0.7792 - val_loss: 0.1057 - val_r_square: 0.8212\n",
            "Epoch 319/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0807 - r_square: 0.7758\n",
            "Epoch 319: val_loss improved from 0.10570 to 0.10546, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0802 - r_square: 0.7779 - val_loss: 0.1055 - val_r_square: 0.8248\n",
            "Epoch 320/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0800 - r_square: 0.7844\n",
            "Epoch 320: val_loss improved from 0.10546 to 0.10524, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0801 - r_square: 0.7794 - val_loss: 0.1052 - val_r_square: 0.8251\n",
            "Epoch 321/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0807 - r_square: 0.7799\n",
            "Epoch 321: val_loss improved from 0.10524 to 0.10505, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0798 - r_square: 0.7816 - val_loss: 0.1051 - val_r_square: 0.8271\n",
            "Epoch 322/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0794 - r_square: 0.7799\n",
            "Epoch 322: val_loss improved from 0.10505 to 0.10491, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0796 - r_square: 0.7811 - val_loss: 0.1049 - val_r_square: 0.8284\n",
            "Epoch 323/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0794 - r_square: 0.7809\n",
            "Epoch 323: val_loss did not improve from 0.10491\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0794 - r_square: 0.7822 - val_loss: 0.1052 - val_r_square: 0.8323\n",
            "Epoch 324/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0803 - r_square: 0.7862\n",
            "Epoch 324: val_loss did not improve from 0.10491\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0792 - r_square: 0.7853 - val_loss: 0.1051 - val_r_square: 0.8264\n",
            "Epoch 325/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0776 - r_square: 0.7777\n",
            "Epoch 325: val_loss improved from 0.10491 to 0.10452, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0790 - r_square: 0.7843 - val_loss: 0.1045 - val_r_square: 0.8327\n",
            "Epoch 326/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0785 - r_square: 0.7866\n",
            "Epoch 326: val_loss improved from 0.10452 to 0.10425, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0788 - r_square: 0.7872 - val_loss: 0.1042 - val_r_square: 0.8335\n",
            "Epoch 327/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0791 - r_square: 0.7795\n",
            "Epoch 327: val_loss improved from 0.10425 to 0.10397, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0787 - r_square: 0.7870 - val_loss: 0.1040 - val_r_square: 0.8316\n",
            "Epoch 328/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0783 - r_square: 0.7866\n",
            "Epoch 328: val_loss improved from 0.10397 to 0.10383, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0784 - r_square: 0.7880 - val_loss: 0.1038 - val_r_square: 0.8338\n",
            "Epoch 329/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0778 - r_square: 0.8036\n",
            "Epoch 329: val_loss improved from 0.10383 to 0.10373, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0782 - r_square: 0.7883 - val_loss: 0.1037 - val_r_square: 0.8347\n",
            "Epoch 330/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0773 - r_square: 0.7893\n",
            "Epoch 330: val_loss improved from 0.10373 to 0.10359, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0780 - r_square: 0.7902 - val_loss: 0.1036 - val_r_square: 0.8374\n",
            "Epoch 331/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0775 - r_square: 0.7886\n",
            "Epoch 331: val_loss improved from 0.10359 to 0.10344, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0778 - r_square: 0.7922 - val_loss: 0.1034 - val_r_square: 0.8382\n",
            "Epoch 332/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0768 - r_square: 0.7891\n",
            "Epoch 332: val_loss improved from 0.10344 to 0.10327, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0776 - r_square: 0.7922 - val_loss: 0.1033 - val_r_square: 0.8388\n",
            "Epoch 333/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0789 - r_square: 0.7870\n",
            "Epoch 333: val_loss improved from 0.10327 to 0.10310, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0774 - r_square: 0.7931 - val_loss: 0.1031 - val_r_square: 0.8393\n",
            "Epoch 334/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0773 - r_square: 0.7949\n",
            "Epoch 334: val_loss improved from 0.10310 to 0.10292, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0773 - r_square: 0.7949 - val_loss: 0.1029 - val_r_square: 0.8392\n",
            "Epoch 335/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0776 - r_square: 0.7938\n",
            "Epoch 335: val_loss improved from 0.10292 to 0.10283, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0771 - r_square: 0.7957 - val_loss: 0.1028 - val_r_square: 0.8420\n",
            "Epoch 336/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0763 - r_square: 0.8078\n",
            "Epoch 336: val_loss improved from 0.10283 to 0.10263, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0768 - r_square: 0.7963 - val_loss: 0.1026 - val_r_square: 0.8425\n",
            "Epoch 337/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0763 - r_square: 0.7956\n",
            "Epoch 337: val_loss did not improve from 0.10263\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0767 - r_square: 0.7974 - val_loss: 0.1028 - val_r_square: 0.8445\n",
            "Epoch 338/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0766 - r_square: 0.7928\n",
            "Epoch 338: val_loss improved from 0.10263 to 0.10242, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0765 - r_square: 0.7984 - val_loss: 0.1024 - val_r_square: 0.8431\n",
            "Epoch 339/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0764 - r_square: 0.8017\n",
            "Epoch 339: val_loss improved from 0.10242 to 0.10228, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0764 - r_square: 0.7990 - val_loss: 0.1023 - val_r_square: 0.8429\n",
            "Epoch 340/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0774 - r_square: 0.8010\n",
            "Epoch 340: val_loss improved from 0.10228 to 0.10205, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0763 - r_square: 0.8000 - val_loss: 0.1021 - val_r_square: 0.8455\n",
            "Epoch 341/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0757 - r_square: 0.8083\n",
            "Epoch 341: val_loss did not improve from 0.10205\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0761 - r_square: 0.8006 - val_loss: 0.1022 - val_r_square: 0.8486\n",
            "Epoch 342/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0733 - r_square: 0.8218\n",
            "Epoch 342: val_loss improved from 0.10205 to 0.10178, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0759 - r_square: 0.8031 - val_loss: 0.1018 - val_r_square: 0.8475\n",
            "Epoch 343/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0763 - r_square: 0.8032\n",
            "Epoch 343: val_loss did not improve from 0.10178\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0757 - r_square: 0.8023 - val_loss: 0.1019 - val_r_square: 0.8452\n",
            "Epoch 344/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0756 - r_square: 0.7930\n",
            "Epoch 344: val_loss improved from 0.10178 to 0.10155, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0756 - r_square: 0.8039 - val_loss: 0.1015 - val_r_square: 0.8479\n",
            "Epoch 345/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0741 - r_square: 0.8154\n",
            "Epoch 345: val_loss did not improve from 0.10155\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0754 - r_square: 0.8043 - val_loss: 0.1023 - val_r_square: 0.8534\n",
            "Epoch 346/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0750 - r_square: 0.8082\n",
            "Epoch 346: val_loss improved from 0.10155 to 0.10154, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0751 - r_square: 0.8059 - val_loss: 0.1015 - val_r_square: 0.8479\n",
            "Epoch 347/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0766 - r_square: 0.7999\n",
            "Epoch 347: val_loss improved from 0.10154 to 0.10108, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0751 - r_square: 0.8063 - val_loss: 0.1011 - val_r_square: 0.8521\n",
            "Epoch 348/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0751 - r_square: 0.8097\n",
            "Epoch 348: val_loss did not improve from 0.10108\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0748 - r_square: 0.8071 - val_loss: 0.1012 - val_r_square: 0.8547\n",
            "Epoch 349/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0725 - r_square: 0.8009\n",
            "Epoch 349: val_loss improved from 0.10108 to 0.10096, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0747 - r_square: 0.8081 - val_loss: 0.1010 - val_r_square: 0.8517\n",
            "Epoch 350/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0752 - r_square: 0.8124\n",
            "Epoch 350: val_loss improved from 0.10096 to 0.10075, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0746 - r_square: 0.8094 - val_loss: 0.1008 - val_r_square: 0.8550\n",
            "Epoch 351/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0739 - r_square: 0.8122\n",
            "Epoch 351: val_loss improved from 0.10075 to 0.10057, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0744 - r_square: 0.8099 - val_loss: 0.1006 - val_r_square: 0.8548\n",
            "Epoch 352/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0745 - r_square: 0.8125\n",
            "Epoch 352: val_loss did not improve from 0.10057\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0742 - r_square: 0.8106 - val_loss: 0.1009 - val_r_square: 0.8589\n",
            "Epoch 353/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0743 - r_square: 0.8111\n",
            "Epoch 353: val_loss improved from 0.10057 to 0.10040, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0743 - r_square: 0.8111 - val_loss: 0.1004 - val_r_square: 0.8584\n",
            "Epoch 354/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0759 - r_square: 0.8071\n",
            "Epoch 354: val_loss improved from 0.10040 to 0.10025, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0742 - r_square: 0.8115 - val_loss: 0.1002 - val_r_square: 0.8564\n",
            "Epoch 355/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0745 - r_square: 0.8098\n",
            "Epoch 355: val_loss improved from 0.10025 to 0.10023, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0739 - r_square: 0.8120 - val_loss: 0.1002 - val_r_square: 0.8592\n",
            "Epoch 356/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0742 - r_square: 0.8114\n",
            "Epoch 356: val_loss did not improve from 0.10023\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0736 - r_square: 0.8141 - val_loss: 0.1003 - val_r_square: 0.8616\n",
            "Epoch 357/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0734 - r_square: 0.8118\n",
            "Epoch 357: val_loss improved from 0.10023 to 0.09989, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0734 - r_square: 0.8155 - val_loss: 0.0999 - val_r_square: 0.8612\n",
            "Epoch 358/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0730 - r_square: 0.8131\n",
            "Epoch 358: val_loss improved from 0.09989 to 0.09972, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0733 - r_square: 0.8157 - val_loss: 0.0997 - val_r_square: 0.8602\n",
            "Epoch 359/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0730 - r_square: 0.8157\n",
            "Epoch 359: val_loss improved from 0.09972 to 0.09961, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0732 - r_square: 0.8172 - val_loss: 0.0996 - val_r_square: 0.8614\n",
            "Epoch 360/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0733 - r_square: 0.8169\n",
            "Epoch 360: val_loss improved from 0.09961 to 0.09957, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0731 - r_square: 0.8170 - val_loss: 0.0996 - val_r_square: 0.8635\n",
            "Epoch 361/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0729 - r_square: 0.8271\n",
            "Epoch 361: val_loss improved from 0.09957 to 0.09950, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0731 - r_square: 0.8180 - val_loss: 0.0995 - val_r_square: 0.8615\n",
            "Epoch 362/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0725 - r_square: 0.8181\n",
            "Epoch 362: val_loss improved from 0.09950 to 0.09917, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0729 - r_square: 0.8196 - val_loss: 0.0992 - val_r_square: 0.8634\n",
            "Epoch 363/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0735 - r_square: 0.8160\n",
            "Epoch 363: val_loss did not improve from 0.09917\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0729 - r_square: 0.8190 - val_loss: 0.0993 - val_r_square: 0.8634\n",
            "Epoch 364/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0714 - r_square: 0.8179\n",
            "Epoch 364: val_loss improved from 0.09917 to 0.09897, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0726 - r_square: 0.8214 - val_loss: 0.0990 - val_r_square: 0.8653\n",
            "Epoch 365/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0729 - r_square: 0.8333\n",
            "Epoch 365: val_loss improved from 0.09897 to 0.09886, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0724 - r_square: 0.8215 - val_loss: 0.0989 - val_r_square: 0.8659\n",
            "Epoch 366/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0719 - r_square: 0.8193\n",
            "Epoch 366: val_loss did not improve from 0.09886\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0722 - r_square: 0.8219 - val_loss: 0.0989 - val_r_square: 0.8666\n",
            "Epoch 367/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0721 - r_square: 0.8258\n",
            "Epoch 367: val_loss improved from 0.09886 to 0.09873, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0722 - r_square: 0.8233 - val_loss: 0.0987 - val_r_square: 0.8663\n",
            "Epoch 368/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0722 - r_square: 0.8172\n",
            "Epoch 368: val_loss did not improve from 0.09873\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0720 - r_square: 0.8227 - val_loss: 0.0991 - val_r_square: 0.8706\n",
            "Epoch 369/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0719 - r_square: 0.8242\n",
            "Epoch 369: val_loss improved from 0.09873 to 0.09850, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0719 - r_square: 0.8242 - val_loss: 0.0985 - val_r_square: 0.8700\n",
            "Epoch 370/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0719 - r_square: 0.8258\n",
            "Epoch 370: val_loss improved from 0.09850 to 0.09836, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0718 - r_square: 0.8252 - val_loss: 0.0984 - val_r_square: 0.8699\n",
            "Epoch 371/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0704 - r_square: 0.8249\n",
            "Epoch 371: val_loss improved from 0.09836 to 0.09822, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0716 - r_square: 0.8260 - val_loss: 0.0982 - val_r_square: 0.8703\n",
            "Epoch 372/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0705 - r_square: 0.8222\n",
            "Epoch 372: val_loss improved from 0.09822 to 0.09820, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0715 - r_square: 0.8261 - val_loss: 0.0982 - val_r_square: 0.8711\n",
            "Epoch 373/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0710 - r_square: 0.8276\n",
            "Epoch 373: val_loss improved from 0.09820 to 0.09804, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0713 - r_square: 0.8282 - val_loss: 0.0980 - val_r_square: 0.8720\n",
            "Epoch 374/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0713 - r_square: 0.8316\n",
            "Epoch 374: val_loss improved from 0.09804 to 0.09792, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0713 - r_square: 0.8275 - val_loss: 0.0979 - val_r_square: 0.8725\n",
            "Epoch 375/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0721 - r_square: 0.8289\n",
            "Epoch 375: val_loss did not improve from 0.09792\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0713 - r_square: 0.8269 - val_loss: 0.0981 - val_r_square: 0.8739\n",
            "Epoch 376/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0711 - r_square: 0.8229\n",
            "Epoch 376: val_loss improved from 0.09792 to 0.09769, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0710 - r_square: 0.8294 - val_loss: 0.0977 - val_r_square: 0.8740\n",
            "Epoch 377/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0710 - r_square: 0.8263\n",
            "Epoch 377: val_loss improved from 0.09769 to 0.09755, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0709 - r_square: 0.8291 - val_loss: 0.0976 - val_r_square: 0.8751\n",
            "Epoch 378/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0709 - r_square: 0.8251\n",
            "Epoch 378: val_loss improved from 0.09755 to 0.09751, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0708 - r_square: 0.8305 - val_loss: 0.0975 - val_r_square: 0.8742\n",
            "Epoch 379/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0705 - r_square: 0.8286\n",
            "Epoch 379: val_loss did not improve from 0.09751\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0707 - r_square: 0.8307 - val_loss: 0.0979 - val_r_square: 0.8779\n",
            "Epoch 380/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0706 - r_square: 0.8312\n",
            "Epoch 380: val_loss improved from 0.09751 to 0.09746, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0706 - r_square: 0.8312 - val_loss: 0.0975 - val_r_square: 0.8776\n",
            "Epoch 381/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0704 - r_square: 0.8332\n",
            "Epoch 381: val_loss improved from 0.09746 to 0.09737, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0703 - r_square: 0.8322 - val_loss: 0.0974 - val_r_square: 0.8775\n",
            "Epoch 382/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0702 - r_square: 0.8331\n",
            "Epoch 382: val_loss improved from 0.09737 to 0.09717, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0702 - r_square: 0.8331 - val_loss: 0.0972 - val_r_square: 0.8756\n",
            "Epoch 383/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0702 - r_square: 0.8267\n",
            "Epoch 383: val_loss did not improve from 0.09717\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0703 - r_square: 0.8329 - val_loss: 0.0978 - val_r_square: 0.8809\n",
            "Epoch 384/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0678 - r_square: 0.8311\n",
            "Epoch 384: val_loss improved from 0.09717 to 0.09706, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0702 - r_square: 0.8344 - val_loss: 0.0971 - val_r_square: 0.8792\n",
            "Epoch 385/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0707 - r_square: 0.8320\n",
            "Epoch 385: val_loss improved from 0.09706 to 0.09680, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0700 - r_square: 0.8350 - val_loss: 0.0968 - val_r_square: 0.8786\n",
            "Epoch 386/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0698 - r_square: 0.8363\n",
            "Epoch 386: val_loss improved from 0.09680 to 0.09673, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0698 - r_square: 0.8354 - val_loss: 0.0967 - val_r_square: 0.8791\n",
            "Epoch 387/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0697 - r_square: 0.8354\n",
            "Epoch 387: val_loss did not improve from 0.09673\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0697 - r_square: 0.8354 - val_loss: 0.0968 - val_r_square: 0.8783\n",
            "Epoch 388/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0712 - r_square: 0.8239\n",
            "Epoch 388: val_loss improved from 0.09673 to 0.09651, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0698 - r_square: 0.8357 - val_loss: 0.0965 - val_r_square: 0.8811\n",
            "Epoch 389/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0697 - r_square: 0.8331\n",
            "Epoch 389: val_loss improved from 0.09651 to 0.09651, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0697 - r_square: 0.8372 - val_loss: 0.0965 - val_r_square: 0.8800\n",
            "Epoch 390/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0687 - r_square: 0.8506\n",
            "Epoch 390: val_loss improved from 0.09651 to 0.09640, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0694 - r_square: 0.8369 - val_loss: 0.0964 - val_r_square: 0.8832\n",
            "Epoch 391/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0688 - r_square: 0.8413\n",
            "Epoch 391: val_loss did not improve from 0.09640\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0692 - r_square: 0.8390 - val_loss: 0.0968 - val_r_square: 0.8844\n",
            "Epoch 392/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0699 - r_square: 0.8415\n",
            "Epoch 392: val_loss improved from 0.09640 to 0.09618, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0692 - r_square: 0.8377 - val_loss: 0.0962 - val_r_square: 0.8826\n",
            "Epoch 393/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0689 - r_square: 0.8392\n",
            "Epoch 393: val_loss did not improve from 0.09618\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0690 - r_square: 0.8393 - val_loss: 0.0962 - val_r_square: 0.8849\n",
            "Epoch 394/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0680 - r_square: 0.8419\n",
            "Epoch 394: val_loss improved from 0.09618 to 0.09585, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0689 - r_square: 0.8404 - val_loss: 0.0959 - val_r_square: 0.8842\n",
            "Epoch 395/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0683 - r_square: 0.8413\n",
            "Epoch 395: val_loss improved from 0.09585 to 0.09576, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0689 - r_square: 0.8404 - val_loss: 0.0958 - val_r_square: 0.8838\n",
            "Epoch 396/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0670 - r_square: 0.8395\n",
            "Epoch 396: val_loss did not improve from 0.09576\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0687 - r_square: 0.8416 - val_loss: 0.0958 - val_r_square: 0.8864\n",
            "Epoch 397/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0674 - r_square: 0.8468\n",
            "Epoch 397: val_loss did not improve from 0.09576\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0684 - r_square: 0.8409 - val_loss: 0.0963 - val_r_square: 0.8823\n",
            "Epoch 398/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0685 - r_square: 0.8436\n",
            "Epoch 398: val_loss improved from 0.09576 to 0.09571, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0685 - r_square: 0.8416 - val_loss: 0.0957 - val_r_square: 0.8870\n",
            "Epoch 399/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0686 - r_square: 0.8445\n",
            "Epoch 399: val_loss improved from 0.09571 to 0.09545, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0685 - r_square: 0.8429 - val_loss: 0.0954 - val_r_square: 0.8861\n",
            "Epoch 400/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0684 - r_square: 0.8423\n",
            "Epoch 400: val_loss did not improve from 0.09545\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0682 - r_square: 0.8434 - val_loss: 0.0955 - val_r_square: 0.8878\n",
            "Epoch 401/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0685 - r_square: 0.8399\n",
            "Epoch 401: val_loss improved from 0.09545 to 0.09540, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0682 - r_square: 0.8431 - val_loss: 0.0954 - val_r_square: 0.8892\n",
            "Epoch 402/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0683 - r_square: 0.8486\n",
            "Epoch 402: val_loss improved from 0.09540 to 0.09512, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0683 - r_square: 0.8453 - val_loss: 0.0951 - val_r_square: 0.8879\n",
            "Epoch 403/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0677 - r_square: 0.8408\n",
            "Epoch 403: val_loss did not improve from 0.09512\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0680 - r_square: 0.8446 - val_loss: 0.0951 - val_r_square: 0.8889\n",
            "Epoch 404/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0673 - r_square: 0.8327\n",
            "Epoch 404: val_loss improved from 0.09512 to 0.09500, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0679 - r_square: 0.8451 - val_loss: 0.0950 - val_r_square: 0.8884\n",
            "Epoch 405/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0682 - r_square: 0.8488\n",
            "Epoch 405: val_loss improved from 0.09500 to 0.09494, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0679 - r_square: 0.8454 - val_loss: 0.0949 - val_r_square: 0.8903\n",
            "Epoch 406/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0679 - r_square: 0.8448\n",
            "Epoch 406: val_loss improved from 0.09494 to 0.09478, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0677 - r_square: 0.8467 - val_loss: 0.0948 - val_r_square: 0.8896\n",
            "Epoch 407/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0678 - r_square: 0.8438\n",
            "Epoch 407: val_loss improved from 0.09478 to 0.09467, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0675 - r_square: 0.8473 - val_loss: 0.0947 - val_r_square: 0.8898\n",
            "Epoch 408/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0689 - r_square: 0.8460\n",
            "Epoch 408: val_loss did not improve from 0.09467\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0675 - r_square: 0.8478 - val_loss: 0.0949 - val_r_square: 0.8885\n",
            "Epoch 409/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0680 - r_square: 0.8468\n",
            "Epoch 409: val_loss improved from 0.09467 to 0.09454, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0674 - r_square: 0.8473 - val_loss: 0.0945 - val_r_square: 0.8924\n",
            "Epoch 410/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0677 - r_square: 0.8523\n",
            "Epoch 410: val_loss improved from 0.09454 to 0.09452, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0672 - r_square: 0.8485 - val_loss: 0.0945 - val_r_square: 0.8915\n",
            "Epoch 411/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0670 - r_square: 0.8505\n",
            "Epoch 411: val_loss improved from 0.09452 to 0.09446, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0673 - r_square: 0.8492 - val_loss: 0.0945 - val_r_square: 0.8926\n",
            "Epoch 412/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0682 - r_square: 0.8524\n",
            "Epoch 412: val_loss did not improve from 0.09446\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0674 - r_square: 0.8497 - val_loss: 0.0945 - val_r_square: 0.8930\n",
            "Epoch 413/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0676 - r_square: 0.8500\n",
            "Epoch 413: val_loss improved from 0.09446 to 0.09425, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0671 - r_square: 0.8505 - val_loss: 0.0943 - val_r_square: 0.8924\n",
            "Epoch 414/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0668 - r_square: 0.8470\n",
            "Epoch 414: val_loss improved from 0.09425 to 0.09423, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0669 - r_square: 0.8503 - val_loss: 0.0942 - val_r_square: 0.8937\n",
            "Epoch 415/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0662 - r_square: 0.8531\n",
            "Epoch 415: val_loss improved from 0.09423 to 0.09413, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0668 - r_square: 0.8508 - val_loss: 0.0941 - val_r_square: 0.8932\n",
            "Epoch 416/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0675 - r_square: 0.8501\n",
            "Epoch 416: val_loss improved from 0.09413 to 0.09384, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0668 - r_square: 0.8512 - val_loss: 0.0938 - val_r_square: 0.8947\n",
            "Epoch 417/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0668 - r_square: 0.8506\n",
            "Epoch 417: val_loss did not improve from 0.09384\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0666 - r_square: 0.8514 - val_loss: 0.0942 - val_r_square: 0.8961\n",
            "Epoch 418/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0656 - r_square: 0.8546\n",
            "Epoch 418: val_loss did not improve from 0.09384\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0665 - r_square: 0.8514 - val_loss: 0.0941 - val_r_square: 0.8967\n",
            "Epoch 419/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0677 - r_square: 0.8560\n",
            "Epoch 419: val_loss improved from 0.09384 to 0.09368, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0667 - r_square: 0.8533 - val_loss: 0.0937 - val_r_square: 0.8956\n",
            "Epoch 420/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0653 - r_square: 0.8559\n",
            "Epoch 420: val_loss did not improve from 0.09368\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0664 - r_square: 0.8540 - val_loss: 0.0937 - val_r_square: 0.8961\n",
            "Epoch 421/1500\n",
            "48/67 [====================>.........] - ETA: 0s - loss: 0.0654 - r_square: 0.8562\n",
            "Epoch 421: val_loss did not improve from 0.09368\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0662 - r_square: 0.8536 - val_loss: 0.0939 - val_r_square: 0.8945\n",
            "Epoch 422/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0665 - r_square: 0.8541\n",
            "Epoch 422: val_loss improved from 0.09368 to 0.09349, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0664 - r_square: 0.8535 - val_loss: 0.0935 - val_r_square: 0.8963\n",
            "Epoch 423/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0658 - r_square: 0.8620\n",
            "Epoch 423: val_loss did not improve from 0.09349\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0660 - r_square: 0.8545 - val_loss: 0.0935 - val_r_square: 0.8968\n",
            "Epoch 424/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0657 - r_square: 0.8547\n",
            "Epoch 424: val_loss did not improve from 0.09349\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0661 - r_square: 0.8550 - val_loss: 0.0935 - val_r_square: 0.8980\n",
            "Epoch 425/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0656 - r_square: 0.8557\n",
            "Epoch 425: val_loss did not improve from 0.09349\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0660 - r_square: 0.8560 - val_loss: 0.0936 - val_r_square: 0.8958\n",
            "Epoch 426/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0653 - r_square: 0.8592\n",
            "Epoch 426: val_loss did not improve from 0.09349\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0657 - r_square: 0.8556 - val_loss: 0.0943 - val_r_square: 0.9005\n",
            "Epoch 427/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0644 - r_square: 0.8570\n",
            "Epoch 427: val_loss improved from 0.09349 to 0.09319, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0658 - r_square: 0.8567 - val_loss: 0.0932 - val_r_square: 0.8985\n",
            "Epoch 428/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0650 - r_square: 0.8570\n",
            "Epoch 428: val_loss improved from 0.09319 to 0.09316, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0656 - r_square: 0.8571 - val_loss: 0.0932 - val_r_square: 0.8992\n",
            "Epoch 429/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0657 - r_square: 0.8544\n",
            "Epoch 429: val_loss did not improve from 0.09316\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0656 - r_square: 0.8563 - val_loss: 0.0932 - val_r_square: 0.9005\n",
            "Epoch 430/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0647 - r_square: 0.8584\n",
            "Epoch 430: val_loss improved from 0.09316 to 0.09303, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0654 - r_square: 0.8576 - val_loss: 0.0930 - val_r_square: 0.8992\n",
            "Epoch 431/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0665 - r_square: 0.8573\n",
            "Epoch 431: val_loss improved from 0.09303 to 0.09302, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0653 - r_square: 0.8581 - val_loss: 0.0930 - val_r_square: 0.9010\n",
            "Epoch 432/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0664 - r_square: 0.8529\n",
            "Epoch 432: val_loss did not improve from 0.09302\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0653 - r_square: 0.8586 - val_loss: 0.0931 - val_r_square: 0.8988\n",
            "Epoch 433/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0647 - r_square: 0.8702\n",
            "Epoch 433: val_loss improved from 0.09302 to 0.09278, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0652 - r_square: 0.8577 - val_loss: 0.0928 - val_r_square: 0.9013\n",
            "Epoch 434/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0662 - r_square: 0.8623\n",
            "Epoch 434: val_loss improved from 0.09278 to 0.09274, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0651 - r_square: 0.8598 - val_loss: 0.0927 - val_r_square: 0.9006\n",
            "Epoch 435/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0646 - r_square: 0.8676\n",
            "Epoch 435: val_loss improved from 0.09274 to 0.09263, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0651 - r_square: 0.8602 - val_loss: 0.0926 - val_r_square: 0.9012\n",
            "Epoch 436/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0653 - r_square: 0.8535\n",
            "Epoch 436: val_loss did not improve from 0.09263\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0649 - r_square: 0.8599 - val_loss: 0.0928 - val_r_square: 0.8999\n",
            "Epoch 437/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0650 - r_square: 0.8626\n",
            "Epoch 437: val_loss improved from 0.09263 to 0.09250, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0648 - r_square: 0.8609 - val_loss: 0.0925 - val_r_square: 0.9019\n",
            "Epoch 438/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0655 - r_square: 0.8563\n",
            "Epoch 438: val_loss did not improve from 0.09250\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0648 - r_square: 0.8609 - val_loss: 0.0925 - val_r_square: 0.9016\n",
            "Epoch 439/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0656 - r_square: 0.8613\n",
            "Epoch 439: val_loss did not improve from 0.09250\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0650 - r_square: 0.8618 - val_loss: 0.0926 - val_r_square: 0.9011\n",
            "Epoch 440/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0647 - r_square: 0.8628\n",
            "Epoch 440: val_loss improved from 0.09250 to 0.09235, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0647 - r_square: 0.8612 - val_loss: 0.0923 - val_r_square: 0.9021\n",
            "Epoch 441/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0660 - r_square: 0.8621\n",
            "Epoch 441: val_loss did not improve from 0.09235\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0647 - r_square: 0.8619 - val_loss: 0.0928 - val_r_square: 0.9050\n",
            "Epoch 442/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0637 - r_square: 0.8710\n",
            "Epoch 442: val_loss improved from 0.09235 to 0.09218, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0645 - r_square: 0.8625 - val_loss: 0.0922 - val_r_square: 0.9035\n",
            "Epoch 443/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0654 - r_square: 0.8606\n",
            "Epoch 443: val_loss improved from 0.09218 to 0.09216, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0644 - r_square: 0.8629 - val_loss: 0.0922 - val_r_square: 0.9034\n",
            "Epoch 444/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0641 - r_square: 0.8666\n",
            "Epoch 444: val_loss improved from 0.09216 to 0.09214, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0645 - r_square: 0.8636 - val_loss: 0.0921 - val_r_square: 0.9044\n",
            "Epoch 445/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0633 - r_square: 0.8618\n",
            "Epoch 445: val_loss improved from 0.09214 to 0.09213, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0643 - r_square: 0.8636 - val_loss: 0.0921 - val_r_square: 0.9052\n",
            "Epoch 446/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0643 - r_square: 0.8632\n",
            "Epoch 446: val_loss improved from 0.09213 to 0.09192, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0643 - r_square: 0.8638 - val_loss: 0.0919 - val_r_square: 0.9045\n",
            "Epoch 447/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0645 - r_square: 0.8636\n",
            "Epoch 447: val_loss did not improve from 0.09192\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0641 - r_square: 0.8637 - val_loss: 0.0923 - val_r_square: 0.9027\n",
            "Epoch 448/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0643 - r_square: 0.8633\n",
            "Epoch 448: val_loss did not improve from 0.09192\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0642 - r_square: 0.8642 - val_loss: 0.0919 - val_r_square: 0.9057\n",
            "Epoch 449/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0655 - r_square: 0.8681\n",
            "Epoch 449: val_loss improved from 0.09192 to 0.09175, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0640 - r_square: 0.8649 - val_loss: 0.0918 - val_r_square: 0.9055\n",
            "Epoch 450/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0618 - r_square: 0.8595\n",
            "Epoch 450: val_loss improved from 0.09175 to 0.09174, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0640 - r_square: 0.8647 - val_loss: 0.0917 - val_r_square: 0.9054\n",
            "Epoch 451/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0630 - r_square: 0.8689\n",
            "Epoch 451: val_loss improved from 0.09174 to 0.09165, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0640 - r_square: 0.8654 - val_loss: 0.0917 - val_r_square: 0.9062\n",
            "Epoch 452/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0636 - r_square: 0.8654\n",
            "Epoch 452: val_loss did not improve from 0.09165\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0639 - r_square: 0.8649 - val_loss: 0.0917 - val_r_square: 0.9070\n",
            "Epoch 453/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0639 - r_square: 0.8670\n",
            "Epoch 453: val_loss improved from 0.09165 to 0.09157, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0640 - r_square: 0.8660 - val_loss: 0.0916 - val_r_square: 0.9053\n",
            "Epoch 454/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0638 - r_square: 0.8627\n",
            "Epoch 454: val_loss did not improve from 0.09157\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0639 - r_square: 0.8665 - val_loss: 0.0920 - val_r_square: 0.9047\n",
            "Epoch 455/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0629 - r_square: 0.8625\n",
            "Epoch 455: val_loss improved from 0.09157 to 0.09151, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0636 - r_square: 0.8665 - val_loss: 0.0915 - val_r_square: 0.9073\n",
            "Epoch 456/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0640 - r_square: 0.8667\n",
            "Epoch 456: val_loss improved from 0.09151 to 0.09127, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0636 - r_square: 0.8668 - val_loss: 0.0913 - val_r_square: 0.9072\n",
            "Epoch 457/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0642 - r_square: 0.8716\n",
            "Epoch 457: val_loss did not improve from 0.09127\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0634 - r_square: 0.8674 - val_loss: 0.0917 - val_r_square: 0.9058\n",
            "Epoch 458/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0633 - r_square: 0.8667\n",
            "Epoch 458: val_loss did not improve from 0.09127\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0634 - r_square: 0.8670 - val_loss: 0.0915 - val_r_square: 0.9080\n",
            "Epoch 459/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0634 - r_square: 0.8682\n",
            "Epoch 459: val_loss did not improve from 0.09127\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0634 - r_square: 0.8682 - val_loss: 0.0917 - val_r_square: 0.9089\n",
            "Epoch 460/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0638 - r_square: 0.8677\n",
            "Epoch 460: val_loss improved from 0.09127 to 0.09120, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0634 - r_square: 0.8683 - val_loss: 0.0912 - val_r_square: 0.9082\n",
            "Epoch 461/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0632 - r_square: 0.8724\n",
            "Epoch 461: val_loss improved from 0.09120 to 0.09106, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0632 - r_square: 0.8683 - val_loss: 0.0911 - val_r_square: 0.9079\n",
            "Epoch 462/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0624 - r_square: 0.8663\n",
            "Epoch 462: val_loss did not improve from 0.09106\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0632 - r_square: 0.8687 - val_loss: 0.0911 - val_r_square: 0.9084\n",
            "Epoch 463/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0627 - r_square: 0.8569\n",
            "Epoch 463: val_loss improved from 0.09106 to 0.09087, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0632 - r_square: 0.8684 - val_loss: 0.0909 - val_r_square: 0.9089\n",
            "Epoch 464/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0634 - r_square: 0.8696\n",
            "Epoch 464: val_loss did not improve from 0.09087\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0630 - r_square: 0.8695 - val_loss: 0.0910 - val_r_square: 0.9094\n",
            "Epoch 465/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0632 - r_square: 0.8610\n",
            "Epoch 465: val_loss improved from 0.09087 to 0.09086, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0629 - r_square: 0.8696 - val_loss: 0.0909 - val_r_square: 0.9096\n",
            "Epoch 466/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0630 - r_square: 0.8728\n",
            "Epoch 466: val_loss did not improve from 0.09086\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0630 - r_square: 0.8701 - val_loss: 0.0911 - val_r_square: 0.9103\n",
            "Epoch 467/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0625 - r_square: 0.8873\n",
            "Epoch 467: val_loss improved from 0.09086 to 0.09078, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0630 - r_square: 0.8696 - val_loss: 0.0908 - val_r_square: 0.9096\n",
            "Epoch 468/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0628 - r_square: 0.8753\n",
            "Epoch 468: val_loss did not improve from 0.09078\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0628 - r_square: 0.8703 - val_loss: 0.0909 - val_r_square: 0.9086\n",
            "Epoch 469/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0627 - r_square: 0.8647\n",
            "Epoch 469: val_loss improved from 0.09078 to 0.09068, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0629 - r_square: 0.8701 - val_loss: 0.0907 - val_r_square: 0.9090\n",
            "Epoch 470/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0633 - r_square: 0.8708\n",
            "Epoch 470: val_loss improved from 0.09068 to 0.09059, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0628 - r_square: 0.8710 - val_loss: 0.0906 - val_r_square: 0.9107\n",
            "Epoch 471/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0623 - r_square: 0.8726\n",
            "Epoch 471: val_loss improved from 0.09059 to 0.09055, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0626 - r_square: 0.8712 - val_loss: 0.0906 - val_r_square: 0.9102\n",
            "Epoch 472/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0611 - r_square: 0.8745\n",
            "Epoch 472: val_loss did not improve from 0.09055\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0626 - r_square: 0.8712 - val_loss: 0.0907 - val_r_square: 0.9115\n",
            "Epoch 473/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0617 - r_square: 0.8844\n",
            "Epoch 473: val_loss improved from 0.09055 to 0.09043, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0626 - r_square: 0.8706 - val_loss: 0.0904 - val_r_square: 0.9111\n",
            "Epoch 474/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0617 - r_square: 0.8754\n",
            "Epoch 474: val_loss did not improve from 0.09043\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0624 - r_square: 0.8720 - val_loss: 0.0905 - val_r_square: 0.9104\n",
            "Epoch 475/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0623 - r_square: 0.8721\n",
            "Epoch 475: val_loss did not improve from 0.09043\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0623 - r_square: 0.8721 - val_loss: 0.0905 - val_r_square: 0.9123\n",
            "Epoch 476/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0616 - r_square: 0.8766\n",
            "Epoch 476: val_loss did not improve from 0.09043\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0624 - r_square: 0.8732 - val_loss: 0.0909 - val_r_square: 0.9093\n",
            "Epoch 477/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0625 - r_square: 0.8720\n",
            "Epoch 477: val_loss did not improve from 0.09043\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0624 - r_square: 0.8724 - val_loss: 0.0905 - val_r_square: 0.9124\n",
            "Epoch 478/1500\n",
            "49/67 [====================>.........] - ETA: 0s - loss: 0.0646 - r_square: 0.8671\n",
            "Epoch 478: val_loss improved from 0.09043 to 0.09043, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0621 - r_square: 0.8732 - val_loss: 0.0904 - val_r_square: 0.9111\n",
            "Epoch 479/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0623 - r_square: 0.8718\n",
            "Epoch 479: val_loss did not improve from 0.09043\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0622 - r_square: 0.8737 - val_loss: 0.0905 - val_r_square: 0.9106\n",
            "Epoch 480/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0627 - r_square: 0.8785\n",
            "Epoch 480: val_loss improved from 0.09043 to 0.09016, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0622 - r_square: 0.8734 - val_loss: 0.0902 - val_r_square: 0.9121\n",
            "Epoch 481/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0617 - r_square: 0.8730\n",
            "Epoch 481: val_loss improved from 0.09016 to 0.09002, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0619 - r_square: 0.8738 - val_loss: 0.0900 - val_r_square: 0.9120\n",
            "Epoch 482/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0629 - r_square: 0.8720\n",
            "Epoch 482: val_loss did not improve from 0.09002\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0619 - r_square: 0.8739 - val_loss: 0.0901 - val_r_square: 0.9128\n",
            "Epoch 483/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0607 - r_square: 0.8665\n",
            "Epoch 483: val_loss did not improve from 0.09002\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0617 - r_square: 0.8750 - val_loss: 0.0904 - val_r_square: 0.9109\n",
            "Epoch 484/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0611 - r_square: 0.8698\n",
            "Epoch 484: val_loss improved from 0.09002 to 0.09002, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0619 - r_square: 0.8739 - val_loss: 0.0900 - val_r_square: 0.9135\n",
            "Epoch 485/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0608 - r_square: 0.8729\n",
            "Epoch 485: val_loss did not improve from 0.09002\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0619 - r_square: 0.8748 - val_loss: 0.0902 - val_r_square: 0.9115\n",
            "Epoch 486/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0620 - r_square: 0.8729\n",
            "Epoch 486: val_loss improved from 0.09002 to 0.09000, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0619 - r_square: 0.8749 - val_loss: 0.0900 - val_r_square: 0.9126\n",
            "Epoch 487/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0623 - r_square: 0.8718\n",
            "Epoch 487: val_loss improved from 0.09000 to 0.08999, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0617 - r_square: 0.8762 - val_loss: 0.0900 - val_r_square: 0.9122\n",
            "Epoch 488/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0617 - r_square: 0.8734\n",
            "Epoch 488: val_loss improved from 0.08999 to 0.08979, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0616 - r_square: 0.8756 - val_loss: 0.0898 - val_r_square: 0.9132\n",
            "Epoch 489/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0610 - r_square: 0.8751\n",
            "Epoch 489: val_loss did not improve from 0.08979\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0615 - r_square: 0.8757 - val_loss: 0.0899 - val_r_square: 0.9124\n",
            "Epoch 490/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0614 - r_square: 0.8783\n",
            "Epoch 490: val_loss did not improve from 0.08979\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0616 - r_square: 0.8759 - val_loss: 0.0898 - val_r_square: 0.9134\n",
            "Epoch 491/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0623 - r_square: 0.8735\n",
            "Epoch 491: val_loss improved from 0.08979 to 0.08959, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0614 - r_square: 0.8760 - val_loss: 0.0896 - val_r_square: 0.9141\n",
            "Epoch 492/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0614 - r_square: 0.8839\n",
            "Epoch 492: val_loss did not improve from 0.08959\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0614 - r_square: 0.8769 - val_loss: 0.0897 - val_r_square: 0.9142\n",
            "Epoch 493/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0602 - r_square: 0.8911\n",
            "Epoch 493: val_loss improved from 0.08959 to 0.08951, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0613 - r_square: 0.8774 - val_loss: 0.0895 - val_r_square: 0.9145\n",
            "Epoch 494/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0612 - r_square: 0.8824\n",
            "Epoch 494: val_loss did not improve from 0.08951\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0613 - r_square: 0.8766 - val_loss: 0.0896 - val_r_square: 0.9144\n",
            "Epoch 495/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0631 - r_square: 0.8656\n",
            "Epoch 495: val_loss did not improve from 0.08951\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0612 - r_square: 0.8766 - val_loss: 0.0898 - val_r_square: 0.9155\n",
            "Epoch 496/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0600 - r_square: 0.8803\n",
            "Epoch 496: val_loss improved from 0.08951 to 0.08938, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0613 - r_square: 0.8770 - val_loss: 0.0894 - val_r_square: 0.9149\n",
            "Epoch 497/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0609 - r_square: 0.8800\n",
            "Epoch 497: val_loss improved from 0.08938 to 0.08936, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0612 - r_square: 0.8773 - val_loss: 0.0894 - val_r_square: 0.9145\n",
            "Epoch 498/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0606 - r_square: 0.8771\n",
            "Epoch 498: val_loss did not improve from 0.08936\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0610 - r_square: 0.8777 - val_loss: 0.0895 - val_r_square: 0.9155\n",
            "Epoch 499/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0606 - r_square: 0.8837\n",
            "Epoch 499: val_loss did not improve from 0.08936\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0609 - r_square: 0.8786 - val_loss: 0.0894 - val_r_square: 0.9146\n",
            "Epoch 500/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0612 - r_square: 0.8838\n",
            "Epoch 500: val_loss improved from 0.08936 to 0.08930, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0611 - r_square: 0.8776 - val_loss: 0.0893 - val_r_square: 0.9150\n",
            "Epoch 501/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0610 - r_square: 0.8846\n",
            "Epoch 501: val_loss did not improve from 0.08930\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0609 - r_square: 0.8788 - val_loss: 0.0896 - val_r_square: 0.9144\n",
            "Epoch 502/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0594 - r_square: 0.8758\n",
            "Epoch 502: val_loss improved from 0.08930 to 0.08924, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0608 - r_square: 0.8783 - val_loss: 0.0892 - val_r_square: 0.9151\n",
            "Epoch 503/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0607 - r_square: 0.8787\n",
            "Epoch 503: val_loss did not improve from 0.08924\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0609 - r_square: 0.8782 - val_loss: 0.0893 - val_r_square: 0.9161\n",
            "Epoch 504/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0615 - r_square: 0.8874\n",
            "Epoch 504: val_loss did not improve from 0.08924\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0610 - r_square: 0.8788 - val_loss: 0.0893 - val_r_square: 0.9166\n",
            "Epoch 505/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0609 - r_square: 0.8799\n",
            "Epoch 505: val_loss did not improve from 0.08924\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0609 - r_square: 0.8799 - val_loss: 0.0893 - val_r_square: 0.9153\n",
            "Epoch 506/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0599 - r_square: 0.8762\n",
            "Epoch 506: val_loss improved from 0.08924 to 0.08909, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0607 - r_square: 0.8794 - val_loss: 0.0891 - val_r_square: 0.9160\n",
            "Epoch 507/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0598 - r_square: 0.8802\n",
            "Epoch 507: val_loss did not improve from 0.08909\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0606 - r_square: 0.8799 - val_loss: 0.0891 - val_r_square: 0.9159\n",
            "Epoch 508/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0616 - r_square: 0.8780\n",
            "Epoch 508: val_loss improved from 0.08909 to 0.08906, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0607 - r_square: 0.8794 - val_loss: 0.0891 - val_r_square: 0.9163\n",
            "Epoch 509/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0612 - r_square: 0.8774\n",
            "Epoch 509: val_loss improved from 0.08906 to 0.08900, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0605 - r_square: 0.8800 - val_loss: 0.0890 - val_r_square: 0.9160\n",
            "Epoch 510/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0590 - r_square: 0.8901\n",
            "Epoch 510: val_loss improved from 0.08900 to 0.08896, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0604 - r_square: 0.8804 - val_loss: 0.0890 - val_r_square: 0.9169\n",
            "Epoch 511/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0612 - r_square: 0.8792\n",
            "Epoch 511: val_loss improved from 0.08896 to 0.08894, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0605 - r_square: 0.8808 - val_loss: 0.0889 - val_r_square: 0.9164\n",
            "Epoch 512/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0609 - r_square: 0.8765\n",
            "Epoch 512: val_loss did not improve from 0.08894\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0603 - r_square: 0.8805 - val_loss: 0.0890 - val_r_square: 0.9176\n",
            "Epoch 513/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0604 - r_square: 0.8816\n",
            "Epoch 513: val_loss improved from 0.08894 to 0.08880, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0604 - r_square: 0.8816 - val_loss: 0.0888 - val_r_square: 0.9176\n",
            "Epoch 514/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0599 - r_square: 0.8852\n",
            "Epoch 514: val_loss improved from 0.08880 to 0.08869, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0602 - r_square: 0.8809 - val_loss: 0.0887 - val_r_square: 0.9168\n",
            "Epoch 515/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0605 - r_square: 0.8795\n",
            "Epoch 515: val_loss improved from 0.08869 to 0.08864, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0603 - r_square: 0.8813 - val_loss: 0.0886 - val_r_square: 0.9178\n",
            "Epoch 516/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0604 - r_square: 0.8851\n",
            "Epoch 516: val_loss did not improve from 0.08864\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0602 - r_square: 0.8814 - val_loss: 0.0887 - val_r_square: 0.9177\n",
            "Epoch 517/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0602 - r_square: 0.8818\n",
            "Epoch 517: val_loss did not improve from 0.08864\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0602 - r_square: 0.8818 - val_loss: 0.0886 - val_r_square: 0.9173\n",
            "Epoch 518/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0609 - r_square: 0.8816\n",
            "Epoch 518: val_loss did not improve from 0.08864\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0601 - r_square: 0.8821 - val_loss: 0.0887 - val_r_square: 0.9172\n",
            "Epoch 519/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0582 - r_square: 0.8846\n",
            "Epoch 519: val_loss improved from 0.08864 to 0.08852, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0601 - r_square: 0.8823 - val_loss: 0.0885 - val_r_square: 0.9172\n",
            "Epoch 520/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0599 - r_square: 0.8822\n",
            "Epoch 520: val_loss improved from 0.08852 to 0.08848, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0601 - r_square: 0.8819 - val_loss: 0.0885 - val_r_square: 0.9182\n",
            "Epoch 521/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0600 - r_square: 0.8877\n",
            "Epoch 521: val_loss did not improve from 0.08848\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0601 - r_square: 0.8827 - val_loss: 0.0886 - val_r_square: 0.9184\n",
            "Epoch 522/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0590 - r_square: 0.8923\n",
            "Epoch 522: val_loss did not improve from 0.08848\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0599 - r_square: 0.8828 - val_loss: 0.0886 - val_r_square: 0.9173\n",
            "Epoch 523/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0610 - r_square: 0.8775\n",
            "Epoch 523: val_loss improved from 0.08848 to 0.08836, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0599 - r_square: 0.8829 - val_loss: 0.0884 - val_r_square: 0.9182\n",
            "Epoch 524/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0595 - r_square: 0.8901\n",
            "Epoch 524: val_loss did not improve from 0.08836\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0598 - r_square: 0.8830 - val_loss: 0.0884 - val_r_square: 0.9182\n",
            "Epoch 525/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0601 - r_square: 0.8925\n",
            "Epoch 525: val_loss improved from 0.08836 to 0.08835, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0598 - r_square: 0.8828 - val_loss: 0.0883 - val_r_square: 0.9179\n",
            "Epoch 526/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0602 - r_square: 0.8855\n",
            "Epoch 526: val_loss did not improve from 0.08835\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0598 - r_square: 0.8839 - val_loss: 0.0891 - val_r_square: 0.9193\n",
            "Epoch 527/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0585 - r_square: 0.8832\n",
            "Epoch 527: val_loss did not improve from 0.08835\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0597 - r_square: 0.8835 - val_loss: 0.0884 - val_r_square: 0.9180\n",
            "Epoch 528/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0604 - r_square: 0.8820\n",
            "Epoch 528: val_loss did not improve from 0.08835\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0597 - r_square: 0.8834 - val_loss: 0.0884 - val_r_square: 0.9183\n",
            "Epoch 529/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0599 - r_square: 0.8815\n",
            "Epoch 529: val_loss improved from 0.08835 to 0.08833, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0596 - r_square: 0.8836 - val_loss: 0.0883 - val_r_square: 0.9192\n",
            "Epoch 530/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0597 - r_square: 0.8886\n",
            "Epoch 530: val_loss did not improve from 0.08833\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0595 - r_square: 0.8846 - val_loss: 0.0884 - val_r_square: 0.9181\n",
            "Epoch 531/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0594 - r_square: 0.8780\n",
            "Epoch 531: val_loss improved from 0.08833 to 0.08815, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0596 - r_square: 0.8841 - val_loss: 0.0881 - val_r_square: 0.9186\n",
            "Epoch 532/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0574 - r_square: 0.8811\n",
            "Epoch 532: val_loss did not improve from 0.08815\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0594 - r_square: 0.8848 - val_loss: 0.0882 - val_r_square: 0.9191\n",
            "Epoch 533/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0603 - r_square: 0.8782\n",
            "Epoch 533: val_loss did not improve from 0.08815\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0594 - r_square: 0.8847 - val_loss: 0.0882 - val_r_square: 0.9186\n",
            "Epoch 534/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0596 - r_square: 0.8933\n",
            "Epoch 534: val_loss did not improve from 0.08815\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0593 - r_square: 0.8846 - val_loss: 0.0882 - val_r_square: 0.9190\n",
            "Epoch 535/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0592 - r_square: 0.8983\n",
            "Epoch 535: val_loss did not improve from 0.08815\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0594 - r_square: 0.8854 - val_loss: 0.0886 - val_r_square: 0.9181\n",
            "Epoch 536/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0599 - r_square: 0.8881\n",
            "Epoch 536: val_loss improved from 0.08815 to 0.08803, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0594 - r_square: 0.8849 - val_loss: 0.0880 - val_r_square: 0.9197\n",
            "Epoch 537/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0596 - r_square: 0.8833\n",
            "Epoch 537: val_loss did not improve from 0.08803\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0592 - r_square: 0.8855 - val_loss: 0.0881 - val_r_square: 0.9203\n",
            "Epoch 538/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0589 - r_square: 0.8898\n",
            "Epoch 538: val_loss improved from 0.08803 to 0.08798, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0594 - r_square: 0.8860 - val_loss: 0.0880 - val_r_square: 0.9191\n",
            "Epoch 539/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0575 - r_square: 0.8955\n",
            "Epoch 539: val_loss improved from 0.08798 to 0.08779, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0591 - r_square: 0.8856 - val_loss: 0.0878 - val_r_square: 0.9201\n",
            "Epoch 540/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0586 - r_square: 0.8894\n",
            "Epoch 540: val_loss did not improve from 0.08779\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0591 - r_square: 0.8861 - val_loss: 0.0878 - val_r_square: 0.9203\n",
            "Epoch 541/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0601 - r_square: 0.8808\n",
            "Epoch 541: val_loss did not improve from 0.08779\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0592 - r_square: 0.8854 - val_loss: 0.0882 - val_r_square: 0.9190\n",
            "Epoch 542/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0590 - r_square: 0.8861\n",
            "Epoch 542: val_loss did not improve from 0.08779\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0590 - r_square: 0.8858 - val_loss: 0.0879 - val_r_square: 0.9195\n",
            "Epoch 543/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0592 - r_square: 0.8867\n",
            "Epoch 543: val_loss improved from 0.08779 to 0.08775, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0590 - r_square: 0.8867 - val_loss: 0.0878 - val_r_square: 0.9198\n",
            "Epoch 544/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0586 - r_square: 0.8969\n",
            "Epoch 544: val_loss improved from 0.08775 to 0.08761, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0590 - r_square: 0.8871 - val_loss: 0.0876 - val_r_square: 0.9204\n",
            "Epoch 545/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0591 - r_square: 0.8883\n",
            "Epoch 545: val_loss did not improve from 0.08761\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0588 - r_square: 0.8870 - val_loss: 0.0876 - val_r_square: 0.9200\n",
            "Epoch 546/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0598 - r_square: 0.8846\n",
            "Epoch 546: val_loss did not improve from 0.08761\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0591 - r_square: 0.8866 - val_loss: 0.0878 - val_r_square: 0.9210\n",
            "Epoch 547/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0578 - r_square: 0.8881\n",
            "Epoch 547: val_loss improved from 0.08761 to 0.08761, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0589 - r_square: 0.8868 - val_loss: 0.0876 - val_r_square: 0.9203\n",
            "Epoch 548/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0590 - r_square: 0.8872\n",
            "Epoch 548: val_loss did not improve from 0.08761\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0588 - r_square: 0.8871 - val_loss: 0.0877 - val_r_square: 0.9209\n",
            "Epoch 549/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0588 - r_square: 0.8851\n",
            "Epoch 549: val_loss improved from 0.08761 to 0.08749, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0587 - r_square: 0.8873 - val_loss: 0.0875 - val_r_square: 0.9207\n",
            "Epoch 550/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0587 - r_square: 0.8872\n",
            "Epoch 550: val_loss improved from 0.08749 to 0.08741, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0587 - r_square: 0.8872 - val_loss: 0.0874 - val_r_square: 0.9207\n",
            "Epoch 551/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0584 - r_square: 0.8870\n",
            "Epoch 551: val_loss did not improve from 0.08741\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0585 - r_square: 0.8881 - val_loss: 0.0878 - val_r_square: 0.9211\n",
            "Epoch 552/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0598 - r_square: 0.8812\n",
            "Epoch 552: val_loss did not improve from 0.08741\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0585 - r_square: 0.8879 - val_loss: 0.0878 - val_r_square: 0.9216\n",
            "Epoch 553/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0586 - r_square: 0.8883\n",
            "Epoch 553: val_loss improved from 0.08741 to 0.08727, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0586 - r_square: 0.8883 - val_loss: 0.0873 - val_r_square: 0.9205\n",
            "Epoch 554/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0590 - r_square: 0.8893\n",
            "Epoch 554: val_loss did not improve from 0.08727\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0587 - r_square: 0.8876 - val_loss: 0.0874 - val_r_square: 0.9213\n",
            "Epoch 555/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0580 - r_square: 0.8886\n",
            "Epoch 555: val_loss improved from 0.08727 to 0.08726, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0586 - r_square: 0.8884 - val_loss: 0.0873 - val_r_square: 0.9211\n",
            "Epoch 556/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0583 - r_square: 0.8838\n",
            "Epoch 556: val_loss did not improve from 0.08726\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0584 - r_square: 0.8886 - val_loss: 0.0875 - val_r_square: 0.9205\n",
            "Epoch 557/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0586 - r_square: 0.8877\n",
            "Epoch 557: val_loss improved from 0.08726 to 0.08726, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0585 - r_square: 0.8891 - val_loss: 0.0873 - val_r_square: 0.9206\n",
            "Epoch 558/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0590 - r_square: 0.8847\n",
            "Epoch 558: val_loss did not improve from 0.08726\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0584 - r_square: 0.8882 - val_loss: 0.0874 - val_r_square: 0.9218\n",
            "Epoch 559/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0573 - r_square: 0.8953\n",
            "Epoch 559: val_loss did not improve from 0.08726\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0584 - r_square: 0.8892 - val_loss: 0.0873 - val_r_square: 0.9215\n",
            "Epoch 560/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0581 - r_square: 0.8828\n",
            "Epoch 560: val_loss improved from 0.08726 to 0.08725, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0582 - r_square: 0.8889 - val_loss: 0.0873 - val_r_square: 0.9213\n",
            "Epoch 561/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0581 - r_square: 0.8878\n",
            "Epoch 561: val_loss improved from 0.08725 to 0.08707, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0582 - r_square: 0.8898 - val_loss: 0.0871 - val_r_square: 0.9215\n",
            "Epoch 562/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0577 - r_square: 0.8894\n",
            "Epoch 562: val_loss improved from 0.08707 to 0.08707, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0582 - r_square: 0.8896 - val_loss: 0.0871 - val_r_square: 0.9216\n",
            "Epoch 563/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0575 - r_square: 0.8890\n",
            "Epoch 563: val_loss did not improve from 0.08707\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0582 - r_square: 0.8898 - val_loss: 0.0871 - val_r_square: 0.9212\n",
            "Epoch 564/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0582 - r_square: 0.8894\n",
            "Epoch 564: val_loss improved from 0.08707 to 0.08702, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0581 - r_square: 0.8901 - val_loss: 0.0870 - val_r_square: 0.9219\n",
            "Epoch 565/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0563 - r_square: 0.8936\n",
            "Epoch 565: val_loss did not improve from 0.08702\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0580 - r_square: 0.8901 - val_loss: 0.0875 - val_r_square: 0.9223\n",
            "Epoch 566/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0581 - r_square: 0.8899\n",
            "Epoch 566: val_loss improved from 0.08702 to 0.08701, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0581 - r_square: 0.8899 - val_loss: 0.0870 - val_r_square: 0.9219\n",
            "Epoch 567/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0581 - r_square: 0.8893\n",
            "Epoch 567: val_loss improved from 0.08701 to 0.08699, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0581 - r_square: 0.8901 - val_loss: 0.0870 - val_r_square: 0.9223\n",
            "Epoch 568/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0565 - r_square: 0.8900\n",
            "Epoch 568: val_loss improved from 0.08699 to 0.08680, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0579 - r_square: 0.8902 - val_loss: 0.0868 - val_r_square: 0.9220\n",
            "Epoch 569/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0580 - r_square: 0.8978\n",
            "Epoch 569: val_loss did not improve from 0.08680\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0579 - r_square: 0.8906 - val_loss: 0.0869 - val_r_square: 0.9219\n",
            "Epoch 570/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0570 - r_square: 0.8982\n",
            "Epoch 570: val_loss did not improve from 0.08680\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0578 - r_square: 0.8911 - val_loss: 0.0869 - val_r_square: 0.9217\n",
            "Epoch 571/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0581 - r_square: 0.8898\n",
            "Epoch 571: val_loss did not improve from 0.08680\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0579 - r_square: 0.8907 - val_loss: 0.0869 - val_r_square: 0.9220\n",
            "Epoch 572/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0556 - r_square: 0.8911\n",
            "Epoch 572: val_loss did not improve from 0.08680\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0578 - r_square: 0.8907 - val_loss: 0.0871 - val_r_square: 0.9215\n",
            "Epoch 573/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0574 - r_square: 0.8941\n",
            "Epoch 573: val_loss did not improve from 0.08680\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0578 - r_square: 0.8910 - val_loss: 0.0868 - val_r_square: 0.9225\n",
            "Epoch 574/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0577 - r_square: 0.8918\n",
            "Epoch 574: val_loss improved from 0.08680 to 0.08668, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0577 - r_square: 0.8918 - val_loss: 0.0867 - val_r_square: 0.9225\n",
            "Epoch 575/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0579 - r_square: 0.8918\n",
            "Epoch 575: val_loss did not improve from 0.08668\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0576 - r_square: 0.8916 - val_loss: 0.0875 - val_r_square: 0.9214\n",
            "Epoch 576/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0579 - r_square: 0.8931\n",
            "Epoch 576: val_loss improved from 0.08668 to 0.08667, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0577 - r_square: 0.8915 - val_loss: 0.0867 - val_r_square: 0.9218\n",
            "Epoch 577/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0568 - r_square: 0.8924\n",
            "Epoch 577: val_loss did not improve from 0.08667\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0576 - r_square: 0.8914 - val_loss: 0.0868 - val_r_square: 0.9229\n",
            "Epoch 578/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0586 - r_square: 0.8897\n",
            "Epoch 578: val_loss improved from 0.08667 to 0.08651, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0575 - r_square: 0.8920 - val_loss: 0.0865 - val_r_square: 0.9226\n",
            "Epoch 579/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0580 - r_square: 0.8908\n",
            "Epoch 579: val_loss did not improve from 0.08651\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0576 - r_square: 0.8926 - val_loss: 0.0868 - val_r_square: 0.9220\n",
            "Epoch 580/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0577 - r_square: 0.8931\n",
            "Epoch 580: val_loss did not improve from 0.08651\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0576 - r_square: 0.8923 - val_loss: 0.0865 - val_r_square: 0.9227\n",
            "Epoch 581/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0578 - r_square: 0.8990\n",
            "Epoch 581: val_loss did not improve from 0.08651\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0574 - r_square: 0.8920 - val_loss: 0.0867 - val_r_square: 0.9225\n",
            "Epoch 582/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0580 - r_square: 0.8897\n",
            "Epoch 582: val_loss improved from 0.08651 to 0.08647, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0573 - r_square: 0.8926 - val_loss: 0.0865 - val_r_square: 0.9227\n",
            "Epoch 583/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0575 - r_square: 0.8918\n",
            "Epoch 583: val_loss did not improve from 0.08647\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0573 - r_square: 0.8927 - val_loss: 0.0865 - val_r_square: 0.9229\n",
            "Epoch 584/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0580 - r_square: 0.8917\n",
            "Epoch 584: val_loss improved from 0.08647 to 0.08642, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0575 - r_square: 0.8924 - val_loss: 0.0864 - val_r_square: 0.9231\n",
            "Epoch 585/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0579 - r_square: 0.8916\n",
            "Epoch 585: val_loss did not improve from 0.08642\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0572 - r_square: 0.8931 - val_loss: 0.0866 - val_r_square: 0.9231\n",
            "Epoch 586/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0580 - r_square: 0.8882\n",
            "Epoch 586: val_loss did not improve from 0.08642\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0572 - r_square: 0.8933 - val_loss: 0.0865 - val_r_square: 0.9226\n",
            "Epoch 587/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0575 - r_square: 0.8919\n",
            "Epoch 587: val_loss improved from 0.08642 to 0.08630, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0574 - r_square: 0.8933 - val_loss: 0.0863 - val_r_square: 0.9229\n",
            "Epoch 588/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0576 - r_square: 0.9030\n",
            "Epoch 588: val_loss improved from 0.08630 to 0.08623, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0573 - r_square: 0.8931 - val_loss: 0.0862 - val_r_square: 0.9230\n",
            "Epoch 589/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0571 - r_square: 0.8976\n",
            "Epoch 589: val_loss did not improve from 0.08623\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0572 - r_square: 0.8935 - val_loss: 0.0871 - val_r_square: 0.9234\n",
            "Epoch 590/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0570 - r_square: 0.8937\n",
            "Epoch 590: val_loss did not improve from 0.08623\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0571 - r_square: 0.8937 - val_loss: 0.0867 - val_r_square: 0.9234\n",
            "Epoch 591/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0586 - r_square: 0.8859\n",
            "Epoch 591: val_loss improved from 0.08623 to 0.08620, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0571 - r_square: 0.8940 - val_loss: 0.0862 - val_r_square: 0.9230\n",
            "Epoch 592/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0573 - r_square: 0.8943\n",
            "Epoch 592: val_loss improved from 0.08620 to 0.08607, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0569 - r_square: 0.8938 - val_loss: 0.0861 - val_r_square: 0.9233\n",
            "Epoch 593/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0576 - r_square: 0.8913\n",
            "Epoch 593: val_loss did not improve from 0.08607\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0570 - r_square: 0.8945 - val_loss: 0.0862 - val_r_square: 0.9234\n",
            "Epoch 594/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0567 - r_square: 0.8954\n",
            "Epoch 594: val_loss did not improve from 0.08607\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0572 - r_square: 0.8945 - val_loss: 0.0866 - val_r_square: 0.9227\n",
            "Epoch 595/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0565 - r_square: 0.8840\n",
            "Epoch 595: val_loss improved from 0.08607 to 0.08594, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0569 - r_square: 0.8946 - val_loss: 0.0859 - val_r_square: 0.9234\n",
            "Epoch 596/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0573 - r_square: 0.8986\n",
            "Epoch 596: val_loss did not improve from 0.08594\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0569 - r_square: 0.8943 - val_loss: 0.0861 - val_r_square: 0.9235\n",
            "Epoch 597/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0563 - r_square: 0.8993\n",
            "Epoch 597: val_loss did not improve from 0.08594\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0569 - r_square: 0.8945 - val_loss: 0.0861 - val_r_square: 0.9234\n",
            "Epoch 598/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0567 - r_square: 0.8931\n",
            "Epoch 598: val_loss did not improve from 0.08594\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0568 - r_square: 0.8955 - val_loss: 0.0862 - val_r_square: 0.9236\n",
            "Epoch 599/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0567 - r_square: 0.8931\n",
            "Epoch 599: val_loss did not improve from 0.08594\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0568 - r_square: 0.8945 - val_loss: 0.0860 - val_r_square: 0.9238\n",
            "Epoch 600/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0573 - r_square: 0.8963\n",
            "Epoch 600: val_loss did not improve from 0.08594\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0567 - r_square: 0.8952 - val_loss: 0.0863 - val_r_square: 0.9238\n",
            "Epoch 601/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0573 - r_square: 0.8945\n",
            "Epoch 601: val_loss improved from 0.08594 to 0.08591, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0567 - r_square: 0.8954 - val_loss: 0.0859 - val_r_square: 0.9234\n",
            "Epoch 602/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0552 - r_square: 0.8998\n",
            "Epoch 602: val_loss did not improve from 0.08591\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0568 - r_square: 0.8954 - val_loss: 0.0873 - val_r_square: 0.9220\n",
            "Epoch 603/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0561 - r_square: 0.8893\n",
            "Epoch 603: val_loss did not improve from 0.08591\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0566 - r_square: 0.8962 - val_loss: 0.0860 - val_r_square: 0.9237\n",
            "Epoch 604/1500\n",
            "49/67 [====================>.........] - ETA: 0s - loss: 0.0559 - r_square: 0.8976\n",
            "Epoch 604: val_loss did not improve from 0.08591\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0567 - r_square: 0.8946 - val_loss: 0.0866 - val_r_square: 0.9239\n",
            "Epoch 605/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0551 - r_square: 0.8881\n",
            "Epoch 605: val_loss did not improve from 0.08591\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0568 - r_square: 0.8957 - val_loss: 0.0860 - val_r_square: 0.9237\n",
            "Epoch 606/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0576 - r_square: 0.8966\n",
            "Epoch 606: val_loss improved from 0.08591 to 0.08582, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0565 - r_square: 0.8960 - val_loss: 0.0858 - val_r_square: 0.9236\n",
            "Epoch 607/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0563 - r_square: 0.8920\n",
            "Epoch 607: val_loss did not improve from 0.08582\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0565 - r_square: 0.8961 - val_loss: 0.0861 - val_r_square: 0.9233\n",
            "Epoch 608/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0577 - r_square: 0.8951\n",
            "Epoch 608: val_loss did not improve from 0.08582\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0565 - r_square: 0.8959 - val_loss: 0.0861 - val_r_square: 0.9241\n",
            "Epoch 609/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0540 - r_square: 0.9064\n",
            "Epoch 609: val_loss improved from 0.08582 to 0.08571, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0563 - r_square: 0.8965 - val_loss: 0.0857 - val_r_square: 0.9237\n",
            "Epoch 610/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0566 - r_square: 0.8971\n",
            "Epoch 610: val_loss improved from 0.08571 to 0.08567, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0564 - r_square: 0.8960 - val_loss: 0.0857 - val_r_square: 0.9242\n",
            "Epoch 611/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0568 - r_square: 0.8988\n",
            "Epoch 611: val_loss did not improve from 0.08567\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0564 - r_square: 0.8964 - val_loss: 0.0863 - val_r_square: 0.9242\n",
            "Epoch 612/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0556 - r_square: 0.8963\n",
            "Epoch 612: val_loss did not improve from 0.08567\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0566 - r_square: 0.8966 - val_loss: 0.0859 - val_r_square: 0.9238\n",
            "Epoch 613/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0581 - r_square: 0.9021\n",
            "Epoch 613: val_loss did not improve from 0.08567\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0563 - r_square: 0.8965 - val_loss: 0.0858 - val_r_square: 0.9241\n",
            "Epoch 614/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0560 - r_square: 0.9008\n",
            "Epoch 614: val_loss improved from 0.08567 to 0.08556, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0562 - r_square: 0.8973 - val_loss: 0.0856 - val_r_square: 0.9238\n",
            "Epoch 615/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0559 - r_square: 0.8979\n",
            "Epoch 615: val_loss did not improve from 0.08556\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0562 - r_square: 0.8973 - val_loss: 0.0856 - val_r_square: 0.9242\n",
            "Epoch 616/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0565 - r_square: 0.9009\n",
            "Epoch 616: val_loss did not improve from 0.08556\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0562 - r_square: 0.8972 - val_loss: 0.0856 - val_r_square: 0.9244\n",
            "Epoch 617/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0576 - r_square: 0.8905\n",
            "Epoch 617: val_loss improved from 0.08556 to 0.08553, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0564 - r_square: 0.8972 - val_loss: 0.0855 - val_r_square: 0.9242\n",
            "Epoch 618/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0558 - r_square: 0.8976\n",
            "Epoch 618: val_loss did not improve from 0.08553\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0562 - r_square: 0.8976 - val_loss: 0.0859 - val_r_square: 0.9244\n",
            "Epoch 619/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0543 - r_square: 0.8910\n",
            "Epoch 619: val_loss did not improve from 0.08553\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0561 - r_square: 0.8975 - val_loss: 0.0861 - val_r_square: 0.9237\n",
            "Epoch 620/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0562 - r_square: 0.8973\n",
            "Epoch 620: val_loss improved from 0.08553 to 0.08543, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0562 - r_square: 0.8973 - val_loss: 0.0854 - val_r_square: 0.9245\n",
            "Epoch 621/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0547 - r_square: 0.8947\n",
            "Epoch 621: val_loss did not improve from 0.08543\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0562 - r_square: 0.8973 - val_loss: 0.0855 - val_r_square: 0.9242\n",
            "Epoch 622/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0568 - r_square: 0.8932\n",
            "Epoch 622: val_loss did not improve from 0.08543\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0559 - r_square: 0.8978 - val_loss: 0.0854 - val_r_square: 0.9244\n",
            "Epoch 623/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0556 - r_square: 0.9026\n",
            "Epoch 623: val_loss did not improve from 0.08543\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0563 - r_square: 0.8981 - val_loss: 0.0860 - val_r_square: 0.9244\n",
            "Epoch 624/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0562 - r_square: 0.8961\n",
            "Epoch 624: val_loss improved from 0.08543 to 0.08543, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0559 - r_square: 0.8979 - val_loss: 0.0854 - val_r_square: 0.9242\n",
            "Epoch 625/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0555 - r_square: 0.8977\n",
            "Epoch 625: val_loss did not improve from 0.08543\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0558 - r_square: 0.8984 - val_loss: 0.0855 - val_r_square: 0.9243\n",
            "Epoch 626/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0563 - r_square: 0.8974\n",
            "Epoch 626: val_loss improved from 0.08543 to 0.08528, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0561 - r_square: 0.8983 - val_loss: 0.0853 - val_r_square: 0.9243\n",
            "Epoch 627/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0555 - r_square: 0.8983\n",
            "Epoch 627: val_loss did not improve from 0.08528\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0557 - r_square: 0.8989 - val_loss: 0.0853 - val_r_square: 0.9246\n",
            "Epoch 628/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0545 - r_square: 0.9072\n",
            "Epoch 628: val_loss did not improve from 0.08528\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0558 - r_square: 0.8988 - val_loss: 0.0854 - val_r_square: 0.9245\n",
            "Epoch 629/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0558 - r_square: 0.8978\n",
            "Epoch 629: val_loss improved from 0.08528 to 0.08518, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0557 - r_square: 0.8986 - val_loss: 0.0852 - val_r_square: 0.9246\n",
            "Epoch 630/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0547 - r_square: 0.8990\n",
            "Epoch 630: val_loss did not improve from 0.08518\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0558 - r_square: 0.8990 - val_loss: 0.0853 - val_r_square: 0.9245\n",
            "Epoch 631/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0544 - r_square: 0.8981\n",
            "Epoch 631: val_loss did not improve from 0.08518\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0557 - r_square: 0.8990 - val_loss: 0.0854 - val_r_square: 0.9241\n",
            "Epoch 632/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0554 - r_square: 0.8988\n",
            "Epoch 632: val_loss did not improve from 0.08518\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0559 - r_square: 0.8988 - val_loss: 0.0853 - val_r_square: 0.9243\n",
            "Epoch 633/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0555 - r_square: 0.9002\n",
            "Epoch 633: val_loss did not improve from 0.08518\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0556 - r_square: 0.8994 - val_loss: 0.0852 - val_r_square: 0.9246\n",
            "Epoch 634/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0565 - r_square: 0.9022\n",
            "Epoch 634: val_loss improved from 0.08518 to 0.08512, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0555 - r_square: 0.8999 - val_loss: 0.0851 - val_r_square: 0.9246\n",
            "Epoch 635/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0540 - r_square: 0.9061\n",
            "Epoch 635: val_loss did not improve from 0.08512\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0554 - r_square: 0.8995 - val_loss: 0.0851 - val_r_square: 0.9247\n",
            "Epoch 636/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0541 - r_square: 0.8985\n",
            "Epoch 636: val_loss did not improve from 0.08512\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0555 - r_square: 0.9000 - val_loss: 0.0856 - val_r_square: 0.9239\n",
            "Epoch 637/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0553 - r_square: 0.8973\n",
            "Epoch 637: val_loss did not improve from 0.08512\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0555 - r_square: 0.8997 - val_loss: 0.0852 - val_r_square: 0.9246\n",
            "Epoch 638/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0556 - r_square: 0.8995\n",
            "Epoch 638: val_loss improved from 0.08512 to 0.08498, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0555 - r_square: 0.8998 - val_loss: 0.0850 - val_r_square: 0.9246\n",
            "Epoch 639/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0547 - r_square: 0.8974\n",
            "Epoch 639: val_loss did not improve from 0.08498\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0557 - r_square: 0.9000 - val_loss: 0.0851 - val_r_square: 0.9244\n",
            "Epoch 640/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0551 - r_square: 0.9035\n",
            "Epoch 640: val_loss did not improve from 0.08498\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0555 - r_square: 0.9001 - val_loss: 0.0851 - val_r_square: 0.9245\n",
            "Epoch 641/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0555 - r_square: 0.9007\n",
            "Epoch 641: val_loss improved from 0.08498 to 0.08487, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0553 - r_square: 0.9005 - val_loss: 0.0849 - val_r_square: 0.9248\n",
            "Epoch 642/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0554 - r_square: 0.9009\n",
            "Epoch 642: val_loss did not improve from 0.08487\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0553 - r_square: 0.9003 - val_loss: 0.0850 - val_r_square: 0.9249\n",
            "Epoch 643/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0551 - r_square: 0.8993\n",
            "Epoch 643: val_loss did not improve from 0.08487\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0553 - r_square: 0.9003 - val_loss: 0.0851 - val_r_square: 0.9249\n",
            "Epoch 644/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0553 - r_square: 0.9002\n",
            "Epoch 644: val_loss did not improve from 0.08487\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0554 - r_square: 0.9011 - val_loss: 0.0849 - val_r_square: 0.9248\n",
            "Epoch 645/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0552 - r_square: 0.9002\n",
            "Epoch 645: val_loss did not improve from 0.08487\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0552 - r_square: 0.9002 - val_loss: 0.0851 - val_r_square: 0.9250\n",
            "Epoch 646/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0553 - r_square: 0.9011\n",
            "Epoch 646: val_loss did not improve from 0.08487\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0553 - r_square: 0.9011 - val_loss: 0.0853 - val_r_square: 0.9243\n",
            "Epoch 647/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0553 - r_square: 0.9006\n",
            "Epoch 647: val_loss did not improve from 0.08487\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0553 - r_square: 0.9006 - val_loss: 0.0850 - val_r_square: 0.9244\n",
            "Epoch 648/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0546 - r_square: 0.9100\n",
            "Epoch 648: val_loss improved from 0.08487 to 0.08485, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0551 - r_square: 0.9011 - val_loss: 0.0849 - val_r_square: 0.9249\n",
            "Epoch 649/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0529 - r_square: 0.9115\n",
            "Epoch 649: val_loss did not improve from 0.08485\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0551 - r_square: 0.9010 - val_loss: 0.0850 - val_r_square: 0.9251\n",
            "Epoch 650/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0541 - r_square: 0.8970\n",
            "Epoch 650: val_loss improved from 0.08485 to 0.08468, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0553 - r_square: 0.9015 - val_loss: 0.0847 - val_r_square: 0.9249\n",
            "Epoch 651/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0547 - r_square: 0.9005\n",
            "Epoch 651: val_loss did not improve from 0.08468\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0551 - r_square: 0.9017 - val_loss: 0.0848 - val_r_square: 0.9248\n",
            "Epoch 652/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0543 - r_square: 0.8996\n",
            "Epoch 652: val_loss improved from 0.08468 to 0.08464, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0550 - r_square: 0.9016 - val_loss: 0.0846 - val_r_square: 0.9249\n",
            "Epoch 653/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0549 - r_square: 0.9047\n",
            "Epoch 653: val_loss did not improve from 0.08464\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0550 - r_square: 0.9015 - val_loss: 0.0848 - val_r_square: 0.9249\n",
            "Epoch 654/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0563 - r_square: 0.8976\n",
            "Epoch 654: val_loss did not improve from 0.08464\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0549 - r_square: 0.9022 - val_loss: 0.0847 - val_r_square: 0.9251\n",
            "Epoch 655/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0541 - r_square: 0.9027\n",
            "Epoch 655: val_loss did not improve from 0.08464\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0549 - r_square: 0.9020 - val_loss: 0.0848 - val_r_square: 0.9250\n",
            "Epoch 656/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0543 - r_square: 0.9099\n",
            "Epoch 656: val_loss did not improve from 0.08464\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0549 - r_square: 0.9021 - val_loss: 0.0847 - val_r_square: 0.9249\n",
            "Epoch 657/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0548 - r_square: 0.9025\n",
            "Epoch 657: val_loss did not improve from 0.08464\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0548 - r_square: 0.9025 - val_loss: 0.0847 - val_r_square: 0.9252\n",
            "Epoch 658/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0531 - r_square: 0.9065\n",
            "Epoch 658: val_loss improved from 0.08464 to 0.08454, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0548 - r_square: 0.9024 - val_loss: 0.0845 - val_r_square: 0.9251\n",
            "Epoch 659/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0546 - r_square: 0.9073\n",
            "Epoch 659: val_loss did not improve from 0.08454\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0547 - r_square: 0.9021 - val_loss: 0.0849 - val_r_square: 0.9252\n",
            "Epoch 660/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0552 - r_square: 0.9089\n",
            "Epoch 660: val_loss did not improve from 0.08454\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0547 - r_square: 0.9027 - val_loss: 0.0848 - val_r_square: 0.9246\n",
            "Epoch 661/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0546 - r_square: 0.9038\n",
            "Epoch 661: val_loss did not improve from 0.08454\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0547 - r_square: 0.9026 - val_loss: 0.0850 - val_r_square: 0.9247\n",
            "Epoch 662/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0556 - r_square: 0.9011\n",
            "Epoch 662: val_loss improved from 0.08454 to 0.08453, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0547 - r_square: 0.9026 - val_loss: 0.0845 - val_r_square: 0.9252\n",
            "Epoch 663/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0547 - r_square: 0.9020\n",
            "Epoch 663: val_loss did not improve from 0.08453\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0548 - r_square: 0.9026 - val_loss: 0.0845 - val_r_square: 0.9250\n",
            "Epoch 664/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0557 - r_square: 0.9001\n",
            "Epoch 664: val_loss did not improve from 0.08453\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0552 - r_square: 0.9030 - val_loss: 0.0847 - val_r_square: 0.9252\n",
            "Epoch 665/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0530 - r_square: 0.9058\n",
            "Epoch 665: val_loss did not improve from 0.08453\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0547 - r_square: 0.9031 - val_loss: 0.0846 - val_r_square: 0.9252\n",
            "Epoch 666/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0561 - r_square: 0.8952\n",
            "Epoch 666: val_loss improved from 0.08453 to 0.08437, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0546 - r_square: 0.9029 - val_loss: 0.0844 - val_r_square: 0.9252\n",
            "Epoch 667/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0540 - r_square: 0.9078\n",
            "Epoch 667: val_loss did not improve from 0.08437\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0548 - r_square: 0.9030 - val_loss: 0.0845 - val_r_square: 0.9252\n",
            "Epoch 668/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0546 - r_square: 0.9091\n",
            "Epoch 668: val_loss improved from 0.08437 to 0.08432, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0546 - r_square: 0.9034 - val_loss: 0.0843 - val_r_square: 0.9251\n",
            "Epoch 669/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0534 - r_square: 0.9074\n",
            "Epoch 669: val_loss did not improve from 0.08432\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0545 - r_square: 0.9038 - val_loss: 0.0849 - val_r_square: 0.9244\n",
            "Epoch 670/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0545 - r_square: 0.9031\n",
            "Epoch 670: val_loss did not improve from 0.08432\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0546 - r_square: 0.9037 - val_loss: 0.0844 - val_r_square: 0.9252\n",
            "Epoch 671/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0549 - r_square: 0.9029\n",
            "Epoch 671: val_loss did not improve from 0.08432\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0546 - r_square: 0.9035 - val_loss: 0.0845 - val_r_square: 0.9253\n",
            "Epoch 672/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0537 - r_square: 0.9051\n",
            "Epoch 672: val_loss improved from 0.08432 to 0.08429, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0544 - r_square: 0.9034 - val_loss: 0.0843 - val_r_square: 0.9252\n",
            "Epoch 673/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0550 - r_square: 0.9027\n",
            "Epoch 673: val_loss did not improve from 0.08429\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0544 - r_square: 0.9041 - val_loss: 0.0844 - val_r_square: 0.9253\n",
            "Epoch 674/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0559 - r_square: 0.9018\n",
            "Epoch 674: val_loss improved from 0.08429 to 0.08419, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0546 - r_square: 0.9038 - val_loss: 0.0842 - val_r_square: 0.9251\n",
            "Epoch 675/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0550 - r_square: 0.9037\n",
            "Epoch 675: val_loss did not improve from 0.08419\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0544 - r_square: 0.9040 - val_loss: 0.0846 - val_r_square: 0.9249\n",
            "Epoch 676/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0542 - r_square: 0.9033\n",
            "Epoch 676: val_loss did not improve from 0.08419\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0543 - r_square: 0.9043 - val_loss: 0.0843 - val_r_square: 0.9251\n",
            "Epoch 677/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0540 - r_square: 0.9033\n",
            "Epoch 677: val_loss did not improve from 0.08419\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0543 - r_square: 0.9042 - val_loss: 0.0845 - val_r_square: 0.9249\n",
            "Epoch 678/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0538 - r_square: 0.9021\n",
            "Epoch 678: val_loss did not improve from 0.08419\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0544 - r_square: 0.9044 - val_loss: 0.0843 - val_r_square: 0.9250\n",
            "Epoch 679/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0538 - r_square: 0.9096\n",
            "Epoch 679: val_loss did not improve from 0.08419\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0543 - r_square: 0.9041 - val_loss: 0.0842 - val_r_square: 0.9250\n",
            "Epoch 680/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0542 - r_square: 0.9081\n",
            "Epoch 680: val_loss did not improve from 0.08419\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0542 - r_square: 0.9045 - val_loss: 0.0842 - val_r_square: 0.9251\n",
            "Epoch 681/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0543 - r_square: 0.9040\n",
            "Epoch 681: val_loss improved from 0.08419 to 0.08410, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0542 - r_square: 0.9044 - val_loss: 0.0841 - val_r_square: 0.9254\n",
            "Epoch 682/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0536 - r_square: 0.8989\n",
            "Epoch 682: val_loss did not improve from 0.08410\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0541 - r_square: 0.9047 - val_loss: 0.0842 - val_r_square: 0.9253\n",
            "Epoch 683/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0544 - r_square: 0.9045\n",
            "Epoch 683: val_loss did not improve from 0.08410\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0542 - r_square: 0.9046 - val_loss: 0.0842 - val_r_square: 0.9255\n",
            "Epoch 684/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0535 - r_square: 0.9049\n",
            "Epoch 684: val_loss did not improve from 0.08410\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0541 - r_square: 0.9044 - val_loss: 0.0842 - val_r_square: 0.9254\n",
            "Epoch 685/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0543 - r_square: 0.9038\n",
            "Epoch 685: val_loss did not improve from 0.08410\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0541 - r_square: 0.9049 - val_loss: 0.0841 - val_r_square: 0.9252\n",
            "Epoch 686/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0549 - r_square: 0.9093\n",
            "Epoch 686: val_loss improved from 0.08410 to 0.08407, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0543 - r_square: 0.9046 - val_loss: 0.0841 - val_r_square: 0.9254\n",
            "Epoch 687/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0545 - r_square: 0.9051\n",
            "Epoch 687: val_loss did not improve from 0.08407\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0541 - r_square: 0.9051 - val_loss: 0.0844 - val_r_square: 0.9250\n",
            "Epoch 688/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0541 - r_square: 0.9044\n",
            "Epoch 688: val_loss did not improve from 0.08407\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0541 - r_square: 0.9053 - val_loss: 0.0842 - val_r_square: 0.9253\n",
            "Epoch 689/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0541 - r_square: 0.9075\n",
            "Epoch 689: val_loss did not improve from 0.08407\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0539 - r_square: 0.9054 - val_loss: 0.0846 - val_r_square: 0.9250\n",
            "Epoch 690/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0506 - r_square: 0.9090\n",
            "Epoch 690: val_loss improved from 0.08407 to 0.08396, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0541 - r_square: 0.9052 - val_loss: 0.0840 - val_r_square: 0.9253\n",
            "Epoch 691/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0544 - r_square: 0.9030\n",
            "Epoch 691: val_loss did not improve from 0.08396\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0539 - r_square: 0.9056 - val_loss: 0.0840 - val_r_square: 0.9253\n",
            "Epoch 692/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0520 - r_square: 0.9071\n",
            "Epoch 692: val_loss improved from 0.08396 to 0.08386, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0539 - r_square: 0.9052 - val_loss: 0.0839 - val_r_square: 0.9255\n",
            "Epoch 693/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0533 - r_square: 0.9044\n",
            "Epoch 693: val_loss did not improve from 0.08386\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0539 - r_square: 0.9056 - val_loss: 0.0840 - val_r_square: 0.9254\n",
            "Epoch 694/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0534 - r_square: 0.9097\n",
            "Epoch 694: val_loss did not improve from 0.08386\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0538 - r_square: 0.9054 - val_loss: 0.0846 - val_r_square: 0.9252\n",
            "Epoch 695/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0540 - r_square: 0.9047\n",
            "Epoch 695: val_loss improved from 0.08386 to 0.08382, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0539 - r_square: 0.9058 - val_loss: 0.0838 - val_r_square: 0.9256\n",
            "Epoch 696/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0542 - r_square: 0.9050\n",
            "Epoch 696: val_loss did not improve from 0.08382\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0539 - r_square: 0.9054 - val_loss: 0.0843 - val_r_square: 0.9256\n",
            "Epoch 697/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0539 - r_square: 0.9059\n",
            "Epoch 697: val_loss did not improve from 0.08382\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0539 - r_square: 0.9059 - val_loss: 0.0841 - val_r_square: 0.9255\n",
            "Epoch 698/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0535 - r_square: 0.9024\n",
            "Epoch 698: val_loss did not improve from 0.08382\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0537 - r_square: 0.9063 - val_loss: 0.0839 - val_r_square: 0.9255\n",
            "Epoch 699/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0532 - r_square: 0.9096\n",
            "Epoch 699: val_loss did not improve from 0.08382\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0538 - r_square: 0.9062 - val_loss: 0.0839 - val_r_square: 0.9255\n",
            "Epoch 700/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0539 - r_square: 0.9055\n",
            "Epoch 700: val_loss did not improve from 0.08382\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0537 - r_square: 0.9067 - val_loss: 0.0842 - val_r_square: 0.9250\n",
            "Epoch 701/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0539 - r_square: 0.9055\n",
            "Epoch 701: val_loss did not improve from 0.08382\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0538 - r_square: 0.9058 - val_loss: 0.0845 - val_r_square: 0.9255\n",
            "Epoch 702/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0537 - r_square: 0.9065\n",
            "Epoch 702: val_loss did not improve from 0.08382\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0537 - r_square: 0.9065 - val_loss: 0.0841 - val_r_square: 0.9253\n",
            "Epoch 703/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0538 - r_square: 0.9057\n",
            "Epoch 703: val_loss did not improve from 0.08382\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0537 - r_square: 0.9063 - val_loss: 0.0848 - val_r_square: 0.9248\n",
            "Epoch 704/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0540 - r_square: 0.9102\n",
            "Epoch 704: val_loss improved from 0.08382 to 0.08373, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0537 - r_square: 0.9065 - val_loss: 0.0837 - val_r_square: 0.9256\n",
            "Epoch 705/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0534 - r_square: 0.9068\n",
            "Epoch 705: val_loss did not improve from 0.08373\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0535 - r_square: 0.9067 - val_loss: 0.0837 - val_r_square: 0.9255\n",
            "Epoch 706/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0544 - r_square: 0.9072\n",
            "Epoch 706: val_loss did not improve from 0.08373\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0536 - r_square: 0.9063 - val_loss: 0.0839 - val_r_square: 0.9253\n",
            "Epoch 707/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0537 - r_square: 0.9070\n",
            "Epoch 707: val_loss did not improve from 0.08373\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0537 - r_square: 0.9065 - val_loss: 0.0840 - val_r_square: 0.9254\n",
            "Epoch 708/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0530 - r_square: 0.9062\n",
            "Epoch 708: val_loss did not improve from 0.08373\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0536 - r_square: 0.9073 - val_loss: 0.0838 - val_r_square: 0.9256\n",
            "Epoch 709/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0532 - r_square: 0.9132\n",
            "Epoch 709: val_loss did not improve from 0.08373\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0537 - r_square: 0.9068 - val_loss: 0.0838 - val_r_square: 0.9254\n",
            "Epoch 710/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0536 - r_square: 0.9065\n",
            "Epoch 710: val_loss did not improve from 0.08373\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0535 - r_square: 0.9069 - val_loss: 0.0838 - val_r_square: 0.9258\n",
            "Epoch 711/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0536 - r_square: 0.9072\n",
            "Epoch 711: val_loss did not improve from 0.08373\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0536 - r_square: 0.9072 - val_loss: 0.0840 - val_r_square: 0.9256\n",
            "Epoch 712/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0547 - r_square: 0.9086\n",
            "Epoch 712: val_loss improved from 0.08373 to 0.08364, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0535 - r_square: 0.9072 - val_loss: 0.0836 - val_r_square: 0.9257\n",
            "Epoch 713/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0531 - r_square: 0.9055\n",
            "Epoch 713: val_loss did not improve from 0.08364\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0533 - r_square: 0.9074 - val_loss: 0.0837 - val_r_square: 0.9256\n",
            "Epoch 714/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0532 - r_square: 0.9096\n",
            "Epoch 714: val_loss did not improve from 0.08364\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0535 - r_square: 0.9072 - val_loss: 0.0838 - val_r_square: 0.9257\n",
            "Epoch 715/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0529 - r_square: 0.9110\n",
            "Epoch 715: val_loss did not improve from 0.08364\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0534 - r_square: 0.9077 - val_loss: 0.0841 - val_r_square: 0.9252\n",
            "Epoch 716/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0531 - r_square: 0.9090\n",
            "Epoch 716: val_loss improved from 0.08364 to 0.08359, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0533 - r_square: 0.9075 - val_loss: 0.0836 - val_r_square: 0.9257\n",
            "Epoch 717/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0534 - r_square: 0.9070\n",
            "Epoch 717: val_loss improved from 0.08359 to 0.08348, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0533 - r_square: 0.9078 - val_loss: 0.0835 - val_r_square: 0.9257\n",
            "Epoch 718/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0530 - r_square: 0.9081\n",
            "Epoch 718: val_loss did not improve from 0.08348\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0535 - r_square: 0.9081 - val_loss: 0.0836 - val_r_square: 0.9255\n",
            "Epoch 719/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0519 - r_square: 0.9055\n",
            "Epoch 719: val_loss did not improve from 0.08348\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0532 - r_square: 0.9079 - val_loss: 0.0836 - val_r_square: 0.9255\n",
            "Epoch 720/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0539 - r_square: 0.9054\n",
            "Epoch 720: val_loss did not improve from 0.08348\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0535 - r_square: 0.9076 - val_loss: 0.0838 - val_r_square: 0.9256\n",
            "Epoch 721/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0534 - r_square: 0.9083\n",
            "Epoch 721: val_loss did not improve from 0.08348\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0533 - r_square: 0.9080 - val_loss: 0.0838 - val_r_square: 0.9253\n",
            "Epoch 722/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0533 - r_square: 0.9105\n",
            "Epoch 722: val_loss did not improve from 0.08348\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0532 - r_square: 0.9084 - val_loss: 0.0839 - val_r_square: 0.9252\n",
            "Epoch 723/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0532 - r_square: 0.9044\n",
            "Epoch 723: val_loss did not improve from 0.08348\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0532 - r_square: 0.9084 - val_loss: 0.0835 - val_r_square: 0.9257\n",
            "Epoch 724/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0534 - r_square: 0.9082\n",
            "Epoch 724: val_loss improved from 0.08348 to 0.08344, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0532 - r_square: 0.9081 - val_loss: 0.0834 - val_r_square: 0.9257\n",
            "Epoch 725/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0531 - r_square: 0.9082\n",
            "Epoch 725: val_loss did not improve from 0.08344\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0531 - r_square: 0.9082 - val_loss: 0.0838 - val_r_square: 0.9256\n",
            "Epoch 726/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0528 - r_square: 0.9097\n",
            "Epoch 726: val_loss did not improve from 0.08344\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0532 - r_square: 0.9082 - val_loss: 0.0845 - val_r_square: 0.9255\n",
            "Epoch 727/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0540 - r_square: 0.9158\n",
            "Epoch 727: val_loss improved from 0.08344 to 0.08328, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0533 - r_square: 0.9083 - val_loss: 0.0833 - val_r_square: 0.9258\n",
            "Epoch 728/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0519 - r_square: 0.9086\n",
            "Epoch 728: val_loss did not improve from 0.08328\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0532 - r_square: 0.9082 - val_loss: 0.0834 - val_r_square: 0.9257\n",
            "Epoch 729/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0523 - r_square: 0.9142\n",
            "Epoch 729: val_loss did not improve from 0.08328\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0530 - r_square: 0.9088 - val_loss: 0.0835 - val_r_square: 0.9257\n",
            "Epoch 730/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0535 - r_square: 0.9158\n",
            "Epoch 730: val_loss did not improve from 0.08328\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0529 - r_square: 0.9088 - val_loss: 0.0833 - val_r_square: 0.9257\n",
            "Epoch 731/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0533 - r_square: 0.9135\n",
            "Epoch 731: val_loss did not improve from 0.08328\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0530 - r_square: 0.9089 - val_loss: 0.0835 - val_r_square: 0.9256\n",
            "Epoch 732/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0540 - r_square: 0.9059\n",
            "Epoch 732: val_loss did not improve from 0.08328\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0530 - r_square: 0.9086 - val_loss: 0.0833 - val_r_square: 0.9258\n",
            "Epoch 733/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0540 - r_square: 0.9048\n",
            "Epoch 733: val_loss did not improve from 0.08328\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0528 - r_square: 0.9095 - val_loss: 0.0838 - val_r_square: 0.9252\n",
            "Epoch 734/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0530 - r_square: 0.9063\n",
            "Epoch 734: val_loss improved from 0.08328 to 0.08325, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0532 - r_square: 0.9088 - val_loss: 0.0833 - val_r_square: 0.9257\n",
            "Epoch 735/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0530 - r_square: 0.9086\n",
            "Epoch 735: val_loss did not improve from 0.08325\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0530 - r_square: 0.9089 - val_loss: 0.0833 - val_r_square: 0.9257\n",
            "Epoch 736/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0535 - r_square: 0.9071\n",
            "Epoch 736: val_loss did not improve from 0.08325\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0529 - r_square: 0.9090 - val_loss: 0.0834 - val_r_square: 0.9257\n",
            "Epoch 737/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0539 - r_square: 0.9059\n",
            "Epoch 737: val_loss did not improve from 0.08325\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0528 - r_square: 0.9095 - val_loss: 0.0833 - val_r_square: 0.9256\n",
            "Epoch 738/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0527 - r_square: 0.9132\n",
            "Epoch 738: val_loss did not improve from 0.08325\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0528 - r_square: 0.9094 - val_loss: 0.0839 - val_r_square: 0.9251\n",
            "Epoch 739/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0521 - r_square: 0.9092\n",
            "Epoch 739: val_loss improved from 0.08325 to 0.08323, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0529 - r_square: 0.9091 - val_loss: 0.0832 - val_r_square: 0.9256\n",
            "Epoch 740/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0521 - r_square: 0.9072\n",
            "Epoch 740: val_loss did not improve from 0.08323\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0528 - r_square: 0.9092 - val_loss: 0.0834 - val_r_square: 0.9257\n",
            "Epoch 741/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0529 - r_square: 0.9084\n",
            "Epoch 741: val_loss improved from 0.08323 to 0.08322, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0529 - r_square: 0.9096 - val_loss: 0.0832 - val_r_square: 0.9256\n",
            "Epoch 742/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0529 - r_square: 0.9124\n",
            "Epoch 742: val_loss improved from 0.08322 to 0.08321, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0527 - r_square: 0.9094 - val_loss: 0.0832 - val_r_square: 0.9257\n",
            "Epoch 743/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0522 - r_square: 0.9127\n",
            "Epoch 743: val_loss did not improve from 0.08321\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0527 - r_square: 0.9100 - val_loss: 0.0833 - val_r_square: 0.9257\n",
            "Epoch 744/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0536 - r_square: 0.9031\n",
            "Epoch 744: val_loss improved from 0.08321 to 0.08313, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0527 - r_square: 0.9096 - val_loss: 0.0831 - val_r_square: 0.9257\n",
            "Epoch 745/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0526 - r_square: 0.9124\n",
            "Epoch 745: val_loss did not improve from 0.08313\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0525 - r_square: 0.9098 - val_loss: 0.0834 - val_r_square: 0.9256\n",
            "Epoch 746/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0538 - r_square: 0.9011\n",
            "Epoch 746: val_loss did not improve from 0.08313\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0527 - r_square: 0.9101 - val_loss: 0.0832 - val_r_square: 0.9257\n",
            "Epoch 747/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0523 - r_square: 0.9006\n",
            "Epoch 747: val_loss did not improve from 0.08313\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0526 - r_square: 0.9102 - val_loss: 0.0834 - val_r_square: 0.9256\n",
            "Epoch 748/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0528 - r_square: 0.9086\n",
            "Epoch 748: val_loss did not improve from 0.08313\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0526 - r_square: 0.9093 - val_loss: 0.0834 - val_r_square: 0.9258\n",
            "Epoch 749/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0513 - r_square: 0.9075\n",
            "Epoch 749: val_loss improved from 0.08313 to 0.08303, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0526 - r_square: 0.9103 - val_loss: 0.0830 - val_r_square: 0.9257\n",
            "Epoch 750/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0519 - r_square: 0.9092\n",
            "Epoch 750: val_loss did not improve from 0.08303\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0525 - r_square: 0.9101 - val_loss: 0.0831 - val_r_square: 0.9256\n",
            "Epoch 751/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0523 - r_square: 0.9103\n",
            "Epoch 751: val_loss improved from 0.08303 to 0.08301, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0525 - r_square: 0.9108 - val_loss: 0.0830 - val_r_square: 0.9257\n",
            "Epoch 752/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0526 - r_square: 0.9108\n",
            "Epoch 752: val_loss did not improve from 0.08301\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0526 - r_square: 0.9108 - val_loss: 0.0835 - val_r_square: 0.9254\n",
            "Epoch 753/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0530 - r_square: 0.9075\n",
            "Epoch 753: val_loss did not improve from 0.08301\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0524 - r_square: 0.9104 - val_loss: 0.0834 - val_r_square: 0.9254\n",
            "Epoch 754/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0518 - r_square: 0.9091\n",
            "Epoch 754: val_loss did not improve from 0.08301\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0525 - r_square: 0.9104 - val_loss: 0.0831 - val_r_square: 0.9256\n",
            "Epoch 755/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0526 - r_square: 0.9111\n",
            "Epoch 755: val_loss improved from 0.08301 to 0.08290, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0524 - r_square: 0.9107 - val_loss: 0.0829 - val_r_square: 0.9257\n",
            "Epoch 756/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0524 - r_square: 0.9118\n",
            "Epoch 756: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0524 - r_square: 0.9107 - val_loss: 0.0830 - val_r_square: 0.9257\n",
            "Epoch 757/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0526 - r_square: 0.9102\n",
            "Epoch 757: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0526 - r_square: 0.9102 - val_loss: 0.0833 - val_r_square: 0.9255\n",
            "Epoch 758/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0524 - r_square: 0.9195\n",
            "Epoch 758: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0527 - r_square: 0.9105 - val_loss: 0.0834 - val_r_square: 0.9255\n",
            "Epoch 759/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0526 - r_square: 0.9106\n",
            "Epoch 759: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0523 - r_square: 0.9109 - val_loss: 0.0831 - val_r_square: 0.9256\n",
            "Epoch 760/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0539 - r_square: 0.9182\n",
            "Epoch 760: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0525 - r_square: 0.9106 - val_loss: 0.0830 - val_r_square: 0.9256\n",
            "Epoch 761/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0513 - r_square: 0.9127\n",
            "Epoch 761: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0523 - r_square: 0.9112 - val_loss: 0.0830 - val_r_square: 0.9257\n",
            "Epoch 762/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0530 - r_square: 0.9137\n",
            "Epoch 762: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0524 - r_square: 0.9109 - val_loss: 0.0829 - val_r_square: 0.9255\n",
            "Epoch 763/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0501 - r_square: 0.9255\n",
            "Epoch 763: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0523 - r_square: 0.9108 - val_loss: 0.0830 - val_r_square: 0.9257\n",
            "Epoch 764/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0519 - r_square: 0.9131\n",
            "Epoch 764: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0522 - r_square: 0.9111 - val_loss: 0.0831 - val_r_square: 0.9257\n",
            "Epoch 765/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0519 - r_square: 0.9080\n",
            "Epoch 765: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0523 - r_square: 0.9116 - val_loss: 0.0834 - val_r_square: 0.9257\n",
            "Epoch 766/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0521 - r_square: 0.9131\n",
            "Epoch 766: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0522 - r_square: 0.9110 - val_loss: 0.0835 - val_r_square: 0.9250\n",
            "Epoch 767/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0519 - r_square: 0.9118\n",
            "Epoch 767: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0521 - r_square: 0.9106 - val_loss: 0.0830 - val_r_square: 0.9256\n",
            "Epoch 768/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0512 - r_square: 0.9110\n",
            "Epoch 768: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0521 - r_square: 0.9113 - val_loss: 0.0830 - val_r_square: 0.9256\n",
            "Epoch 769/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0519 - r_square: 0.9150\n",
            "Epoch 769: val_loss did not improve from 0.08290\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0521 - r_square: 0.9111 - val_loss: 0.0831 - val_r_square: 0.9256\n",
            "Epoch 770/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0522 - r_square: 0.9114\n",
            "Epoch 770: val_loss improved from 0.08290 to 0.08269, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0522 - r_square: 0.9114 - val_loss: 0.0827 - val_r_square: 0.9256\n",
            "Epoch 771/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0522 - r_square: 0.9100\n",
            "Epoch 771: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.0520 - r_square: 0.9113 - val_loss: 0.0833 - val_r_square: 0.9254\n",
            "Epoch 772/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0517 - r_square: 0.9123\n",
            "Epoch 772: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0521 - r_square: 0.9116 - val_loss: 0.0828 - val_r_square: 0.9255\n",
            "Epoch 773/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0520 - r_square: 0.9117\n",
            "Epoch 773: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0521 - r_square: 0.9114 - val_loss: 0.0829 - val_r_square: 0.9256\n",
            "Epoch 774/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0523 - r_square: 0.9120\n",
            "Epoch 774: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0520 - r_square: 0.9119 - val_loss: 0.0827 - val_r_square: 0.9256\n",
            "Epoch 775/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0524 - r_square: 0.9104\n",
            "Epoch 775: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0521 - r_square: 0.9116 - val_loss: 0.0830 - val_r_square: 0.9256\n",
            "Epoch 776/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0520 - r_square: 0.9121\n",
            "Epoch 776: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0520 - r_square: 0.9121 - val_loss: 0.0831 - val_r_square: 0.9253\n",
            "Epoch 777/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0518 - r_square: 0.9092\n",
            "Epoch 777: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.0521 - r_square: 0.9111 - val_loss: 0.0828 - val_r_square: 0.9255\n",
            "Epoch 778/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0519 - r_square: 0.9110\n",
            "Epoch 778: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0520 - r_square: 0.9120 - val_loss: 0.0834 - val_r_square: 0.9252\n",
            "Epoch 779/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0527 - r_square: 0.9116\n",
            "Epoch 779: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0521 - r_square: 0.9122 - val_loss: 0.0829 - val_r_square: 0.9256\n",
            "Epoch 780/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0518 - r_square: 0.9109\n",
            "Epoch 780: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0519 - r_square: 0.9120 - val_loss: 0.0828 - val_r_square: 0.9255\n",
            "Epoch 781/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0521 - r_square: 0.9126\n",
            "Epoch 781: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0519 - r_square: 0.9124 - val_loss: 0.0831 - val_r_square: 0.9253\n",
            "Epoch 782/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0523 - r_square: 0.9107\n",
            "Epoch 782: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0519 - r_square: 0.9127 - val_loss: 0.0829 - val_r_square: 0.9255\n",
            "Epoch 783/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0519 - r_square: 0.9121\n",
            "Epoch 783: val_loss did not improve from 0.08269\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0519 - r_square: 0.9121 - val_loss: 0.0836 - val_r_square: 0.9251\n",
            "Epoch 784/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0518 - r_square: 0.9133\n",
            "Epoch 784: val_loss improved from 0.08269 to 0.08262, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0522 - r_square: 0.9124 - val_loss: 0.0826 - val_r_square: 0.9256\n",
            "Epoch 785/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0520 - r_square: 0.9102\n",
            "Epoch 785: val_loss improved from 0.08262 to 0.08250, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0518 - r_square: 0.9124 - val_loss: 0.0825 - val_r_square: 0.9256\n",
            "Epoch 786/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0526 - r_square: 0.9135\n",
            "Epoch 786: val_loss did not improve from 0.08250\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0517 - r_square: 0.9121 - val_loss: 0.0829 - val_r_square: 0.9254\n",
            "Epoch 787/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0519 - r_square: 0.9097\n",
            "Epoch 787: val_loss did not improve from 0.08250\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0519 - r_square: 0.9126 - val_loss: 0.0833 - val_r_square: 0.9252\n",
            "Epoch 788/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0510 - r_square: 0.9133\n",
            "Epoch 788: val_loss did not improve from 0.08250\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0518 - r_square: 0.9121 - val_loss: 0.0827 - val_r_square: 0.9255\n",
            "Epoch 789/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0508 - r_square: 0.9117\n",
            "Epoch 789: val_loss did not improve from 0.08250\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0519 - r_square: 0.9129 - val_loss: 0.0834 - val_r_square: 0.9251\n",
            "Epoch 790/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0514 - r_square: 0.9127\n",
            "Epoch 790: val_loss did not improve from 0.08250\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0518 - r_square: 0.9126 - val_loss: 0.0827 - val_r_square: 0.9257\n",
            "Epoch 791/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0527 - r_square: 0.9111\n",
            "Epoch 791: val_loss did not improve from 0.08250\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0519 - r_square: 0.9130 - val_loss: 0.0831 - val_r_square: 0.9253\n",
            "Epoch 792/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0523 - r_square: 0.9146\n",
            "Epoch 792: val_loss did not improve from 0.08250\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0518 - r_square: 0.9131 - val_loss: 0.0825 - val_r_square: 0.9254\n",
            "Epoch 793/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0521 - r_square: 0.9117\n",
            "Epoch 793: val_loss did not improve from 0.08250\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0517 - r_square: 0.9129 - val_loss: 0.0827 - val_r_square: 0.9255\n",
            "Epoch 794/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0516 - r_square: 0.9114\n",
            "Epoch 794: val_loss improved from 0.08250 to 0.08231, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0520 - r_square: 0.9123 - val_loss: 0.0823 - val_r_square: 0.9256\n",
            "Epoch 795/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0509 - r_square: 0.9119\n",
            "Epoch 795: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0516 - r_square: 0.9128 - val_loss: 0.0829 - val_r_square: 0.9254\n",
            "Epoch 796/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0516 - r_square: 0.9117\n",
            "Epoch 796: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0519 - r_square: 0.9130 - val_loss: 0.0825 - val_r_square: 0.9255\n",
            "Epoch 797/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0513 - r_square: 0.9176\n",
            "Epoch 797: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0515 - r_square: 0.9133 - val_loss: 0.0827 - val_r_square: 0.9255\n",
            "Epoch 798/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0505 - r_square: 0.9211\n",
            "Epoch 798: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0516 - r_square: 0.9126 - val_loss: 0.0824 - val_r_square: 0.9254\n",
            "Epoch 799/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0514 - r_square: 0.9139\n",
            "Epoch 799: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0515 - r_square: 0.9131 - val_loss: 0.0825 - val_r_square: 0.9256\n",
            "Epoch 800/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0518 - r_square: 0.9122\n",
            "Epoch 800: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0515 - r_square: 0.9133 - val_loss: 0.0825 - val_r_square: 0.9255\n",
            "Epoch 801/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0511 - r_square: 0.9117\n",
            "Epoch 801: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0514 - r_square: 0.9132 - val_loss: 0.0828 - val_r_square: 0.9255\n",
            "Epoch 802/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0518 - r_square: 0.9148\n",
            "Epoch 802: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0516 - r_square: 0.9136 - val_loss: 0.0827 - val_r_square: 0.9254\n",
            "Epoch 803/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0516 - r_square: 0.9136\n",
            "Epoch 803: val_loss improved from 0.08231 to 0.08231, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0516 - r_square: 0.9136 - val_loss: 0.0823 - val_r_square: 0.9255\n",
            "Epoch 804/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0517 - r_square: 0.9169\n",
            "Epoch 804: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0515 - r_square: 0.9135 - val_loss: 0.0824 - val_r_square: 0.9256\n",
            "Epoch 805/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0510 - r_square: 0.9176\n",
            "Epoch 805: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0514 - r_square: 0.9137 - val_loss: 0.0826 - val_r_square: 0.9254\n",
            "Epoch 806/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0508 - r_square: 0.9134\n",
            "Epoch 806: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0514 - r_square: 0.9139 - val_loss: 0.0824 - val_r_square: 0.9255\n",
            "Epoch 807/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0494 - r_square: 0.9130\n",
            "Epoch 807: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0512 - r_square: 0.9137 - val_loss: 0.0830 - val_r_square: 0.9252\n",
            "Epoch 808/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0514 - r_square: 0.9138\n",
            "Epoch 808: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0514 - r_square: 0.9138 - val_loss: 0.0825 - val_r_square: 0.9254\n",
            "Epoch 809/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0499 - r_square: 0.9229\n",
            "Epoch 809: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0514 - r_square: 0.9140 - val_loss: 0.0831 - val_r_square: 0.9250\n",
            "Epoch 810/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0514 - r_square: 0.9139\n",
            "Epoch 810: val_loss did not improve from 0.08231\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0514 - r_square: 0.9139 - val_loss: 0.0825 - val_r_square: 0.9254\n",
            "Epoch 811/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0523 - r_square: 0.9078\n",
            "Epoch 811: val_loss improved from 0.08231 to 0.08223, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0514 - r_square: 0.9131 - val_loss: 0.0822 - val_r_square: 0.9254\n",
            "Epoch 812/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0510 - r_square: 0.9122\n",
            "Epoch 812: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0513 - r_square: 0.9145 - val_loss: 0.0827 - val_r_square: 0.9252\n",
            "Epoch 813/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0512 - r_square: 0.9143\n",
            "Epoch 813: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0512 - r_square: 0.9143 - val_loss: 0.0826 - val_r_square: 0.9253\n",
            "Epoch 814/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0513 - r_square: 0.9139\n",
            "Epoch 814: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0511 - r_square: 0.9144 - val_loss: 0.0830 - val_r_square: 0.9252\n",
            "Epoch 815/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0511 - r_square: 0.9117\n",
            "Epoch 815: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0514 - r_square: 0.9140 - val_loss: 0.0825 - val_r_square: 0.9252\n",
            "Epoch 816/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0503 - r_square: 0.9168\n",
            "Epoch 816: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0511 - r_square: 0.9142 - val_loss: 0.0824 - val_r_square: 0.9254\n",
            "Epoch 817/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0518 - r_square: 0.9086\n",
            "Epoch 817: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0512 - r_square: 0.9141 - val_loss: 0.0830 - val_r_square: 0.9249\n",
            "Epoch 818/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0511 - r_square: 0.9156\n",
            "Epoch 818: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0512 - r_square: 0.9143 - val_loss: 0.0826 - val_r_square: 0.9251\n",
            "Epoch 819/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0514 - r_square: 0.9142\n",
            "Epoch 819: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0512 - r_square: 0.9141 - val_loss: 0.0824 - val_r_square: 0.9253\n",
            "Epoch 820/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0511 - r_square: 0.9145\n",
            "Epoch 820: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0511 - r_square: 0.9145 - val_loss: 0.0822 - val_r_square: 0.9254\n",
            "Epoch 821/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0510 - r_square: 0.9120\n",
            "Epoch 821: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0513 - r_square: 0.9145 - val_loss: 0.0823 - val_r_square: 0.9254\n",
            "Epoch 822/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0515 - r_square: 0.9135\n",
            "Epoch 822: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0511 - r_square: 0.9146 - val_loss: 0.0831 - val_r_square: 0.9248\n",
            "Epoch 823/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0527 - r_square: 0.9100\n",
            "Epoch 823: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0513 - r_square: 0.9145 - val_loss: 0.0822 - val_r_square: 0.9253\n",
            "Epoch 824/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0509 - r_square: 0.9172\n",
            "Epoch 824: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0513 - r_square: 0.9148 - val_loss: 0.0823 - val_r_square: 0.9252\n",
            "Epoch 825/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0506 - r_square: 0.9152\n",
            "Epoch 825: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0510 - r_square: 0.9145 - val_loss: 0.0823 - val_r_square: 0.9253\n",
            "Epoch 826/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0510 - r_square: 0.9149\n",
            "Epoch 826: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0510 - r_square: 0.9149 - val_loss: 0.0822 - val_r_square: 0.9253\n",
            "Epoch 827/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0513 - r_square: 0.9143\n",
            "Epoch 827: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0511 - r_square: 0.9148 - val_loss: 0.0825 - val_r_square: 0.9253\n",
            "Epoch 828/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0504 - r_square: 0.9161\n",
            "Epoch 828: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0509 - r_square: 0.9150 - val_loss: 0.0825 - val_r_square: 0.9251\n",
            "Epoch 829/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0515 - r_square: 0.9166\n",
            "Epoch 829: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0513 - r_square: 0.9145 - val_loss: 0.0827 - val_r_square: 0.9251\n",
            "Epoch 830/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0510 - r_square: 0.9135\n",
            "Epoch 830: val_loss did not improve from 0.08223\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0510 - r_square: 0.9147 - val_loss: 0.0824 - val_r_square: 0.9253\n",
            "Epoch 831/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0511 - r_square: 0.9129\n",
            "Epoch 831: val_loss improved from 0.08223 to 0.08204, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0512 - r_square: 0.9146 - val_loss: 0.0820 - val_r_square: 0.9253\n",
            "Epoch 832/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0514 - r_square: 0.9146\n",
            "Epoch 832: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0509 - r_square: 0.9154 - val_loss: 0.0821 - val_r_square: 0.9253\n",
            "Epoch 833/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0499 - r_square: 0.9184\n",
            "Epoch 833: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0509 - r_square: 0.9151 - val_loss: 0.0824 - val_r_square: 0.9253\n",
            "Epoch 834/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0509 - r_square: 0.9132\n",
            "Epoch 834: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0510 - r_square: 0.9155 - val_loss: 0.0844 - val_r_square: 0.9239\n",
            "Epoch 835/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0502 - r_square: 0.9137\n",
            "Epoch 835: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0511 - r_square: 0.9144 - val_loss: 0.0822 - val_r_square: 0.9252\n",
            "Epoch 836/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0512 - r_square: 0.9117\n",
            "Epoch 836: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0508 - r_square: 0.9154 - val_loss: 0.0823 - val_r_square: 0.9252\n",
            "Epoch 837/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0505 - r_square: 0.9148\n",
            "Epoch 837: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0507 - r_square: 0.9154 - val_loss: 0.0824 - val_r_square: 0.9253\n",
            "Epoch 838/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0515 - r_square: 0.9134\n",
            "Epoch 838: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0509 - r_square: 0.9156 - val_loss: 0.0821 - val_r_square: 0.9252\n",
            "Epoch 839/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0513 - r_square: 0.9118\n",
            "Epoch 839: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0507 - r_square: 0.9152 - val_loss: 0.0824 - val_r_square: 0.9253\n",
            "Epoch 840/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0504 - r_square: 0.9172\n",
            "Epoch 840: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0508 - r_square: 0.9159 - val_loss: 0.0821 - val_r_square: 0.9251\n",
            "Epoch 841/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0508 - r_square: 0.9154\n",
            "Epoch 841: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0508 - r_square: 0.9154 - val_loss: 0.0824 - val_r_square: 0.9251\n",
            "Epoch 842/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0509 - r_square: 0.9157\n",
            "Epoch 842: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0507 - r_square: 0.9160 - val_loss: 0.0822 - val_r_square: 0.9250\n",
            "Epoch 843/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0514 - r_square: 0.9149\n",
            "Epoch 843: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0509 - r_square: 0.9151 - val_loss: 0.0825 - val_r_square: 0.9249\n",
            "Epoch 844/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0511 - r_square: 0.9164\n",
            "Epoch 844: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0506 - r_square: 0.9158 - val_loss: 0.0822 - val_r_square: 0.9251\n",
            "Epoch 845/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0514 - r_square: 0.9161\n",
            "Epoch 845: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0506 - r_square: 0.9157 - val_loss: 0.0822 - val_r_square: 0.9250\n",
            "Epoch 846/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0511 - r_square: 0.9161\n",
            "Epoch 846: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0508 - r_square: 0.9161 - val_loss: 0.0821 - val_r_square: 0.9251\n",
            "Epoch 847/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0507 - r_square: 0.9157\n",
            "Epoch 847: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0507 - r_square: 0.9157 - val_loss: 0.0821 - val_r_square: 0.9251\n",
            "Epoch 848/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0498 - r_square: 0.9203\n",
            "Epoch 848: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0506 - r_square: 0.9159 - val_loss: 0.0827 - val_r_square: 0.9246\n",
            "Epoch 849/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0508 - r_square: 0.9215\n",
            "Epoch 849: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0507 - r_square: 0.9160 - val_loss: 0.0822 - val_r_square: 0.9251\n",
            "Epoch 850/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0507 - r_square: 0.9165\n",
            "Epoch 850: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0506 - r_square: 0.9157 - val_loss: 0.0821 - val_r_square: 0.9249\n",
            "Epoch 851/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0506 - r_square: 0.9154\n",
            "Epoch 851: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0506 - r_square: 0.9154 - val_loss: 0.0821 - val_r_square: 0.9249\n",
            "Epoch 852/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0512 - r_square: 0.9189\n",
            "Epoch 852: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0507 - r_square: 0.9157 - val_loss: 0.0821 - val_r_square: 0.9250\n",
            "Epoch 853/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0506 - r_square: 0.9156\n",
            "Epoch 853: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0507 - r_square: 0.9162 - val_loss: 0.0822 - val_r_square: 0.9249\n",
            "Epoch 854/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0510 - r_square: 0.9153\n",
            "Epoch 854: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0506 - r_square: 0.9165 - val_loss: 0.0822 - val_r_square: 0.9248\n",
            "Epoch 855/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0504 - r_square: 0.9169\n",
            "Epoch 855: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0506 - r_square: 0.9161 - val_loss: 0.0821 - val_r_square: 0.9251\n",
            "Epoch 856/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0511 - r_square: 0.9150\n",
            "Epoch 856: val_loss did not improve from 0.08204\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0504 - r_square: 0.9163 - val_loss: 0.0823 - val_r_square: 0.9248\n",
            "Epoch 857/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0514 - r_square: 0.9087\n",
            "Epoch 857: val_loss improved from 0.08204 to 0.08190, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0506 - r_square: 0.9163 - val_loss: 0.0819 - val_r_square: 0.9250\n",
            "Epoch 858/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0508 - r_square: 0.9195\n",
            "Epoch 858: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0506 - r_square: 0.9166 - val_loss: 0.0822 - val_r_square: 0.9249\n",
            "Epoch 859/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0505 - r_square: 0.9162\n",
            "Epoch 859: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0505 - r_square: 0.9162 - val_loss: 0.0822 - val_r_square: 0.9250\n",
            "Epoch 860/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0503 - r_square: 0.9171\n",
            "Epoch 860: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0506 - r_square: 0.9161 - val_loss: 0.0822 - val_r_square: 0.9249\n",
            "Epoch 861/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0516 - r_square: 0.9112\n",
            "Epoch 861: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0505 - r_square: 0.9166 - val_loss: 0.0821 - val_r_square: 0.9250\n",
            "Epoch 862/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0509 - r_square: 0.9113\n",
            "Epoch 862: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0504 - r_square: 0.9165 - val_loss: 0.0824 - val_r_square: 0.9246\n",
            "Epoch 863/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0508 - r_square: 0.9183\n",
            "Epoch 863: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0505 - r_square: 0.9161 - val_loss: 0.0821 - val_r_square: 0.9249\n",
            "Epoch 864/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0506 - r_square: 0.9159\n",
            "Epoch 864: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0504 - r_square: 0.9165 - val_loss: 0.0822 - val_r_square: 0.9249\n",
            "Epoch 865/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0503 - r_square: 0.9211\n",
            "Epoch 865: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0505 - r_square: 0.9167 - val_loss: 0.0819 - val_r_square: 0.9250\n",
            "Epoch 866/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0499 - r_square: 0.9131\n",
            "Epoch 866: val_loss improved from 0.08190 to 0.08190, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0503 - r_square: 0.9166 - val_loss: 0.0819 - val_r_square: 0.9249\n",
            "Epoch 867/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0500 - r_square: 0.9120\n",
            "Epoch 867: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0505 - r_square: 0.9168 - val_loss: 0.0820 - val_r_square: 0.9248\n",
            "Epoch 868/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0504 - r_square: 0.9167\n",
            "Epoch 868: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0504 - r_square: 0.9167 - val_loss: 0.0822 - val_r_square: 0.9247\n",
            "Epoch 869/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0500 - r_square: 0.9156\n",
            "Epoch 869: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0503 - r_square: 0.9166 - val_loss: 0.0826 - val_r_square: 0.9242\n",
            "Epoch 870/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0497 - r_square: 0.9158\n",
            "Epoch 870: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0502 - r_square: 0.9169 - val_loss: 0.0819 - val_r_square: 0.9248\n",
            "Epoch 871/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0500 - r_square: 0.9179\n",
            "Epoch 871: val_loss did not improve from 0.08190\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0503 - r_square: 0.9171 - val_loss: 0.0828 - val_r_square: 0.9241\n",
            "Epoch 872/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0506 - r_square: 0.9169\n",
            "Epoch 872: val_loss improved from 0.08190 to 0.08186, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0503 - r_square: 0.9170 - val_loss: 0.0819 - val_r_square: 0.9248\n",
            "Epoch 873/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0490 - r_square: 0.9155\n",
            "Epoch 873: val_loss did not improve from 0.08186\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0503 - r_square: 0.9170 - val_loss: 0.0821 - val_r_square: 0.9247\n",
            "Epoch 874/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0496 - r_square: 0.9148\n",
            "Epoch 874: val_loss did not improve from 0.08186\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0501 - r_square: 0.9172 - val_loss: 0.0820 - val_r_square: 0.9248\n",
            "Epoch 875/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0504 - r_square: 0.9163\n",
            "Epoch 875: val_loss did not improve from 0.08186\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0502 - r_square: 0.9168 - val_loss: 0.0820 - val_r_square: 0.9246\n",
            "Epoch 876/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0497 - r_square: 0.9136\n",
            "Epoch 876: val_loss did not improve from 0.08186\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0502 - r_square: 0.9171 - val_loss: 0.0820 - val_r_square: 0.9246\n",
            "Epoch 877/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0506 - r_square: 0.9224\n",
            "Epoch 877: val_loss did not improve from 0.08186\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0501 - r_square: 0.9169 - val_loss: 0.0826 - val_r_square: 0.9244\n",
            "Epoch 878/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0501 - r_square: 0.9162\n",
            "Epoch 878: val_loss did not improve from 0.08186\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0502 - r_square: 0.9170 - val_loss: 0.0819 - val_r_square: 0.9247\n",
            "Epoch 879/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0499 - r_square: 0.9214\n",
            "Epoch 879: val_loss improved from 0.08186 to 0.08177, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0502 - r_square: 0.9175 - val_loss: 0.0818 - val_r_square: 0.9246\n",
            "Epoch 880/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0503 - r_square: 0.9158\n",
            "Epoch 880: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0500 - r_square: 0.9173 - val_loss: 0.0819 - val_r_square: 0.9246\n",
            "Epoch 881/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0506 - r_square: 0.9218\n",
            "Epoch 881: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0502 - r_square: 0.9174 - val_loss: 0.0819 - val_r_square: 0.9247\n",
            "Epoch 882/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0509 - r_square: 0.9175\n",
            "Epoch 882: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0502 - r_square: 0.9173 - val_loss: 0.0819 - val_r_square: 0.9246\n",
            "Epoch 883/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0493 - r_square: 0.9203\n",
            "Epoch 883: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0501 - r_square: 0.9172 - val_loss: 0.0818 - val_r_square: 0.9248\n",
            "Epoch 884/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0499 - r_square: 0.9250\n",
            "Epoch 884: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0500 - r_square: 0.9176 - val_loss: 0.0823 - val_r_square: 0.9244\n",
            "Epoch 885/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0491 - r_square: 0.9168\n",
            "Epoch 885: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0501 - r_square: 0.9181 - val_loss: 0.0819 - val_r_square: 0.9245\n",
            "Epoch 886/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0495 - r_square: 0.9228\n",
            "Epoch 886: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0500 - r_square: 0.9172 - val_loss: 0.0818 - val_r_square: 0.9247\n",
            "Epoch 887/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0507 - r_square: 0.9185\n",
            "Epoch 887: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0498 - r_square: 0.9174 - val_loss: 0.0822 - val_r_square: 0.9243\n",
            "Epoch 888/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0516 - r_square: 0.9153\n",
            "Epoch 888: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0499 - r_square: 0.9177 - val_loss: 0.0820 - val_r_square: 0.9245\n",
            "Epoch 889/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0504 - r_square: 0.9181\n",
            "Epoch 889: val_loss did not improve from 0.08177\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0500 - r_square: 0.9177 - val_loss: 0.0818 - val_r_square: 0.9246\n",
            "Epoch 890/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0500 - r_square: 0.9155\n",
            "Epoch 890: val_loss improved from 0.08177 to 0.08176, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0499 - r_square: 0.9177 - val_loss: 0.0818 - val_r_square: 0.9246\n",
            "Epoch 891/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0499 - r_square: 0.9184\n",
            "Epoch 891: val_loss did not improve from 0.08176\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0502 - r_square: 0.9178 - val_loss: 0.0822 - val_r_square: 0.9245\n",
            "Epoch 892/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0498 - r_square: 0.9165\n",
            "Epoch 892: val_loss did not improve from 0.08176\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0500 - r_square: 0.9183 - val_loss: 0.0823 - val_r_square: 0.9242\n",
            "Epoch 893/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0506 - r_square: 0.9113\n",
            "Epoch 893: val_loss did not improve from 0.08176\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0501 - r_square: 0.9177 - val_loss: 0.0818 - val_r_square: 0.9246\n",
            "Epoch 894/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0496 - r_square: 0.9141\n",
            "Epoch 894: val_loss did not improve from 0.08176\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0499 - r_square: 0.9180 - val_loss: 0.0819 - val_r_square: 0.9245\n",
            "Epoch 895/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0500 - r_square: 0.9159\n",
            "Epoch 895: val_loss improved from 0.08176 to 0.08175, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0499 - r_square: 0.9183 - val_loss: 0.0818 - val_r_square: 0.9246\n",
            "Epoch 896/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0500 - r_square: 0.9229\n",
            "Epoch 896: val_loss improved from 0.08175 to 0.08148, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0498 - r_square: 0.9181 - val_loss: 0.0815 - val_r_square: 0.9245\n",
            "Epoch 897/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0496 - r_square: 0.9172\n",
            "Epoch 897: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0500 - r_square: 0.9180 - val_loss: 0.0819 - val_r_square: 0.9245\n",
            "Epoch 898/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0493 - r_square: 0.9182\n",
            "Epoch 898: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0499 - r_square: 0.9182 - val_loss: 0.0818 - val_r_square: 0.9245\n",
            "Epoch 899/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0503 - r_square: 0.9125\n",
            "Epoch 899: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0498 - r_square: 0.9181 - val_loss: 0.0817 - val_r_square: 0.9246\n",
            "Epoch 900/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0494 - r_square: 0.9155\n",
            "Epoch 900: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0500 - r_square: 0.9179 - val_loss: 0.0817 - val_r_square: 0.9245\n",
            "Epoch 901/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0493 - r_square: 0.9132\n",
            "Epoch 901: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0498 - r_square: 0.9180 - val_loss: 0.0817 - val_r_square: 0.9244\n",
            "Epoch 902/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0494 - r_square: 0.9233\n",
            "Epoch 902: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0498 - r_square: 0.9183 - val_loss: 0.0816 - val_r_square: 0.9246\n",
            "Epoch 903/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0499 - r_square: 0.9220\n",
            "Epoch 903: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0497 - r_square: 0.9182 - val_loss: 0.0816 - val_r_square: 0.9245\n",
            "Epoch 904/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0500 - r_square: 0.9174\n",
            "Epoch 904: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0498 - r_square: 0.9183 - val_loss: 0.0818 - val_r_square: 0.9246\n",
            "Epoch 905/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0490 - r_square: 0.9246\n",
            "Epoch 905: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0496 - r_square: 0.9183 - val_loss: 0.0820 - val_r_square: 0.9243\n",
            "Epoch 906/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0497 - r_square: 0.9183\n",
            "Epoch 906: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0497 - r_square: 0.9183 - val_loss: 0.0817 - val_r_square: 0.9244\n",
            "Epoch 907/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0491 - r_square: 0.9188\n",
            "Epoch 907: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0498 - r_square: 0.9184 - val_loss: 0.0816 - val_r_square: 0.9244\n",
            "Epoch 908/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0498 - r_square: 0.9187\n",
            "Epoch 908: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0497 - r_square: 0.9184 - val_loss: 0.0819 - val_r_square: 0.9242\n",
            "Epoch 909/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0489 - r_square: 0.9193\n",
            "Epoch 909: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0498 - r_square: 0.9186 - val_loss: 0.0819 - val_r_square: 0.9243\n",
            "Epoch 910/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0498 - r_square: 0.9192\n",
            "Epoch 910: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0497 - r_square: 0.9187 - val_loss: 0.0816 - val_r_square: 0.9244\n",
            "Epoch 911/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0494 - r_square: 0.9136\n",
            "Epoch 911: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0496 - r_square: 0.9184 - val_loss: 0.0818 - val_r_square: 0.9243\n",
            "Epoch 912/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0513 - r_square: 0.9211\n",
            "Epoch 912: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0498 - r_square: 0.9188 - val_loss: 0.0819 - val_r_square: 0.9243\n",
            "Epoch 913/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0496 - r_square: 0.9154\n",
            "Epoch 913: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0496 - r_square: 0.9187 - val_loss: 0.0820 - val_r_square: 0.9243\n",
            "Epoch 914/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0500 - r_square: 0.9116\n",
            "Epoch 914: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0496 - r_square: 0.9186 - val_loss: 0.0816 - val_r_square: 0.9244\n",
            "Epoch 915/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0492 - r_square: 0.9204\n",
            "Epoch 915: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0495 - r_square: 0.9184 - val_loss: 0.0816 - val_r_square: 0.9244\n",
            "Epoch 916/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0484 - r_square: 0.9157\n",
            "Epoch 916: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0496 - r_square: 0.9187 - val_loss: 0.0819 - val_r_square: 0.9245\n",
            "Epoch 917/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0495 - r_square: 0.9189\n",
            "Epoch 917: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0495 - r_square: 0.9189 - val_loss: 0.0816 - val_r_square: 0.9243\n",
            "Epoch 918/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0505 - r_square: 0.9183\n",
            "Epoch 918: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0495 - r_square: 0.9185 - val_loss: 0.0817 - val_r_square: 0.9242\n",
            "Epoch 919/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0490 - r_square: 0.9133\n",
            "Epoch 919: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0496 - r_square: 0.9185 - val_loss: 0.0816 - val_r_square: 0.9242\n",
            "Epoch 920/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0499 - r_square: 0.9225\n",
            "Epoch 920: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0495 - r_square: 0.9187 - val_loss: 0.0817 - val_r_square: 0.9243\n",
            "Epoch 921/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0501 - r_square: 0.9134\n",
            "Epoch 921: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0496 - r_square: 0.9189 - val_loss: 0.0817 - val_r_square: 0.9243\n",
            "Epoch 922/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0481 - r_square: 0.9193\n",
            "Epoch 922: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0495 - r_square: 0.9192 - val_loss: 0.0816 - val_r_square: 0.9242\n",
            "Epoch 923/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0479 - r_square: 0.9238\n",
            "Epoch 923: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0495 - r_square: 0.9186 - val_loss: 0.0816 - val_r_square: 0.9243\n",
            "Epoch 924/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0493 - r_square: 0.9189\n",
            "Epoch 924: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0494 - r_square: 0.9190 - val_loss: 0.0816 - val_r_square: 0.9242\n",
            "Epoch 925/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0491 - r_square: 0.9199\n",
            "Epoch 925: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0494 - r_square: 0.9188 - val_loss: 0.0815 - val_r_square: 0.9244\n",
            "Epoch 926/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0498 - r_square: 0.9175\n",
            "Epoch 926: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0495 - r_square: 0.9192 - val_loss: 0.0828 - val_r_square: 0.9238\n",
            "Epoch 927/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0485 - r_square: 0.9212\n",
            "Epoch 927: val_loss did not improve from 0.08148\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0498 - r_square: 0.9192 - val_loss: 0.0817 - val_r_square: 0.9242\n",
            "Epoch 928/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0494 - r_square: 0.9180\n",
            "Epoch 928: val_loss improved from 0.08148 to 0.08139, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0494 - r_square: 0.9192 - val_loss: 0.0814 - val_r_square: 0.9242\n",
            "Epoch 929/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0497 - r_square: 0.9124\n",
            "Epoch 929: val_loss did not improve from 0.08139\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0493 - r_square: 0.9192 - val_loss: 0.0814 - val_r_square: 0.9242\n",
            "Epoch 930/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0487 - r_square: 0.9162\n",
            "Epoch 930: val_loss did not improve from 0.08139\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0493 - r_square: 0.9191 - val_loss: 0.0819 - val_r_square: 0.9241\n",
            "Epoch 931/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0494 - r_square: 0.9193\n",
            "Epoch 931: val_loss did not improve from 0.08139\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0494 - r_square: 0.9193 - val_loss: 0.0820 - val_r_square: 0.9236\n",
            "Epoch 932/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0499 - r_square: 0.9181\n",
            "Epoch 932: val_loss improved from 0.08139 to 0.08136, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0495 - r_square: 0.9196 - val_loss: 0.0814 - val_r_square: 0.9241\n",
            "Epoch 933/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0494 - r_square: 0.9193\n",
            "Epoch 933: val_loss did not improve from 0.08136\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0494 - r_square: 0.9193 - val_loss: 0.0818 - val_r_square: 0.9240\n",
            "Epoch 934/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0495 - r_square: 0.9195\n",
            "Epoch 934: val_loss improved from 0.08136 to 0.08128, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0493 - r_square: 0.9196 - val_loss: 0.0813 - val_r_square: 0.9242\n",
            "Epoch 935/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0487 - r_square: 0.9201\n",
            "Epoch 935: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0492 - r_square: 0.9195 - val_loss: 0.0815 - val_r_square: 0.9242\n",
            "Epoch 936/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0499 - r_square: 0.9176\n",
            "Epoch 936: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0492 - r_square: 0.9194 - val_loss: 0.0815 - val_r_square: 0.9239\n",
            "Epoch 937/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0490 - r_square: 0.9194\n",
            "Epoch 937: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0492 - r_square: 0.9192 - val_loss: 0.0817 - val_r_square: 0.9239\n",
            "Epoch 938/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0493 - r_square: 0.9189\n",
            "Epoch 938: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0493 - r_square: 0.9196 - val_loss: 0.0818 - val_r_square: 0.9240\n",
            "Epoch 939/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0489 - r_square: 0.9178\n",
            "Epoch 939: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0493 - r_square: 0.9193 - val_loss: 0.0818 - val_r_square: 0.9239\n",
            "Epoch 940/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0501 - r_square: 0.9236\n",
            "Epoch 940: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0492 - r_square: 0.9191 - val_loss: 0.0817 - val_r_square: 0.9241\n",
            "Epoch 941/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0486 - r_square: 0.9196\n",
            "Epoch 941: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0493 - r_square: 0.9195 - val_loss: 0.0814 - val_r_square: 0.9239\n",
            "Epoch 942/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0495 - r_square: 0.9108\n",
            "Epoch 942: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0493 - r_square: 0.9197 - val_loss: 0.0815 - val_r_square: 0.9240\n",
            "Epoch 943/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0494 - r_square: 0.9191\n",
            "Epoch 943: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0495 - r_square: 0.9192 - val_loss: 0.0814 - val_r_square: 0.9241\n",
            "Epoch 944/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0493 - r_square: 0.9197\n",
            "Epoch 944: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0492 - r_square: 0.9196 - val_loss: 0.0820 - val_r_square: 0.9235\n",
            "Epoch 945/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0494 - r_square: 0.9206\n",
            "Epoch 945: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0494 - r_square: 0.9206 - val_loss: 0.0817 - val_r_square: 0.9240\n",
            "Epoch 946/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0507 - r_square: 0.9196\n",
            "Epoch 946: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0492 - r_square: 0.9198 - val_loss: 0.0814 - val_r_square: 0.9240\n",
            "Epoch 947/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0502 - r_square: 0.9132\n",
            "Epoch 947: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0491 - r_square: 0.9198 - val_loss: 0.0815 - val_r_square: 0.9239\n",
            "Epoch 948/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0491 - r_square: 0.9197\n",
            "Epoch 948: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0491 - r_square: 0.9197 - val_loss: 0.0825 - val_r_square: 0.9236\n",
            "Epoch 949/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0488 - r_square: 0.9209\n",
            "Epoch 949: val_loss did not improve from 0.08128\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0493 - r_square: 0.9207 - val_loss: 0.0814 - val_r_square: 0.9241\n",
            "Epoch 950/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0496 - r_square: 0.9237\n",
            "Epoch 950: val_loss improved from 0.08128 to 0.08120, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0490 - r_square: 0.9200 - val_loss: 0.0812 - val_r_square: 0.9241\n",
            "Epoch 951/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0497 - r_square: 0.9160\n",
            "Epoch 951: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0491 - r_square: 0.9202 - val_loss: 0.0814 - val_r_square: 0.9239\n",
            "Epoch 952/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0479 - r_square: 0.9197\n",
            "Epoch 952: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0490 - r_square: 0.9193 - val_loss: 0.0816 - val_r_square: 0.9240\n",
            "Epoch 953/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0476 - r_square: 0.9212\n",
            "Epoch 953: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0490 - r_square: 0.9199 - val_loss: 0.0813 - val_r_square: 0.9237\n",
            "Epoch 954/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0478 - r_square: 0.9212\n",
            "Epoch 954: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0490 - r_square: 0.9200 - val_loss: 0.0815 - val_r_square: 0.9238\n",
            "Epoch 955/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0508 - r_square: 0.9182\n",
            "Epoch 955: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0490 - r_square: 0.9201 - val_loss: 0.0814 - val_r_square: 0.9240\n",
            "Epoch 956/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0499 - r_square: 0.9207\n",
            "Epoch 956: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0490 - r_square: 0.9200 - val_loss: 0.0814 - val_r_square: 0.9237\n",
            "Epoch 957/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0490 - r_square: 0.9202\n",
            "Epoch 957: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0490 - r_square: 0.9202 - val_loss: 0.0816 - val_r_square: 0.9236\n",
            "Epoch 958/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0484 - r_square: 0.9230\n",
            "Epoch 958: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0488 - r_square: 0.9203 - val_loss: 0.0812 - val_r_square: 0.9238\n",
            "Epoch 959/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0486 - r_square: 0.9222\n",
            "Epoch 959: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0490 - r_square: 0.9205 - val_loss: 0.0815 - val_r_square: 0.9236\n",
            "Epoch 960/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0488 - r_square: 0.9202\n",
            "Epoch 960: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0488 - r_square: 0.9202 - val_loss: 0.0819 - val_r_square: 0.9236\n",
            "Epoch 961/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0490 - r_square: 0.9201\n",
            "Epoch 961: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0490 - r_square: 0.9201 - val_loss: 0.0813 - val_r_square: 0.9239\n",
            "Epoch 962/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0490 - r_square: 0.9201\n",
            "Epoch 962: val_loss did not improve from 0.08120\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0489 - r_square: 0.9202 - val_loss: 0.0813 - val_r_square: 0.9236\n",
            "Epoch 963/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0492 - r_square: 0.9201\n",
            "Epoch 963: val_loss improved from 0.08120 to 0.08115, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0489 - r_square: 0.9207 - val_loss: 0.0812 - val_r_square: 0.9238\n",
            "Epoch 964/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0479 - r_square: 0.9195\n",
            "Epoch 964: val_loss did not improve from 0.08115\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0494 - r_square: 0.9201 - val_loss: 0.0827 - val_r_square: 0.9233\n",
            "Epoch 965/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0499 - r_square: 0.9139\n",
            "Epoch 965: val_loss did not improve from 0.08115\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0491 - r_square: 0.9200 - val_loss: 0.0812 - val_r_square: 0.9238\n",
            "Epoch 966/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0473 - r_square: 0.9282\n",
            "Epoch 966: val_loss did not improve from 0.08115\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0489 - r_square: 0.9201 - val_loss: 0.0815 - val_r_square: 0.9235\n",
            "Epoch 967/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0489 - r_square: 0.9242\n",
            "Epoch 967: val_loss did not improve from 0.08115\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0489 - r_square: 0.9202 - val_loss: 0.0815 - val_r_square: 0.9237\n",
            "Epoch 968/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0489 - r_square: 0.9198\n",
            "Epoch 968: val_loss did not improve from 0.08115\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0488 - r_square: 0.9201 - val_loss: 0.0817 - val_r_square: 0.9234\n",
            "Epoch 969/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0491 - r_square: 0.9207\n",
            "Epoch 969: val_loss improved from 0.08115 to 0.08103, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0491 - r_square: 0.9202 - val_loss: 0.0810 - val_r_square: 0.9237\n",
            "Epoch 970/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0485 - r_square: 0.9290\n",
            "Epoch 970: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0489 - r_square: 0.9204 - val_loss: 0.0816 - val_r_square: 0.9235\n",
            "Epoch 971/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0484 - r_square: 0.9270\n",
            "Epoch 971: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0489 - r_square: 0.9204 - val_loss: 0.0811 - val_r_square: 0.9239\n",
            "Epoch 972/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0499 - r_square: 0.9147\n",
            "Epoch 972: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0488 - r_square: 0.9206 - val_loss: 0.0815 - val_r_square: 0.9237\n",
            "Epoch 973/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0482 - r_square: 0.9189\n",
            "Epoch 973: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0488 - r_square: 0.9207 - val_loss: 0.0819 - val_r_square: 0.9230\n",
            "Epoch 974/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0488 - r_square: 0.9282\n",
            "Epoch 974: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0488 - r_square: 0.9206 - val_loss: 0.0818 - val_r_square: 0.9234\n",
            "Epoch 975/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0489 - r_square: 0.9200\n",
            "Epoch 975: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0489 - r_square: 0.9202 - val_loss: 0.0811 - val_r_square: 0.9237\n",
            "Epoch 976/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0476 - r_square: 0.9222\n",
            "Epoch 976: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0488 - r_square: 0.9207 - val_loss: 0.0812 - val_r_square: 0.9235\n",
            "Epoch 977/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0494 - r_square: 0.9215\n",
            "Epoch 977: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0487 - r_square: 0.9206 - val_loss: 0.0811 - val_r_square: 0.9237\n",
            "Epoch 978/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0487 - r_square: 0.9188\n",
            "Epoch 978: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0488 - r_square: 0.9205 - val_loss: 0.0821 - val_r_square: 0.9232\n",
            "Epoch 979/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0483 - r_square: 0.9192\n",
            "Epoch 979: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0488 - r_square: 0.9208 - val_loss: 0.0814 - val_r_square: 0.9237\n",
            "Epoch 980/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0486 - r_square: 0.9273\n",
            "Epoch 980: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0487 - r_square: 0.9207 - val_loss: 0.0811 - val_r_square: 0.9237\n",
            "Epoch 981/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0488 - r_square: 0.9256\n",
            "Epoch 981: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0487 - r_square: 0.9206 - val_loss: 0.0814 - val_r_square: 0.9234\n",
            "Epoch 982/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0494 - r_square: 0.9171\n",
            "Epoch 982: val_loss did not improve from 0.08103\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0486 - r_square: 0.9212 - val_loss: 0.0820 - val_r_square: 0.9233\n",
            "Epoch 983/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0502 - r_square: 0.9238\n",
            "Epoch 983: val_loss improved from 0.08103 to 0.08098, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0487 - r_square: 0.9206 - val_loss: 0.0810 - val_r_square: 0.9236\n",
            "Epoch 984/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0490 - r_square: 0.9175\n",
            "Epoch 984: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0486 - r_square: 0.9209 - val_loss: 0.0815 - val_r_square: 0.9236\n",
            "Epoch 985/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0487 - r_square: 0.9216\n",
            "Epoch 985: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0485 - r_square: 0.9210 - val_loss: 0.0811 - val_r_square: 0.9235\n",
            "Epoch 986/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0496 - r_square: 0.9151\n",
            "Epoch 986: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0486 - r_square: 0.9209 - val_loss: 0.0814 - val_r_square: 0.9235\n",
            "Epoch 987/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0487 - r_square: 0.9201\n",
            "Epoch 987: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0486 - r_square: 0.9212 - val_loss: 0.0812 - val_r_square: 0.9235\n",
            "Epoch 988/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0486 - r_square: 0.9208\n",
            "Epoch 988: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0486 - r_square: 0.9208 - val_loss: 0.0814 - val_r_square: 0.9235\n",
            "Epoch 989/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0488 - r_square: 0.9246\n",
            "Epoch 989: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0488 - r_square: 0.9210 - val_loss: 0.0812 - val_r_square: 0.9235\n",
            "Epoch 990/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0502 - r_square: 0.9225\n",
            "Epoch 990: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0488 - r_square: 0.9216 - val_loss: 0.0816 - val_r_square: 0.9232\n",
            "Epoch 991/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0485 - r_square: 0.9131\n",
            "Epoch 991: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0484 - r_square: 0.9212 - val_loss: 0.0812 - val_r_square: 0.9233\n",
            "Epoch 992/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0474 - r_square: 0.9188\n",
            "Epoch 992: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0484 - r_square: 0.9211 - val_loss: 0.0812 - val_r_square: 0.9233\n",
            "Epoch 993/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0487 - r_square: 0.9191\n",
            "Epoch 993: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0485 - r_square: 0.9213 - val_loss: 0.0812 - val_r_square: 0.9235\n",
            "Epoch 994/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0473 - r_square: 0.9150\n",
            "Epoch 994: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0485 - r_square: 0.9215 - val_loss: 0.0815 - val_r_square: 0.9234\n",
            "Epoch 995/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0491 - r_square: 0.9193\n",
            "Epoch 995: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0485 - r_square: 0.9214 - val_loss: 0.0810 - val_r_square: 0.9235\n",
            "Epoch 996/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0495 - r_square: 0.9256\n",
            "Epoch 996: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0485 - r_square: 0.9216 - val_loss: 0.0810 - val_r_square: 0.9235\n",
            "Epoch 997/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0493 - r_square: 0.9244\n",
            "Epoch 997: val_loss did not improve from 0.08098\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0484 - r_square: 0.9214 - val_loss: 0.0810 - val_r_square: 0.9235\n",
            "Epoch 998/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0483 - r_square: 0.9191\n",
            "Epoch 998: val_loss improved from 0.08098 to 0.08089, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0484 - r_square: 0.9213 - val_loss: 0.0809 - val_r_square: 0.9236\n",
            "Epoch 999/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0482 - r_square: 0.9200\n",
            "Epoch 999: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0484 - r_square: 0.9210 - val_loss: 0.0818 - val_r_square: 0.9229\n",
            "Epoch 1000/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0501 - r_square: 0.9189\n",
            "Epoch 1000: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0484 - r_square: 0.9215 - val_loss: 0.0810 - val_r_square: 0.9235\n",
            "Epoch 1001/1500\n",
            "49/67 [====================>.........] - ETA: 0s - loss: 0.0465 - r_square: 0.9175\n",
            "Epoch 1001: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0483 - r_square: 0.9212 - val_loss: 0.0811 - val_r_square: 0.9235\n",
            "Epoch 1002/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0483 - r_square: 0.9176\n",
            "Epoch 1002: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0484 - r_square: 0.9212 - val_loss: 0.0811 - val_r_square: 0.9235\n",
            "Epoch 1003/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0491 - r_square: 0.9182\n",
            "Epoch 1003: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0483 - r_square: 0.9215 - val_loss: 0.0812 - val_r_square: 0.9235\n",
            "Epoch 1004/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0490 - r_square: 0.9269\n",
            "Epoch 1004: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0484 - r_square: 0.9213 - val_loss: 0.0813 - val_r_square: 0.9234\n",
            "Epoch 1005/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0483 - r_square: 0.9268\n",
            "Epoch 1005: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0483 - r_square: 0.9215 - val_loss: 0.0814 - val_r_square: 0.9233\n",
            "Epoch 1006/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0474 - r_square: 0.9190\n",
            "Epoch 1006: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0484 - r_square: 0.9213 - val_loss: 0.0813 - val_r_square: 0.9232\n",
            "Epoch 1007/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0483 - r_square: 0.9211\n",
            "Epoch 1007: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0483 - r_square: 0.9211 - val_loss: 0.0809 - val_r_square: 0.9233\n",
            "Epoch 1008/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0480 - r_square: 0.9199\n",
            "Epoch 1008: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0482 - r_square: 0.9220 - val_loss: 0.0822 - val_r_square: 0.9230\n",
            "Epoch 1009/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0470 - r_square: 0.9248\n",
            "Epoch 1009: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0484 - r_square: 0.9213 - val_loss: 0.0811 - val_r_square: 0.9233\n",
            "Epoch 1010/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0483 - r_square: 0.9252\n",
            "Epoch 1010: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0483 - r_square: 0.9217 - val_loss: 0.0812 - val_r_square: 0.9233\n",
            "Epoch 1011/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0481 - r_square: 0.9223\n",
            "Epoch 1011: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0482 - r_square: 0.9216 - val_loss: 0.0810 - val_r_square: 0.9233\n",
            "Epoch 1012/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0483 - r_square: 0.9203\n",
            "Epoch 1012: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0482 - r_square: 0.9216 - val_loss: 0.0810 - val_r_square: 0.9232\n",
            "Epoch 1013/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0502 - r_square: 0.9153\n",
            "Epoch 1013: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0482 - r_square: 0.9221 - val_loss: 0.0809 - val_r_square: 0.9233\n",
            "Epoch 1014/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0493 - r_square: 0.9211\n",
            "Epoch 1014: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0483 - r_square: 0.9218 - val_loss: 0.0810 - val_r_square: 0.9232\n",
            "Epoch 1015/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0478 - r_square: 0.9215\n",
            "Epoch 1015: val_loss did not improve from 0.08089\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0482 - r_square: 0.9218 - val_loss: 0.0810 - val_r_square: 0.9232\n",
            "Epoch 1016/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0466 - r_square: 0.9221\n",
            "Epoch 1016: val_loss improved from 0.08089 to 0.08068, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0481 - r_square: 0.9221 - val_loss: 0.0807 - val_r_square: 0.9234\n",
            "Epoch 1017/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0485 - r_square: 0.9206\n",
            "Epoch 1017: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 1s 16ms/step - loss: 0.0483 - r_square: 0.9216 - val_loss: 0.0810 - val_r_square: 0.9233\n",
            "Epoch 1018/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0476 - r_square: 0.9234\n",
            "Epoch 1018: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0482 - r_square: 0.9220 - val_loss: 0.0810 - val_r_square: 0.9231\n",
            "Epoch 1019/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0483 - r_square: 0.9218\n",
            "Epoch 1019: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0483 - r_square: 0.9218 - val_loss: 0.0812 - val_r_square: 0.9233\n",
            "Epoch 1020/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0488 - r_square: 0.9155\n",
            "Epoch 1020: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0483 - r_square: 0.9214 - val_loss: 0.0809 - val_r_square: 0.9232\n",
            "Epoch 1021/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0474 - r_square: 0.9228\n",
            "Epoch 1021: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0481 - r_square: 0.9220 - val_loss: 0.0821 - val_r_square: 0.9227\n",
            "Epoch 1022/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0481 - r_square: 0.9225\n",
            "Epoch 1022: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0482 - r_square: 0.9220 - val_loss: 0.0816 - val_r_square: 0.9229\n",
            "Epoch 1023/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0485 - r_square: 0.9237\n",
            "Epoch 1023: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0482 - r_square: 0.9219 - val_loss: 0.0809 - val_r_square: 0.9233\n",
            "Epoch 1024/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0480 - r_square: 0.9279\n",
            "Epoch 1024: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0479 - r_square: 0.9218 - val_loss: 0.0812 - val_r_square: 0.9233\n",
            "Epoch 1025/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0472 - r_square: 0.9229\n",
            "Epoch 1025: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0481 - r_square: 0.9220 - val_loss: 0.0809 - val_r_square: 0.9231\n",
            "Epoch 1026/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0487 - r_square: 0.9224\n",
            "Epoch 1026: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0481 - r_square: 0.9219 - val_loss: 0.0812 - val_r_square: 0.9232\n",
            "Epoch 1027/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0490 - r_square: 0.9205\n",
            "Epoch 1027: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0482 - r_square: 0.9217 - val_loss: 0.0808 - val_r_square: 0.9233\n",
            "Epoch 1028/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0479 - r_square: 0.9210\n",
            "Epoch 1028: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0479 - r_square: 0.9219 - val_loss: 0.0810 - val_r_square: 0.9232\n",
            "Epoch 1029/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0476 - r_square: 0.9257\n",
            "Epoch 1029: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0481 - r_square: 0.9221 - val_loss: 0.0810 - val_r_square: 0.9232\n",
            "Epoch 1030/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0474 - r_square: 0.9222\n",
            "Epoch 1030: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0479 - r_square: 0.9222 - val_loss: 0.0809 - val_r_square: 0.9231\n",
            "Epoch 1031/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0492 - r_square: 0.9227\n",
            "Epoch 1031: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0479 - r_square: 0.9220 - val_loss: 0.0823 - val_r_square: 0.9221\n",
            "Epoch 1032/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0481 - r_square: 0.9258\n",
            "Epoch 1032: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0479 - r_square: 0.9222 - val_loss: 0.0809 - val_r_square: 0.9233\n",
            "Epoch 1033/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0472 - r_square: 0.9217\n",
            "Epoch 1033: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0480 - r_square: 0.9224 - val_loss: 0.0811 - val_r_square: 0.9231\n",
            "Epoch 1034/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0482 - r_square: 0.9213\n",
            "Epoch 1034: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0480 - r_square: 0.9222 - val_loss: 0.0811 - val_r_square: 0.9231\n",
            "Epoch 1035/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0476 - r_square: 0.9187\n",
            "Epoch 1035: val_loss did not improve from 0.08068\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0479 - r_square: 0.9222 - val_loss: 0.0807 - val_r_square: 0.9232\n",
            "Epoch 1036/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0479 - r_square: 0.9224\n",
            "Epoch 1036: val_loss improved from 0.08068 to 0.08059, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0479 - r_square: 0.9224 - val_loss: 0.0806 - val_r_square: 0.9232\n",
            "Epoch 1037/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0480 - r_square: 0.9217\n",
            "Epoch 1037: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0478 - r_square: 0.9225 - val_loss: 0.0810 - val_r_square: 0.9231\n",
            "Epoch 1038/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0481 - r_square: 0.9244\n",
            "Epoch 1038: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0479 - r_square: 0.9222 - val_loss: 0.0807 - val_r_square: 0.9232\n",
            "Epoch 1039/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0481 - r_square: 0.9237\n",
            "Epoch 1039: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0478 - r_square: 0.9228 - val_loss: 0.0814 - val_r_square: 0.9227\n",
            "Epoch 1040/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0477 - r_square: 0.9209\n",
            "Epoch 1040: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0478 - r_square: 0.9221 - val_loss: 0.0810 - val_r_square: 0.9232\n",
            "Epoch 1041/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0478 - r_square: 0.9226\n",
            "Epoch 1041: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0478 - r_square: 0.9226 - val_loss: 0.0808 - val_r_square: 0.9233\n",
            "Epoch 1042/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0480 - r_square: 0.9215\n",
            "Epoch 1042: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0478 - r_square: 0.9227 - val_loss: 0.0806 - val_r_square: 0.9231\n",
            "Epoch 1043/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0485 - r_square: 0.9237\n",
            "Epoch 1043: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0478 - r_square: 0.9227 - val_loss: 0.0808 - val_r_square: 0.9232\n",
            "Epoch 1044/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0476 - r_square: 0.9247\n",
            "Epoch 1044: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0478 - r_square: 0.9226 - val_loss: 0.0808 - val_r_square: 0.9232\n",
            "Epoch 1045/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0478 - r_square: 0.9223\n",
            "Epoch 1045: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0478 - r_square: 0.9227 - val_loss: 0.0807 - val_r_square: 0.9232\n",
            "Epoch 1046/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0480 - r_square: 0.9190\n",
            "Epoch 1046: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0477 - r_square: 0.9227 - val_loss: 0.0808 - val_r_square: 0.9230\n",
            "Epoch 1047/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0477 - r_square: 0.9220\n",
            "Epoch 1047: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0480 - r_square: 0.9221 - val_loss: 0.0812 - val_r_square: 0.9229\n",
            "Epoch 1048/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0475 - r_square: 0.9245\n",
            "Epoch 1048: val_loss improved from 0.08059 to 0.08059, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0476 - r_square: 0.9228 - val_loss: 0.0806 - val_r_square: 0.9231\n",
            "Epoch 1049/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0463 - r_square: 0.9180\n",
            "Epoch 1049: val_loss did not improve from 0.08059\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0477 - r_square: 0.9229 - val_loss: 0.0808 - val_r_square: 0.9229\n",
            "Epoch 1050/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0480 - r_square: 0.9237\n",
            "Epoch 1050: val_loss improved from 0.08059 to 0.08058, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0478 - r_square: 0.9223 - val_loss: 0.0806 - val_r_square: 0.9231\n",
            "Epoch 1051/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0477 - r_square: 0.9230\n",
            "Epoch 1051: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0477 - r_square: 0.9228 - val_loss: 0.0815 - val_r_square: 0.9227\n",
            "Epoch 1052/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0481 - r_square: 0.9193\n",
            "Epoch 1052: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0476 - r_square: 0.9226 - val_loss: 0.0810 - val_r_square: 0.9231\n",
            "Epoch 1053/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0467 - r_square: 0.9268\n",
            "Epoch 1053: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0477 - r_square: 0.9226 - val_loss: 0.0806 - val_r_square: 0.9231\n",
            "Epoch 1054/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0475 - r_square: 0.9241\n",
            "Epoch 1054: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0477 - r_square: 0.9231 - val_loss: 0.0810 - val_r_square: 0.9232\n",
            "Epoch 1055/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0476 - r_square: 0.9221\n",
            "Epoch 1055: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0475 - r_square: 0.9232 - val_loss: 0.0811 - val_r_square: 0.9229\n",
            "Epoch 1056/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0478 - r_square: 0.9220\n",
            "Epoch 1056: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0476 - r_square: 0.9229 - val_loss: 0.0814 - val_r_square: 0.9230\n",
            "Epoch 1057/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0474 - r_square: 0.9224\n",
            "Epoch 1057: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0475 - r_square: 0.9232 - val_loss: 0.0806 - val_r_square: 0.9230\n",
            "Epoch 1058/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0476 - r_square: 0.9240\n",
            "Epoch 1058: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0475 - r_square: 0.9232 - val_loss: 0.0808 - val_r_square: 0.9232\n",
            "Epoch 1059/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0476 - r_square: 0.9227\n",
            "Epoch 1059: val_loss improved from 0.08058 to 0.08058, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0476 - r_square: 0.9231 - val_loss: 0.0806 - val_r_square: 0.9230\n",
            "Epoch 1060/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0477 - r_square: 0.9278\n",
            "Epoch 1060: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0475 - r_square: 0.9231 - val_loss: 0.0808 - val_r_square: 0.9230\n",
            "Epoch 1061/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0484 - r_square: 0.9222\n",
            "Epoch 1061: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0476 - r_square: 0.9236 - val_loss: 0.0814 - val_r_square: 0.9227\n",
            "Epoch 1062/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0469 - r_square: 0.9252\n",
            "Epoch 1062: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0477 - r_square: 0.9233 - val_loss: 0.0807 - val_r_square: 0.9229\n",
            "Epoch 1063/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0466 - r_square: 0.9253\n",
            "Epoch 1063: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0474 - r_square: 0.9231 - val_loss: 0.0807 - val_r_square: 0.9230\n",
            "Epoch 1064/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0480 - r_square: 0.9313\n",
            "Epoch 1064: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0476 - r_square: 0.9234 - val_loss: 0.0808 - val_r_square: 0.9228\n",
            "Epoch 1065/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0472 - r_square: 0.9268\n",
            "Epoch 1065: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0476 - r_square: 0.9229 - val_loss: 0.0814 - val_r_square: 0.9229\n",
            "Epoch 1066/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0471 - r_square: 0.9231\n",
            "Epoch 1066: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0474 - r_square: 0.9230 - val_loss: 0.0807 - val_r_square: 0.9231\n",
            "Epoch 1067/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0473 - r_square: 0.9281\n",
            "Epoch 1067: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0473 - r_square: 0.9231 - val_loss: 0.0807 - val_r_square: 0.9230\n",
            "Epoch 1068/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0476 - r_square: 0.9233\n",
            "Epoch 1068: val_loss did not improve from 0.08058\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0474 - r_square: 0.9233 - val_loss: 0.0808 - val_r_square: 0.9230\n",
            "Epoch 1069/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0477 - r_square: 0.9258\n",
            "Epoch 1069: val_loss improved from 0.08058 to 0.08049, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0474 - r_square: 0.9230 - val_loss: 0.0805 - val_r_square: 0.9230\n",
            "Epoch 1070/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0467 - r_square: 0.9219\n",
            "Epoch 1070: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0474 - r_square: 0.9231 - val_loss: 0.0812 - val_r_square: 0.9229\n",
            "Epoch 1071/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0471 - r_square: 0.9182\n",
            "Epoch 1071: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0475 - r_square: 0.9232 - val_loss: 0.0807 - val_r_square: 0.9228\n",
            "Epoch 1072/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0472 - r_square: 0.9207\n",
            "Epoch 1072: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0473 - r_square: 0.9232 - val_loss: 0.0807 - val_r_square: 0.9229\n",
            "Epoch 1073/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0470 - r_square: 0.9242\n",
            "Epoch 1073: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0474 - r_square: 0.9237 - val_loss: 0.0813 - val_r_square: 0.9224\n",
            "Epoch 1074/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0482 - r_square: 0.9188\n",
            "Epoch 1074: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0474 - r_square: 0.9235 - val_loss: 0.0810 - val_r_square: 0.9229\n",
            "Epoch 1075/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0476 - r_square: 0.9220\n",
            "Epoch 1075: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0473 - r_square: 0.9235 - val_loss: 0.0808 - val_r_square: 0.9226\n",
            "Epoch 1076/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0473 - r_square: 0.9236\n",
            "Epoch 1076: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0473 - r_square: 0.9236 - val_loss: 0.0808 - val_r_square: 0.9229\n",
            "Epoch 1077/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0477 - r_square: 0.9265\n",
            "Epoch 1077: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0473 - r_square: 0.9238 - val_loss: 0.0807 - val_r_square: 0.9231\n",
            "Epoch 1078/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0470 - r_square: 0.9239\n",
            "Epoch 1078: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0472 - r_square: 0.9232 - val_loss: 0.0807 - val_r_square: 0.9228\n",
            "Epoch 1079/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0459 - r_square: 0.9279\n",
            "Epoch 1079: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0474 - r_square: 0.9235 - val_loss: 0.0805 - val_r_square: 0.9229\n",
            "Epoch 1080/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0476 - r_square: 0.9235\n",
            "Epoch 1080: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0473 - r_square: 0.9236 - val_loss: 0.0806 - val_r_square: 0.9229\n",
            "Epoch 1081/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0474 - r_square: 0.9227\n",
            "Epoch 1081: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0475 - r_square: 0.9229 - val_loss: 0.0808 - val_r_square: 0.9228\n",
            "Epoch 1082/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0472 - r_square: 0.9267\n",
            "Epoch 1082: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0471 - r_square: 0.9234 - val_loss: 0.0811 - val_r_square: 0.9223\n",
            "Epoch 1083/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0479 - r_square: 0.9215\n",
            "Epoch 1083: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0473 - r_square: 0.9238 - val_loss: 0.0806 - val_r_square: 0.9228\n",
            "Epoch 1084/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0479 - r_square: 0.9223\n",
            "Epoch 1084: val_loss did not improve from 0.08049\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0471 - r_square: 0.9232 - val_loss: 0.0816 - val_r_square: 0.9225\n",
            "Epoch 1085/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0469 - r_square: 0.9189\n",
            "Epoch 1085: val_loss improved from 0.08049 to 0.08046, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0473 - r_square: 0.9237 - val_loss: 0.0805 - val_r_square: 0.9229\n",
            "Epoch 1086/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0474 - r_square: 0.9248\n",
            "Epoch 1086: val_loss did not improve from 0.08046\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0472 - r_square: 0.9238 - val_loss: 0.0810 - val_r_square: 0.9228\n",
            "Epoch 1087/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0474 - r_square: 0.9204\n",
            "Epoch 1087: val_loss did not improve from 0.08046\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0472 - r_square: 0.9237 - val_loss: 0.0806 - val_r_square: 0.9230\n",
            "Epoch 1088/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0487 - r_square: 0.9266\n",
            "Epoch 1088: val_loss improved from 0.08046 to 0.08025, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0474 - r_square: 0.9236 - val_loss: 0.0803 - val_r_square: 0.9230\n",
            "Epoch 1089/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0455 - r_square: 0.9223\n",
            "Epoch 1089: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0473 - r_square: 0.9237 - val_loss: 0.0811 - val_r_square: 0.9228\n",
            "Epoch 1090/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0467 - r_square: 0.9259\n",
            "Epoch 1090: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0472 - r_square: 0.9238 - val_loss: 0.0804 - val_r_square: 0.9228\n",
            "Epoch 1091/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0470 - r_square: 0.9240\n",
            "Epoch 1091: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0470 - r_square: 0.9240 - val_loss: 0.0805 - val_r_square: 0.9229\n",
            "Epoch 1092/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0468 - r_square: 0.9227\n",
            "Epoch 1092: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0472 - r_square: 0.9239 - val_loss: 0.0805 - val_r_square: 0.9228\n",
            "Epoch 1093/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0468 - r_square: 0.9285\n",
            "Epoch 1093: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0470 - r_square: 0.9240 - val_loss: 0.0803 - val_r_square: 0.9229\n",
            "Epoch 1094/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0466 - r_square: 0.9216\n",
            "Epoch 1094: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0470 - r_square: 0.9240 - val_loss: 0.0806 - val_r_square: 0.9228\n",
            "Epoch 1095/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0468 - r_square: 0.9239\n",
            "Epoch 1095: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0469 - r_square: 0.9241 - val_loss: 0.0805 - val_r_square: 0.9229\n",
            "Epoch 1096/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0468 - r_square: 0.9256\n",
            "Epoch 1096: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0469 - r_square: 0.9240 - val_loss: 0.0807 - val_r_square: 0.9228\n",
            "Epoch 1097/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0466 - r_square: 0.9255\n",
            "Epoch 1097: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0469 - r_square: 0.9244 - val_loss: 0.0808 - val_r_square: 0.9225\n",
            "Epoch 1098/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0475 - r_square: 0.9298\n",
            "Epoch 1098: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0474 - r_square: 0.9239 - val_loss: 0.0804 - val_r_square: 0.9229\n",
            "Epoch 1099/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0467 - r_square: 0.9248\n",
            "Epoch 1099: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0471 - r_square: 0.9240 - val_loss: 0.0808 - val_r_square: 0.9226\n",
            "Epoch 1100/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0464 - r_square: 0.9266\n",
            "Epoch 1100: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0469 - r_square: 0.9242 - val_loss: 0.0808 - val_r_square: 0.9226\n",
            "Epoch 1101/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0476 - r_square: 0.9193\n",
            "Epoch 1101: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0470 - r_square: 0.9244 - val_loss: 0.0806 - val_r_square: 0.9229\n",
            "Epoch 1102/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0482 - r_square: 0.9281\n",
            "Epoch 1102: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0473 - r_square: 0.9241 - val_loss: 0.0806 - val_r_square: 0.9228\n",
            "Epoch 1103/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0470 - r_square: 0.9301\n",
            "Epoch 1103: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0468 - r_square: 0.9241 - val_loss: 0.0805 - val_r_square: 0.9228\n",
            "Epoch 1104/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0463 - r_square: 0.9171\n",
            "Epoch 1104: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0470 - r_square: 0.9241 - val_loss: 0.0803 - val_r_square: 0.9228\n",
            "Epoch 1105/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0476 - r_square: 0.9224\n",
            "Epoch 1105: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0469 - r_square: 0.9235 - val_loss: 0.0811 - val_r_square: 0.9228\n",
            "Epoch 1106/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0470 - r_square: 0.9241\n",
            "Epoch 1106: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0470 - r_square: 0.9241 - val_loss: 0.0804 - val_r_square: 0.9227\n",
            "Epoch 1107/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0473 - r_square: 0.9297\n",
            "Epoch 1107: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0470 - r_square: 0.9237 - val_loss: 0.0813 - val_r_square: 0.9223\n",
            "Epoch 1108/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0472 - r_square: 0.9237\n",
            "Epoch 1108: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0470 - r_square: 0.9241 - val_loss: 0.0815 - val_r_square: 0.9224\n",
            "Epoch 1109/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0466 - r_square: 0.9244\n",
            "Epoch 1109: val_loss did not improve from 0.08025\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0469 - r_square: 0.9242 - val_loss: 0.0803 - val_r_square: 0.9227\n",
            "Epoch 1110/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0471 - r_square: 0.9229\n",
            "Epoch 1110: val_loss improved from 0.08025 to 0.08016, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0469 - r_square: 0.9240 - val_loss: 0.0802 - val_r_square: 0.9228\n",
            "Epoch 1111/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0467 - r_square: 0.9268\n",
            "Epoch 1111: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0467 - r_square: 0.9242 - val_loss: 0.0804 - val_r_square: 0.9227\n",
            "Epoch 1112/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0471 - r_square: 0.9226\n",
            "Epoch 1112: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0468 - r_square: 0.9244 - val_loss: 0.0803 - val_r_square: 0.9227\n",
            "Epoch 1113/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0474 - r_square: 0.9232\n",
            "Epoch 1113: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0469 - r_square: 0.9241 - val_loss: 0.0806 - val_r_square: 0.9226\n",
            "Epoch 1114/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0475 - r_square: 0.9248\n",
            "Epoch 1114: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0467 - r_square: 0.9246 - val_loss: 0.0804 - val_r_square: 0.9227\n",
            "Epoch 1115/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0477 - r_square: 0.9239\n",
            "Epoch 1115: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0467 - r_square: 0.9242 - val_loss: 0.0803 - val_r_square: 0.9227\n",
            "Epoch 1116/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0464 - r_square: 0.9280\n",
            "Epoch 1116: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0467 - r_square: 0.9248 - val_loss: 0.0805 - val_r_square: 0.9226\n",
            "Epoch 1117/1500\n",
            "49/67 [====================>.........] - ETA: 0s - loss: 0.0470 - r_square: 0.9248\n",
            "Epoch 1117: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0467 - r_square: 0.9246 - val_loss: 0.0804 - val_r_square: 0.9227\n",
            "Epoch 1118/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0465 - r_square: 0.9289\n",
            "Epoch 1118: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0468 - r_square: 0.9242 - val_loss: 0.0806 - val_r_square: 0.9226\n",
            "Epoch 1119/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0463 - r_square: 0.9219\n",
            "Epoch 1119: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0466 - r_square: 0.9248 - val_loss: 0.0803 - val_r_square: 0.9227\n",
            "Epoch 1120/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0462 - r_square: 0.9258\n",
            "Epoch 1120: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0466 - r_square: 0.9245 - val_loss: 0.0802 - val_r_square: 0.9226\n",
            "Epoch 1121/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0440 - r_square: 0.9304\n",
            "Epoch 1121: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0465 - r_square: 0.9247 - val_loss: 0.0814 - val_r_square: 0.9222\n",
            "Epoch 1122/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0467 - r_square: 0.9217\n",
            "Epoch 1122: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0468 - r_square: 0.9244 - val_loss: 0.0802 - val_r_square: 0.9226\n",
            "Epoch 1123/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0464 - r_square: 0.9303\n",
            "Epoch 1123: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0465 - r_square: 0.9248 - val_loss: 0.0802 - val_r_square: 0.9227\n",
            "Epoch 1124/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0453 - r_square: 0.9236\n",
            "Epoch 1124: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0466 - r_square: 0.9251 - val_loss: 0.0804 - val_r_square: 0.9226\n",
            "Epoch 1125/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0461 - r_square: 0.9302\n",
            "Epoch 1125: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0466 - r_square: 0.9243 - val_loss: 0.0802 - val_r_square: 0.9226\n",
            "Epoch 1126/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0466 - r_square: 0.9163\n",
            "Epoch 1126: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0465 - r_square: 0.9246 - val_loss: 0.0803 - val_r_square: 0.9225\n",
            "Epoch 1127/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0466 - r_square: 0.9243\n",
            "Epoch 1127: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0466 - r_square: 0.9245 - val_loss: 0.0805 - val_r_square: 0.9226\n",
            "Epoch 1128/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0469 - r_square: 0.9248\n",
            "Epoch 1128: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0465 - r_square: 0.9250 - val_loss: 0.0804 - val_r_square: 0.9226\n",
            "Epoch 1129/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0466 - r_square: 0.9259\n",
            "Epoch 1129: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0465 - r_square: 0.9249 - val_loss: 0.0811 - val_r_square: 0.9224\n",
            "Epoch 1130/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0461 - r_square: 0.9255\n",
            "Epoch 1130: val_loss did not improve from 0.08016\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0464 - r_square: 0.9250 - val_loss: 0.0804 - val_r_square: 0.9227\n",
            "Epoch 1131/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0441 - r_square: 0.9211\n",
            "Epoch 1131: val_loss improved from 0.08016 to 0.08002, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0465 - r_square: 0.9249 - val_loss: 0.0800 - val_r_square: 0.9226\n",
            "Epoch 1132/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0475 - r_square: 0.9208\n",
            "Epoch 1132: val_loss did not improve from 0.08002\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0465 - r_square: 0.9249 - val_loss: 0.0802 - val_r_square: 0.9226\n",
            "Epoch 1133/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0455 - r_square: 0.9218\n",
            "Epoch 1133: val_loss improved from 0.08002 to 0.07999, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0464 - r_square: 0.9250 - val_loss: 0.0800 - val_r_square: 0.9227\n",
            "Epoch 1134/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0457 - r_square: 0.9236\n",
            "Epoch 1134: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0463 - r_square: 0.9252 - val_loss: 0.0802 - val_r_square: 0.9227\n",
            "Epoch 1135/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0462 - r_square: 0.9248\n",
            "Epoch 1135: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0463 - r_square: 0.9250 - val_loss: 0.0806 - val_r_square: 0.9224\n",
            "Epoch 1136/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0452 - r_square: 0.9310\n",
            "Epoch 1136: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0465 - r_square: 0.9248 - val_loss: 0.0801 - val_r_square: 0.9225\n",
            "Epoch 1137/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0465 - r_square: 0.9248\n",
            "Epoch 1137: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0465 - r_square: 0.9248 - val_loss: 0.0801 - val_r_square: 0.9224\n",
            "Epoch 1138/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0461 - r_square: 0.9260\n",
            "Epoch 1138: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0462 - r_square: 0.9248 - val_loss: 0.0807 - val_r_square: 0.9223\n",
            "Epoch 1139/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0468 - r_square: 0.9281\n",
            "Epoch 1139: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0466 - r_square: 0.9247 - val_loss: 0.0805 - val_r_square: 0.9223\n",
            "Epoch 1140/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0461 - r_square: 0.9245\n",
            "Epoch 1140: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0463 - r_square: 0.9248 - val_loss: 0.0800 - val_r_square: 0.9224\n",
            "Epoch 1141/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0462 - r_square: 0.9252\n",
            "Epoch 1141: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0462 - r_square: 0.9252 - val_loss: 0.0800 - val_r_square: 0.9226\n",
            "Epoch 1142/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0459 - r_square: 0.9297\n",
            "Epoch 1142: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0465 - r_square: 0.9251 - val_loss: 0.0800 - val_r_square: 0.9227\n",
            "Epoch 1143/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0461 - r_square: 0.9249\n",
            "Epoch 1143: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0465 - r_square: 0.9251 - val_loss: 0.0801 - val_r_square: 0.9226\n",
            "Epoch 1144/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0473 - r_square: 0.9261\n",
            "Epoch 1144: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0464 - r_square: 0.9254 - val_loss: 0.0804 - val_r_square: 0.9222\n",
            "Epoch 1145/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0463 - r_square: 0.9247\n",
            "Epoch 1145: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0463 - r_square: 0.9247 - val_loss: 0.0804 - val_r_square: 0.9224\n",
            "Epoch 1146/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0467 - r_square: 0.9251\n",
            "Epoch 1146: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0466 - r_square: 0.9252 - val_loss: 0.0800 - val_r_square: 0.9225\n",
            "Epoch 1147/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0464 - r_square: 0.9264\n",
            "Epoch 1147: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0462 - r_square: 0.9254 - val_loss: 0.0804 - val_r_square: 0.9223\n",
            "Epoch 1148/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0465 - r_square: 0.9249\n",
            "Epoch 1148: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0461 - r_square: 0.9253 - val_loss: 0.0803 - val_r_square: 0.9223\n",
            "Epoch 1149/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0456 - r_square: 0.9259\n",
            "Epoch 1149: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0463 - r_square: 0.9252 - val_loss: 0.0801 - val_r_square: 0.9223\n",
            "Epoch 1150/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0456 - r_square: 0.9224\n",
            "Epoch 1150: val_loss did not improve from 0.07999\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0460 - r_square: 0.9251 - val_loss: 0.0803 - val_r_square: 0.9224\n",
            "Epoch 1151/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0461 - r_square: 0.9248\n",
            "Epoch 1151: val_loss improved from 0.07999 to 0.07993, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0461 - r_square: 0.9252 - val_loss: 0.0799 - val_r_square: 0.9225\n",
            "Epoch 1152/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0470 - r_square: 0.9213\n",
            "Epoch 1152: val_loss did not improve from 0.07993\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0461 - r_square: 0.9253 - val_loss: 0.0800 - val_r_square: 0.9223\n",
            "Epoch 1153/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0461 - r_square: 0.9257\n",
            "Epoch 1153: val_loss did not improve from 0.07993\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0461 - r_square: 0.9257 - val_loss: 0.0804 - val_r_square: 0.9225\n",
            "Epoch 1154/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0455 - r_square: 0.9255\n",
            "Epoch 1154: val_loss did not improve from 0.07993\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0461 - r_square: 0.9256 - val_loss: 0.0807 - val_r_square: 0.9221\n",
            "Epoch 1155/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0464 - r_square: 0.9255\n",
            "Epoch 1155: val_loss improved from 0.07993 to 0.07990, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0461 - r_square: 0.9252 - val_loss: 0.0799 - val_r_square: 0.9224\n",
            "Epoch 1156/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0457 - r_square: 0.9266\n",
            "Epoch 1156: val_loss did not improve from 0.07990\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0460 - r_square: 0.9257 - val_loss: 0.0803 - val_r_square: 0.9221\n",
            "Epoch 1157/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0460 - r_square: 0.9289\n",
            "Epoch 1157: val_loss did not improve from 0.07990\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0462 - r_square: 0.9252 - val_loss: 0.0800 - val_r_square: 0.9223\n",
            "Epoch 1158/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0461 - r_square: 0.9252\n",
            "Epoch 1158: val_loss did not improve from 0.07990\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0461 - r_square: 0.9252 - val_loss: 0.0805 - val_r_square: 0.9224\n",
            "Epoch 1159/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0460 - r_square: 0.9255\n",
            "Epoch 1159: val_loss did not improve from 0.07990\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0460 - r_square: 0.9255 - val_loss: 0.0800 - val_r_square: 0.9223\n",
            "Epoch 1160/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0459 - r_square: 0.9258\n",
            "Epoch 1160: val_loss did not improve from 0.07990\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0459 - r_square: 0.9258 - val_loss: 0.0809 - val_r_square: 0.9220\n",
            "Epoch 1161/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0464 - r_square: 0.9255\n",
            "Epoch 1161: val_loss did not improve from 0.07990\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0460 - r_square: 0.9258 - val_loss: 0.0800 - val_r_square: 0.9224\n",
            "Epoch 1162/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0460 - r_square: 0.9255\n",
            "Epoch 1162: val_loss did not improve from 0.07990\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0460 - r_square: 0.9255 - val_loss: 0.0805 - val_r_square: 0.9222\n",
            "Epoch 1163/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0461 - r_square: 0.9251\n",
            "Epoch 1163: val_loss did not improve from 0.07990\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0461 - r_square: 0.9251 - val_loss: 0.0800 - val_r_square: 0.9224\n",
            "Epoch 1164/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0464 - r_square: 0.9248\n",
            "Epoch 1164: val_loss did not improve from 0.07990\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0459 - r_square: 0.9262 - val_loss: 0.0801 - val_r_square: 0.9224\n",
            "Epoch 1165/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0458 - r_square: 0.9254\n",
            "Epoch 1165: val_loss improved from 0.07990 to 0.07981, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0459 - r_square: 0.9258 - val_loss: 0.0798 - val_r_square: 0.9224\n",
            "Epoch 1166/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0465 - r_square: 0.9248\n",
            "Epoch 1166: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0460 - r_square: 0.9256 - val_loss: 0.0800 - val_r_square: 0.9221\n",
            "Epoch 1167/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0459 - r_square: 0.9263\n",
            "Epoch 1167: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0458 - r_square: 0.9258 - val_loss: 0.0799 - val_r_square: 0.9223\n",
            "Epoch 1168/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0460 - r_square: 0.9249\n",
            "Epoch 1168: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0458 - r_square: 0.9261 - val_loss: 0.0803 - val_r_square: 0.9223\n",
            "Epoch 1169/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0462 - r_square: 0.9335\n",
            "Epoch 1169: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0459 - r_square: 0.9255 - val_loss: 0.0806 - val_r_square: 0.9221\n",
            "Epoch 1170/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0448 - r_square: 0.9244\n",
            "Epoch 1170: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0459 - r_square: 0.9258 - val_loss: 0.0799 - val_r_square: 0.9225\n",
            "Epoch 1171/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0459 - r_square: 0.9241\n",
            "Epoch 1171: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0459 - r_square: 0.9255 - val_loss: 0.0802 - val_r_square: 0.9224\n",
            "Epoch 1172/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0457 - r_square: 0.9255\n",
            "Epoch 1172: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0458 - r_square: 0.9256 - val_loss: 0.0801 - val_r_square: 0.9222\n",
            "Epoch 1173/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0453 - r_square: 0.9299\n",
            "Epoch 1173: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0459 - r_square: 0.9255 - val_loss: 0.0803 - val_r_square: 0.9226\n",
            "Epoch 1174/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0457 - r_square: 0.9237\n",
            "Epoch 1174: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0459 - r_square: 0.9256 - val_loss: 0.0800 - val_r_square: 0.9223\n",
            "Epoch 1175/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0460 - r_square: 0.9239\n",
            "Epoch 1175: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0458 - r_square: 0.9259 - val_loss: 0.0805 - val_r_square: 0.9221\n",
            "Epoch 1176/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0463 - r_square: 0.9205\n",
            "Epoch 1176: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0458 - r_square: 0.9252 - val_loss: 0.0802 - val_r_square: 0.9222\n",
            "Epoch 1177/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0464 - r_square: 0.9315\n",
            "Epoch 1177: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0457 - r_square: 0.9258 - val_loss: 0.0799 - val_r_square: 0.9222\n",
            "Epoch 1178/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0454 - r_square: 0.9261\n",
            "Epoch 1178: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0456 - r_square: 0.9258 - val_loss: 0.0799 - val_r_square: 0.9223\n",
            "Epoch 1179/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0460 - r_square: 0.9251\n",
            "Epoch 1179: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0457 - r_square: 0.9260 - val_loss: 0.0800 - val_r_square: 0.9223\n",
            "Epoch 1180/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0457 - r_square: 0.9260\n",
            "Epoch 1180: val_loss did not improve from 0.07981\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0457 - r_square: 0.9260 - val_loss: 0.0799 - val_r_square: 0.9222\n",
            "Epoch 1181/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0458 - r_square: 0.9189\n",
            "Epoch 1181: val_loss improved from 0.07981 to 0.07980, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0457 - r_square: 0.9261 - val_loss: 0.0798 - val_r_square: 0.9222\n",
            "Epoch 1182/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0453 - r_square: 0.9197\n",
            "Epoch 1182: val_loss did not improve from 0.07980\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0456 - r_square: 0.9258 - val_loss: 0.0802 - val_r_square: 0.9222\n",
            "Epoch 1183/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0458 - r_square: 0.9227\n",
            "Epoch 1183: val_loss improved from 0.07980 to 0.07965, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0456 - r_square: 0.9258 - val_loss: 0.0797 - val_r_square: 0.9224\n",
            "Epoch 1184/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0458 - r_square: 0.9254\n",
            "Epoch 1184: val_loss improved from 0.07965 to 0.07964, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0456 - r_square: 0.9258 - val_loss: 0.0796 - val_r_square: 0.9222\n",
            "Epoch 1185/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0454 - r_square: 0.9318\n",
            "Epoch 1185: val_loss did not improve from 0.07964\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0456 - r_square: 0.9261 - val_loss: 0.0801 - val_r_square: 0.9223\n",
            "Epoch 1186/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0458 - r_square: 0.9323\n",
            "Epoch 1186: val_loss did not improve from 0.07964\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0456 - r_square: 0.9264 - val_loss: 0.0800 - val_r_square: 0.9223\n",
            "Epoch 1187/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0456 - r_square: 0.9255\n",
            "Epoch 1187: val_loss did not improve from 0.07964\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0456 - r_square: 0.9255 - val_loss: 0.0799 - val_r_square: 0.9222\n",
            "Epoch 1188/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0454 - r_square: 0.9253\n",
            "Epoch 1188: val_loss did not improve from 0.07964\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0457 - r_square: 0.9257 - val_loss: 0.0799 - val_r_square: 0.9222\n",
            "Epoch 1189/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0454 - r_square: 0.9261\n",
            "Epoch 1189: val_loss did not improve from 0.07964\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0454 - r_square: 0.9261 - val_loss: 0.0797 - val_r_square: 0.9223\n",
            "Epoch 1190/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0453 - r_square: 0.9261\n",
            "Epoch 1190: val_loss did not improve from 0.07964\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0453 - r_square: 0.9262 - val_loss: 0.0806 - val_r_square: 0.9220\n",
            "Epoch 1191/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0461 - r_square: 0.9315\n",
            "Epoch 1191: val_loss did not improve from 0.07964\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0456 - r_square: 0.9262 - val_loss: 0.0801 - val_r_square: 0.9220\n",
            "Epoch 1192/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0455 - r_square: 0.9262\n",
            "Epoch 1192: val_loss did not improve from 0.07964\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0456 - r_square: 0.9262 - val_loss: 0.0801 - val_r_square: 0.9223\n",
            "Epoch 1193/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0451 - r_square: 0.9274\n",
            "Epoch 1193: val_loss did not improve from 0.07964\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0455 - r_square: 0.9260 - val_loss: 0.0798 - val_r_square: 0.9222\n",
            "Epoch 1194/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0460 - r_square: 0.9291\n",
            "Epoch 1194: val_loss improved from 0.07964 to 0.07959, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0455 - r_square: 0.9260 - val_loss: 0.0796 - val_r_square: 0.9223\n",
            "Epoch 1195/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0459 - r_square: 0.9282\n",
            "Epoch 1195: val_loss did not improve from 0.07959\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0453 - r_square: 0.9265 - val_loss: 0.0802 - val_r_square: 0.9222\n",
            "Epoch 1196/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0453 - r_square: 0.9263\n",
            "Epoch 1196: val_loss did not improve from 0.07959\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0453 - r_square: 0.9263 - val_loss: 0.0796 - val_r_square: 0.9222\n",
            "Epoch 1197/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0448 - r_square: 0.9266\n",
            "Epoch 1197: val_loss improved from 0.07959 to 0.07957, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0454 - r_square: 0.9265 - val_loss: 0.0796 - val_r_square: 0.9221\n",
            "Epoch 1198/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0458 - r_square: 0.9285\n",
            "Epoch 1198: val_loss improved from 0.07957 to 0.07956, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0455 - r_square: 0.9268 - val_loss: 0.0796 - val_r_square: 0.9221\n",
            "Epoch 1199/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0453 - r_square: 0.9263\n",
            "Epoch 1199: val_loss did not improve from 0.07956\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0453 - r_square: 0.9263 - val_loss: 0.0799 - val_r_square: 0.9222\n",
            "Epoch 1200/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0449 - r_square: 0.9298\n",
            "Epoch 1200: val_loss did not improve from 0.07956\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0453 - r_square: 0.9263 - val_loss: 0.0801 - val_r_square: 0.9221\n",
            "Epoch 1201/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0454 - r_square: 0.9240\n",
            "Epoch 1201: val_loss did not improve from 0.07956\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0453 - r_square: 0.9264 - val_loss: 0.0800 - val_r_square: 0.9222\n",
            "Epoch 1202/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0455 - r_square: 0.9267\n",
            "Epoch 1202: val_loss did not improve from 0.07956\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0453 - r_square: 0.9266 - val_loss: 0.0798 - val_r_square: 0.9221\n",
            "Epoch 1203/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0462 - r_square: 0.9265\n",
            "Epoch 1203: val_loss did not improve from 0.07956\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0453 - r_square: 0.9263 - val_loss: 0.0806 - val_r_square: 0.9217\n",
            "Epoch 1204/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0454 - r_square: 0.9265\n",
            "Epoch 1204: val_loss did not improve from 0.07956\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0452 - r_square: 0.9263 - val_loss: 0.0796 - val_r_square: 0.9222\n",
            "Epoch 1205/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0458 - r_square: 0.9261\n",
            "Epoch 1205: val_loss improved from 0.07956 to 0.07946, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0453 - r_square: 0.9265 - val_loss: 0.0795 - val_r_square: 0.9222\n",
            "Epoch 1206/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0462 - r_square: 0.9250\n",
            "Epoch 1206: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0452 - r_square: 0.9266 - val_loss: 0.0797 - val_r_square: 0.9220\n",
            "Epoch 1207/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0453 - r_square: 0.9262\n",
            "Epoch 1207: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0452 - r_square: 0.9267 - val_loss: 0.0801 - val_r_square: 0.9220\n",
            "Epoch 1208/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0455 - r_square: 0.9266\n",
            "Epoch 1208: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0455 - r_square: 0.9266 - val_loss: 0.0797 - val_r_square: 0.9220\n",
            "Epoch 1209/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0449 - r_square: 0.9265\n",
            "Epoch 1209: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0452 - r_square: 0.9267 - val_loss: 0.0799 - val_r_square: 0.9222\n",
            "Epoch 1210/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0452 - r_square: 0.9268\n",
            "Epoch 1210: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0452 - r_square: 0.9268 - val_loss: 0.0796 - val_r_square: 0.9222\n",
            "Epoch 1211/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0455 - r_square: 0.9262\n",
            "Epoch 1211: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0452 - r_square: 0.9266 - val_loss: 0.0799 - val_r_square: 0.9221\n",
            "Epoch 1212/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0456 - r_square: 0.9260\n",
            "Epoch 1212: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0451 - r_square: 0.9268 - val_loss: 0.0795 - val_r_square: 0.9221\n",
            "Epoch 1213/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0454 - r_square: 0.9266\n",
            "Epoch 1213: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0453 - r_square: 0.9270 - val_loss: 0.0796 - val_r_square: 0.9220\n",
            "Epoch 1214/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0435 - r_square: 0.9244\n",
            "Epoch 1214: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0452 - r_square: 0.9265 - val_loss: 0.0796 - val_r_square: 0.9221\n",
            "Epoch 1215/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0450 - r_square: 0.9266\n",
            "Epoch 1215: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0450 - r_square: 0.9266 - val_loss: 0.0800 - val_r_square: 0.9223\n",
            "Epoch 1216/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0451 - r_square: 0.9163\n",
            "Epoch 1216: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0453 - r_square: 0.9262 - val_loss: 0.0799 - val_r_square: 0.9221\n",
            "Epoch 1217/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0451 - r_square: 0.9280\n",
            "Epoch 1217: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0452 - r_square: 0.9269 - val_loss: 0.0800 - val_r_square: 0.9220\n",
            "Epoch 1218/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0451 - r_square: 0.9193\n",
            "Epoch 1218: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0451 - r_square: 0.9265 - val_loss: 0.0800 - val_r_square: 0.9222\n",
            "Epoch 1219/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0454 - r_square: 0.9276\n",
            "Epoch 1219: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0452 - r_square: 0.9266 - val_loss: 0.0800 - val_r_square: 0.9216\n",
            "Epoch 1220/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0453 - r_square: 0.9355\n",
            "Epoch 1220: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0451 - r_square: 0.9271 - val_loss: 0.0796 - val_r_square: 0.9220\n",
            "Epoch 1221/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0447 - r_square: 0.9336\n",
            "Epoch 1221: val_loss did not improve from 0.07946\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0451 - r_square: 0.9267 - val_loss: 0.0799 - val_r_square: 0.9220\n",
            "Epoch 1222/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0459 - r_square: 0.9292\n",
            "Epoch 1222: val_loss improved from 0.07946 to 0.07940, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0451 - r_square: 0.9265 - val_loss: 0.0794 - val_r_square: 0.9222\n",
            "Epoch 1223/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0452 - r_square: 0.9295\n",
            "Epoch 1223: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0450 - r_square: 0.9268 - val_loss: 0.0797 - val_r_square: 0.9222\n",
            "Epoch 1224/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0449 - r_square: 0.9269\n",
            "Epoch 1224: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0449 - r_square: 0.9269 - val_loss: 0.0797 - val_r_square: 0.9220\n",
            "Epoch 1225/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0447 - r_square: 0.9269\n",
            "Epoch 1225: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0451 - r_square: 0.9269 - val_loss: 0.0796 - val_r_square: 0.9221\n",
            "Epoch 1226/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0444 - r_square: 0.9272\n",
            "Epoch 1226: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.0450 - r_square: 0.9272 - val_loss: 0.0796 - val_r_square: 0.9221\n",
            "Epoch 1227/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0449 - r_square: 0.9269\n",
            "Epoch 1227: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0449 - r_square: 0.9269 - val_loss: 0.0796 - val_r_square: 0.9220\n",
            "Epoch 1228/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0455 - r_square: 0.9261\n",
            "Epoch 1228: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0452 - r_square: 0.9268 - val_loss: 0.0796 - val_r_square: 0.9221\n",
            "Epoch 1229/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0450 - r_square: 0.9278\n",
            "Epoch 1229: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0449 - r_square: 0.9270 - val_loss: 0.0794 - val_r_square: 0.9221\n",
            "Epoch 1230/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0451 - r_square: 0.9278\n",
            "Epoch 1230: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0451 - r_square: 0.9270 - val_loss: 0.0800 - val_r_square: 0.9219\n",
            "Epoch 1231/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0459 - r_square: 0.9230\n",
            "Epoch 1231: val_loss improved from 0.07940 to 0.07940, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0449 - r_square: 0.9269 - val_loss: 0.0794 - val_r_square: 0.9220\n",
            "Epoch 1232/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0442 - r_square: 0.9297\n",
            "Epoch 1232: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0448 - r_square: 0.9269 - val_loss: 0.0796 - val_r_square: 0.9220\n",
            "Epoch 1233/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0450 - r_square: 0.9265\n",
            "Epoch 1233: val_loss did not improve from 0.07940\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0449 - r_square: 0.9266 - val_loss: 0.0795 - val_r_square: 0.9220\n",
            "Epoch 1234/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0450 - r_square: 0.9302\n",
            "Epoch 1234: val_loss improved from 0.07940 to 0.07937, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0449 - r_square: 0.9270 - val_loss: 0.0794 - val_r_square: 0.9220\n",
            "Epoch 1235/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0447 - r_square: 0.9276\n",
            "Epoch 1235: val_loss did not improve from 0.07937\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0449 - r_square: 0.9268 - val_loss: 0.0794 - val_r_square: 0.9219\n",
            "Epoch 1236/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0448 - r_square: 0.9234\n",
            "Epoch 1236: val_loss improved from 0.07937 to 0.07935, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0448 - r_square: 0.9268 - val_loss: 0.0794 - val_r_square: 0.9220\n",
            "Epoch 1237/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0445 - r_square: 0.9263\n",
            "Epoch 1237: val_loss improved from 0.07935 to 0.07931, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 15ms/step - loss: 0.0447 - r_square: 0.9273 - val_loss: 0.0793 - val_r_square: 0.9220\n",
            "Epoch 1238/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0448 - r_square: 0.9313\n",
            "Epoch 1238: val_loss did not improve from 0.07931\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0448 - r_square: 0.9273 - val_loss: 0.0795 - val_r_square: 0.9222\n",
            "Epoch 1239/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0449 - r_square: 0.9269\n",
            "Epoch 1239: val_loss did not improve from 0.07931\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0449 - r_square: 0.9269 - val_loss: 0.0803 - val_r_square: 0.9219\n",
            "Epoch 1240/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0440 - r_square: 0.9289\n",
            "Epoch 1240: val_loss did not improve from 0.07931\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0447 - r_square: 0.9272 - val_loss: 0.0794 - val_r_square: 0.9220\n",
            "Epoch 1241/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0447 - r_square: 0.9271\n",
            "Epoch 1241: val_loss did not improve from 0.07931\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0447 - r_square: 0.9271 - val_loss: 0.0795 - val_r_square: 0.9220\n",
            "Epoch 1242/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0448 - r_square: 0.9272\n",
            "Epoch 1242: val_loss did not improve from 0.07931\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0448 - r_square: 0.9272 - val_loss: 0.0795 - val_r_square: 0.9219\n",
            "Epoch 1243/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0447 - r_square: 0.9251\n",
            "Epoch 1243: val_loss improved from 0.07931 to 0.07919, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0447 - r_square: 0.9273 - val_loss: 0.0792 - val_r_square: 0.9219\n",
            "Epoch 1244/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0434 - r_square: 0.9298\n",
            "Epoch 1244: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0446 - r_square: 0.9271 - val_loss: 0.0799 - val_r_square: 0.9221\n",
            "Epoch 1245/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0447 - r_square: 0.9253\n",
            "Epoch 1245: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0448 - r_square: 0.9268 - val_loss: 0.0798 - val_r_square: 0.9219\n",
            "Epoch 1246/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0447 - r_square: 0.9273\n",
            "Epoch 1246: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0447 - r_square: 0.9273 - val_loss: 0.0792 - val_r_square: 0.9220\n",
            "Epoch 1247/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0449 - r_square: 0.9265\n",
            "Epoch 1247: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0447 - r_square: 0.9272 - val_loss: 0.0795 - val_r_square: 0.9219\n",
            "Epoch 1248/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0444 - r_square: 0.9266\n",
            "Epoch 1248: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0447 - r_square: 0.9269 - val_loss: 0.0812 - val_r_square: 0.9214\n",
            "Epoch 1249/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0447 - r_square: 0.9275\n",
            "Epoch 1249: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.0447 - r_square: 0.9275 - val_loss: 0.0793 - val_r_square: 0.9220\n",
            "Epoch 1250/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0448 - r_square: 0.9283\n",
            "Epoch 1250: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0447 - r_square: 0.9273 - val_loss: 0.0800 - val_r_square: 0.9217\n",
            "Epoch 1251/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0448 - r_square: 0.9272\n",
            "Epoch 1251: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0447 - r_square: 0.9273 - val_loss: 0.0797 - val_r_square: 0.9218\n",
            "Epoch 1252/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0451 - r_square: 0.9277\n",
            "Epoch 1252: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0448 - r_square: 0.9276 - val_loss: 0.0793 - val_r_square: 0.9219\n",
            "Epoch 1253/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0446 - r_square: 0.9274\n",
            "Epoch 1253: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0446 - r_square: 0.9274 - val_loss: 0.0794 - val_r_square: 0.9220\n",
            "Epoch 1254/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0437 - r_square: 0.9282\n",
            "Epoch 1254: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0445 - r_square: 0.9279 - val_loss: 0.0794 - val_r_square: 0.9220\n",
            "Epoch 1255/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0442 - r_square: 0.9268\n",
            "Epoch 1255: val_loss did not improve from 0.07919\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0444 - r_square: 0.9275 - val_loss: 0.0792 - val_r_square: 0.9222\n",
            "Epoch 1256/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0442 - r_square: 0.9255\n",
            "Epoch 1256: val_loss improved from 0.07919 to 0.07915, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0445 - r_square: 0.9274 - val_loss: 0.0792 - val_r_square: 0.9221\n",
            "Epoch 1257/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0449 - r_square: 0.9290\n",
            "Epoch 1257: val_loss did not improve from 0.07915\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0445 - r_square: 0.9277 - val_loss: 0.0794 - val_r_square: 0.9219\n",
            "Epoch 1258/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0444 - r_square: 0.9272\n",
            "Epoch 1258: val_loss did not improve from 0.07915\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0444 - r_square: 0.9275 - val_loss: 0.0795 - val_r_square: 0.9218\n",
            "Epoch 1259/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0442 - r_square: 0.9259\n",
            "Epoch 1259: val_loss improved from 0.07915 to 0.07905, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0445 - r_square: 0.9272 - val_loss: 0.0791 - val_r_square: 0.9219\n",
            "Epoch 1260/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0442 - r_square: 0.9257\n",
            "Epoch 1260: val_loss did not improve from 0.07905\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.0446 - r_square: 0.9272 - val_loss: 0.0795 - val_r_square: 0.9218\n",
            "Epoch 1261/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0445 - r_square: 0.9275\n",
            "Epoch 1261: val_loss did not improve from 0.07905\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0445 - r_square: 0.9275 - val_loss: 0.0796 - val_r_square: 0.9220\n",
            "Epoch 1262/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0445 - r_square: 0.9275\n",
            "Epoch 1262: val_loss did not improve from 0.07905\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0445 - r_square: 0.9275 - val_loss: 0.0792 - val_r_square: 0.9220\n",
            "Epoch 1263/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0440 - r_square: 0.9256\n",
            "Epoch 1263: val_loss did not improve from 0.07905\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0446 - r_square: 0.9272 - val_loss: 0.0799 - val_r_square: 0.9218\n",
            "Epoch 1264/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0446 - r_square: 0.9258\n",
            "Epoch 1264: val_loss improved from 0.07905 to 0.07897, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0446 - r_square: 0.9278 - val_loss: 0.0790 - val_r_square: 0.9220\n",
            "Epoch 1265/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0443 - r_square: 0.9252\n",
            "Epoch 1265: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0446 - r_square: 0.9272 - val_loss: 0.0791 - val_r_square: 0.9220\n",
            "Epoch 1266/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0442 - r_square: 0.9269\n",
            "Epoch 1266: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0444 - r_square: 0.9276 - val_loss: 0.0798 - val_r_square: 0.9218\n",
            "Epoch 1267/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0452 - r_square: 0.9250\n",
            "Epoch 1267: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0442 - r_square: 0.9276 - val_loss: 0.0793 - val_r_square: 0.9216\n",
            "Epoch 1268/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0445 - r_square: 0.9263\n",
            "Epoch 1268: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0444 - r_square: 0.9279 - val_loss: 0.0790 - val_r_square: 0.9221\n",
            "Epoch 1269/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0443 - r_square: 0.9277\n",
            "Epoch 1269: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0443 - r_square: 0.9277 - val_loss: 0.0792 - val_r_square: 0.9219\n",
            "Epoch 1270/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0437 - r_square: 0.9253\n",
            "Epoch 1270: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0443 - r_square: 0.9275 - val_loss: 0.0797 - val_r_square: 0.9218\n",
            "Epoch 1271/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0447 - r_square: 0.9293\n",
            "Epoch 1271: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0444 - r_square: 0.9277 - val_loss: 0.0794 - val_r_square: 0.9218\n",
            "Epoch 1272/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0452 - r_square: 0.9268\n",
            "Epoch 1272: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0444 - r_square: 0.9276 - val_loss: 0.0795 - val_r_square: 0.9219\n",
            "Epoch 1273/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0439 - r_square: 0.9295\n",
            "Epoch 1273: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0442 - r_square: 0.9278 - val_loss: 0.0797 - val_r_square: 0.9216\n",
            "Epoch 1274/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0445 - r_square: 0.9281\n",
            "Epoch 1274: val_loss did not improve from 0.07897\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0445 - r_square: 0.9281 - val_loss: 0.0791 - val_r_square: 0.9220\n",
            "Epoch 1275/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0444 - r_square: 0.9283\n",
            "Epoch 1275: val_loss improved from 0.07897 to 0.07883, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0444 - r_square: 0.9279 - val_loss: 0.0788 - val_r_square: 0.9219\n",
            "Epoch 1276/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0445 - r_square: 0.9347\n",
            "Epoch 1276: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0443 - r_square: 0.9274 - val_loss: 0.0795 - val_r_square: 0.9216\n",
            "Epoch 1277/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0444 - r_square: 0.9274\n",
            "Epoch 1277: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0444 - r_square: 0.9276 - val_loss: 0.0794 - val_r_square: 0.9216\n",
            "Epoch 1278/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0442 - r_square: 0.9277\n",
            "Epoch 1278: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0442 - r_square: 0.9280 - val_loss: 0.0792 - val_r_square: 0.9218\n",
            "Epoch 1279/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0445 - r_square: 0.9245\n",
            "Epoch 1279: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0441 - r_square: 0.9274 - val_loss: 0.0795 - val_r_square: 0.9212\n",
            "Epoch 1280/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0441 - r_square: 0.9312\n",
            "Epoch 1280: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0440 - r_square: 0.9281 - val_loss: 0.0795 - val_r_square: 0.9217\n",
            "Epoch 1281/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0442 - r_square: 0.9273\n",
            "Epoch 1281: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0440 - r_square: 0.9280 - val_loss: 0.0794 - val_r_square: 0.9218\n",
            "Epoch 1282/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0443 - r_square: 0.9268\n",
            "Epoch 1282: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0442 - r_square: 0.9278 - val_loss: 0.0793 - val_r_square: 0.9216\n",
            "Epoch 1283/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0450 - r_square: 0.9311\n",
            "Epoch 1283: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0444 - r_square: 0.9278 - val_loss: 0.0790 - val_r_square: 0.9217\n",
            "Epoch 1284/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0450 - r_square: 0.9197\n",
            "Epoch 1284: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0441 - r_square: 0.9279 - val_loss: 0.0793 - val_r_square: 0.9214\n",
            "Epoch 1285/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0440 - r_square: 0.9257\n",
            "Epoch 1285: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0441 - r_square: 0.9276 - val_loss: 0.0791 - val_r_square: 0.9217\n",
            "Epoch 1286/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0438 - r_square: 0.9278\n",
            "Epoch 1286: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0441 - r_square: 0.9279 - val_loss: 0.0790 - val_r_square: 0.9219\n",
            "Epoch 1287/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0439 - r_square: 0.9315\n",
            "Epoch 1287: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0440 - r_square: 0.9282 - val_loss: 0.0792 - val_r_square: 0.9215\n",
            "Epoch 1288/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0441 - r_square: 0.9278\n",
            "Epoch 1288: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0441 - r_square: 0.9278 - val_loss: 0.0795 - val_r_square: 0.9213\n",
            "Epoch 1289/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0438 - r_square: 0.9279\n",
            "Epoch 1289: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0441 - r_square: 0.9280 - val_loss: 0.0795 - val_r_square: 0.9219\n",
            "Epoch 1290/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0438 - r_square: 0.9290\n",
            "Epoch 1290: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0441 - r_square: 0.9277 - val_loss: 0.0790 - val_r_square: 0.9216\n",
            "Epoch 1291/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0439 - r_square: 0.9285\n",
            "Epoch 1291: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0439 - r_square: 0.9281 - val_loss: 0.0792 - val_r_square: 0.9216\n",
            "Epoch 1292/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0444 - r_square: 0.9273\n",
            "Epoch 1292: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0440 - r_square: 0.9283 - val_loss: 0.0790 - val_r_square: 0.9218\n",
            "Epoch 1293/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0448 - r_square: 0.9228\n",
            "Epoch 1293: val_loss did not improve from 0.07883\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0440 - r_square: 0.9278 - val_loss: 0.0791 - val_r_square: 0.9215\n",
            "Epoch 1294/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0441 - r_square: 0.9280\n",
            "Epoch 1294: val_loss improved from 0.07883 to 0.07882, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0440 - r_square: 0.9279 - val_loss: 0.0788 - val_r_square: 0.9217\n",
            "Epoch 1295/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0440 - r_square: 0.9274\n",
            "Epoch 1295: val_loss did not improve from 0.07882\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0440 - r_square: 0.9283 - val_loss: 0.0789 - val_r_square: 0.9217\n",
            "Epoch 1296/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0436 - r_square: 0.9276\n",
            "Epoch 1296: val_loss did not improve from 0.07882\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0438 - r_square: 0.9282 - val_loss: 0.0792 - val_r_square: 0.9216\n",
            "Epoch 1297/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0433 - r_square: 0.9276\n",
            "Epoch 1297: val_loss did not improve from 0.07882\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0440 - r_square: 0.9281 - val_loss: 0.0793 - val_r_square: 0.9216\n",
            "Epoch 1298/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0439 - r_square: 0.9289\n",
            "Epoch 1298: val_loss did not improve from 0.07882\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0438 - r_square: 0.9291 - val_loss: 0.0791 - val_r_square: 0.9218\n",
            "Epoch 1299/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0443 - r_square: 0.9289\n",
            "Epoch 1299: val_loss did not improve from 0.07882\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0439 - r_square: 0.9280 - val_loss: 0.0793 - val_r_square: 0.9216\n",
            "Epoch 1300/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0438 - r_square: 0.9286\n",
            "Epoch 1300: val_loss did not improve from 0.07882\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0438 - r_square: 0.9281 - val_loss: 0.0790 - val_r_square: 0.9217\n",
            "Epoch 1301/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0440 - r_square: 0.9253\n",
            "Epoch 1301: val_loss improved from 0.07882 to 0.07876, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0439 - r_square: 0.9279 - val_loss: 0.0788 - val_r_square: 0.9217\n",
            "Epoch 1302/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0438 - r_square: 0.9306\n",
            "Epoch 1302: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.0440 - r_square: 0.9280 - val_loss: 0.0790 - val_r_square: 0.9214\n",
            "Epoch 1303/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0441 - r_square: 0.9266\n",
            "Epoch 1303: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0438 - r_square: 0.9283 - val_loss: 0.0791 - val_r_square: 0.9219\n",
            "Epoch 1304/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0443 - r_square: 0.9252\n",
            "Epoch 1304: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0437 - r_square: 0.9280 - val_loss: 0.0790 - val_r_square: 0.9215\n",
            "Epoch 1305/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0440 - r_square: 0.9274\n",
            "Epoch 1305: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0437 - r_square: 0.9284 - val_loss: 0.0789 - val_r_square: 0.9218\n",
            "Epoch 1306/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0439 - r_square: 0.9281\n",
            "Epoch 1306: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0437 - r_square: 0.9281 - val_loss: 0.0789 - val_r_square: 0.9216\n",
            "Epoch 1307/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0437 - r_square: 0.9298\n",
            "Epoch 1307: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0437 - r_square: 0.9281 - val_loss: 0.0794 - val_r_square: 0.9213\n",
            "Epoch 1308/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0437 - r_square: 0.9309\n",
            "Epoch 1308: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0438 - r_square: 0.9284 - val_loss: 0.0788 - val_r_square: 0.9216\n",
            "Epoch 1309/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0447 - r_square: 0.9251\n",
            "Epoch 1309: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0437 - r_square: 0.9284 - val_loss: 0.0792 - val_r_square: 0.9216\n",
            "Epoch 1310/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0437 - r_square: 0.9284\n",
            "Epoch 1310: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0437 - r_square: 0.9284 - val_loss: 0.0792 - val_r_square: 0.9215\n",
            "Epoch 1311/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0436 - r_square: 0.9285\n",
            "Epoch 1311: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0436 - r_square: 0.9282 - val_loss: 0.0791 - val_r_square: 0.9217\n",
            "Epoch 1312/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0434 - r_square: 0.9283\n",
            "Epoch 1312: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0437 - r_square: 0.9286 - val_loss: 0.0794 - val_r_square: 0.9216\n",
            "Epoch 1313/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0443 - r_square: 0.9271\n",
            "Epoch 1313: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.0439 - r_square: 0.9280 - val_loss: 0.0789 - val_r_square: 0.9216\n",
            "Epoch 1314/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0437 - r_square: 0.9280\n",
            "Epoch 1314: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0437 - r_square: 0.9280 - val_loss: 0.0790 - val_r_square: 0.9216\n",
            "Epoch 1315/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0429 - r_square: 0.9307\n",
            "Epoch 1315: val_loss did not improve from 0.07876\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0436 - r_square: 0.9283 - val_loss: 0.0796 - val_r_square: 0.9214\n",
            "Epoch 1316/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0433 - r_square: 0.9279\n",
            "Epoch 1316: val_loss improved from 0.07876 to 0.07860, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0438 - r_square: 0.9286 - val_loss: 0.0786 - val_r_square: 0.9216\n",
            "Epoch 1317/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0438 - r_square: 0.9348\n",
            "Epoch 1317: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 12ms/step - loss: 0.0436 - r_square: 0.9281 - val_loss: 0.0789 - val_r_square: 0.9215\n",
            "Epoch 1318/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0436 - r_square: 0.9277\n",
            "Epoch 1318: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0435 - r_square: 0.9288 - val_loss: 0.0790 - val_r_square: 0.9213\n",
            "Epoch 1319/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0432 - r_square: 0.9285\n",
            "Epoch 1319: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 14ms/step - loss: 0.0435 - r_square: 0.9285 - val_loss: 0.0790 - val_r_square: 0.9215\n",
            "Epoch 1320/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0432 - r_square: 0.9286\n",
            "Epoch 1320: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0436 - r_square: 0.9283 - val_loss: 0.0786 - val_r_square: 0.9217\n",
            "Epoch 1321/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0434 - r_square: 0.9287\n",
            "Epoch 1321: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0435 - r_square: 0.9285 - val_loss: 0.0789 - val_r_square: 0.9216\n",
            "Epoch 1322/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0438 - r_square: 0.9265\n",
            "Epoch 1322: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0438 - r_square: 0.9283 - val_loss: 0.0787 - val_r_square: 0.9218\n",
            "Epoch 1323/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0431 - r_square: 0.9282\n",
            "Epoch 1323: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0435 - r_square: 0.9283 - val_loss: 0.0787 - val_r_square: 0.9215\n",
            "Epoch 1324/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0438 - r_square: 0.9270\n",
            "Epoch 1324: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0438 - r_square: 0.9283 - val_loss: 0.0792 - val_r_square: 0.9213\n",
            "Epoch 1325/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0429 - r_square: 0.9300\n",
            "Epoch 1325: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0434 - r_square: 0.9287 - val_loss: 0.0793 - val_r_square: 0.9215\n",
            "Epoch 1326/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0436 - r_square: 0.9306\n",
            "Epoch 1326: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0435 - r_square: 0.9288 - val_loss: 0.0789 - val_r_square: 0.9216\n",
            "Epoch 1327/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0436 - r_square: 0.9318\n",
            "Epoch 1327: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0437 - r_square: 0.9283 - val_loss: 0.0788 - val_r_square: 0.9216\n",
            "Epoch 1328/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0434 - r_square: 0.9310\n",
            "Epoch 1328: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0438 - r_square: 0.9285 - val_loss: 0.0787 - val_r_square: 0.9215\n",
            "Epoch 1329/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0436 - r_square: 0.9302\n",
            "Epoch 1329: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0436 - r_square: 0.9287 - val_loss: 0.0792 - val_r_square: 0.9216\n",
            "Epoch 1330/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0428 - r_square: 0.9280\n",
            "Epoch 1330: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0435 - r_square: 0.9284 - val_loss: 0.0788 - val_r_square: 0.9216\n",
            "Epoch 1331/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0436 - r_square: 0.9260\n",
            "Epoch 1331: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0434 - r_square: 0.9281 - val_loss: 0.0787 - val_r_square: 0.9216\n",
            "Epoch 1332/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0439 - r_square: 0.9274\n",
            "Epoch 1332: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0434 - r_square: 0.9285 - val_loss: 0.0788 - val_r_square: 0.9216\n",
            "Epoch 1333/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0433 - r_square: 0.9306\n",
            "Epoch 1333: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0435 - r_square: 0.9286 - val_loss: 0.0789 - val_r_square: 0.9215\n",
            "Epoch 1334/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0435 - r_square: 0.9305\n",
            "Epoch 1334: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0434 - r_square: 0.9285 - val_loss: 0.0789 - val_r_square: 0.9215\n",
            "Epoch 1335/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0432 - r_square: 0.9305\n",
            "Epoch 1335: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0436 - r_square: 0.9288 - val_loss: 0.0788 - val_r_square: 0.9217\n",
            "Epoch 1336/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0438 - r_square: 0.9275\n",
            "Epoch 1336: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0435 - r_square: 0.9288 - val_loss: 0.0788 - val_r_square: 0.9216\n",
            "Epoch 1337/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0433 - r_square: 0.9285\n",
            "Epoch 1337: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0434 - r_square: 0.9288 - val_loss: 0.0790 - val_r_square: 0.9216\n",
            "Epoch 1338/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0429 - r_square: 0.9272\n",
            "Epoch 1338: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0433 - r_square: 0.9284 - val_loss: 0.0789 - val_r_square: 0.9217\n",
            "Epoch 1339/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0432 - r_square: 0.9296\n",
            "Epoch 1339: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0434 - r_square: 0.9290 - val_loss: 0.0794 - val_r_square: 0.9215\n",
            "Epoch 1340/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0436 - r_square: 0.9289\n",
            "Epoch 1340: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0436 - r_square: 0.9287 - val_loss: 0.0790 - val_r_square: 0.9213\n",
            "Epoch 1341/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0434 - r_square: 0.9288\n",
            "Epoch 1341: val_loss did not improve from 0.07860\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0434 - r_square: 0.9288 - val_loss: 0.0790 - val_r_square: 0.9216\n",
            "Epoch 1342/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0437 - r_square: 0.9289\n",
            "Epoch 1342: val_loss improved from 0.07860 to 0.07856, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0434 - r_square: 0.9289 - val_loss: 0.0786 - val_r_square: 0.9217\n",
            "Epoch 1343/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0428 - r_square: 0.9280\n",
            "Epoch 1343: val_loss did not improve from 0.07856\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0434 - r_square: 0.9290 - val_loss: 0.0790 - val_r_square: 0.9217\n",
            "Epoch 1344/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0437 - r_square: 0.9295\n",
            "Epoch 1344: val_loss did not improve from 0.07856\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0432 - r_square: 0.9289 - val_loss: 0.0787 - val_r_square: 0.9215\n",
            "Epoch 1345/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0439 - r_square: 0.9335\n",
            "Epoch 1345: val_loss did not improve from 0.07856\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0435 - r_square: 0.9288 - val_loss: 0.0787 - val_r_square: 0.9214\n",
            "Epoch 1346/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0432 - r_square: 0.9290\n",
            "Epoch 1346: val_loss improved from 0.07856 to 0.07854, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0432 - r_square: 0.9290 - val_loss: 0.0785 - val_r_square: 0.9216\n",
            "Epoch 1347/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0431 - r_square: 0.9282\n",
            "Epoch 1347: val_loss improved from 0.07854 to 0.07833, saving model to best_model.h5\n",
            "67/67 [==============================] - 1s 13ms/step - loss: 0.0433 - r_square: 0.9289 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1348/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0433 - r_square: 0.9285\n",
            "Epoch 1348: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0432 - r_square: 0.9287 - val_loss: 0.0787 - val_r_square: 0.9214\n",
            "Epoch 1349/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0433 - r_square: 0.9285\n",
            "Epoch 1349: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 1s 11ms/step - loss: 0.0431 - r_square: 0.9286 - val_loss: 0.0787 - val_r_square: 0.9219\n",
            "Epoch 1350/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0431 - r_square: 0.9288\n",
            "Epoch 1350: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0433 - r_square: 0.9286 - val_loss: 0.0787 - val_r_square: 0.9218\n",
            "Epoch 1351/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0434 - r_square: 0.9292\n",
            "Epoch 1351: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 1s 10ms/step - loss: 0.0434 - r_square: 0.9287 - val_loss: 0.0793 - val_r_square: 0.9215\n",
            "Epoch 1352/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0435 - r_square: 0.9285\n",
            "Epoch 1352: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0433 - r_square: 0.9289 - val_loss: 0.0786 - val_r_square: 0.9216\n",
            "Epoch 1353/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0431 - r_square: 0.9275\n",
            "Epoch 1353: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0431 - r_square: 0.9293 - val_loss: 0.0790 - val_r_square: 0.9215\n",
            "Epoch 1354/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0434 - r_square: 0.9259\n",
            "Epoch 1354: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0431 - r_square: 0.9290 - val_loss: 0.0788 - val_r_square: 0.9216\n",
            "Epoch 1355/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0434 - r_square: 0.9274\n",
            "Epoch 1355: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0431 - r_square: 0.9289 - val_loss: 0.0785 - val_r_square: 0.9216\n",
            "Epoch 1356/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0430 - r_square: 0.9267\n",
            "Epoch 1356: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0431 - r_square: 0.9290 - val_loss: 0.0787 - val_r_square: 0.9214\n",
            "Epoch 1357/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0433 - r_square: 0.9268\n",
            "Epoch 1357: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0431 - r_square: 0.9291 - val_loss: 0.0788 - val_r_square: 0.9216\n",
            "Epoch 1358/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0426 - r_square: 0.9286\n",
            "Epoch 1358: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0430 - r_square: 0.9288 - val_loss: 0.0785 - val_r_square: 0.9216\n",
            "Epoch 1359/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0432 - r_square: 0.9285\n",
            "Epoch 1359: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0431 - r_square: 0.9291 - val_loss: 0.0786 - val_r_square: 0.9216\n",
            "Epoch 1360/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0433 - r_square: 0.9279\n",
            "Epoch 1360: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0430 - r_square: 0.9291 - val_loss: 0.0787 - val_r_square: 0.9214\n",
            "Epoch 1361/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0434 - r_square: 0.9291\n",
            "Epoch 1361: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0432 - r_square: 0.9288 - val_loss: 0.0792 - val_r_square: 0.9213\n",
            "Epoch 1362/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0436 - r_square: 0.9270\n",
            "Epoch 1362: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0430 - r_square: 0.9290 - val_loss: 0.0789 - val_r_square: 0.9214\n",
            "Epoch 1363/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0437 - r_square: 0.9309\n",
            "Epoch 1363: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0431 - r_square: 0.9290 - val_loss: 0.0790 - val_r_square: 0.9215\n",
            "Epoch 1364/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0433 - r_square: 0.9290\n",
            "Epoch 1364: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0430 - r_square: 0.9291 - val_loss: 0.0784 - val_r_square: 0.9216\n",
            "Epoch 1365/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0427 - r_square: 0.9273\n",
            "Epoch 1365: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0431 - r_square: 0.9293 - val_loss: 0.0784 - val_r_square: 0.9214\n",
            "Epoch 1366/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0423 - r_square: 0.9282\n",
            "Epoch 1366: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0431 - r_square: 0.9289 - val_loss: 0.0794 - val_r_square: 0.9207\n",
            "Epoch 1367/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0426 - r_square: 0.9294\n",
            "Epoch 1367: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0431 - r_square: 0.9291 - val_loss: 0.0788 - val_r_square: 0.9216\n",
            "Epoch 1368/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0431 - r_square: 0.9273\n",
            "Epoch 1368: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0429 - r_square: 0.9295 - val_loss: 0.0789 - val_r_square: 0.9216\n",
            "Epoch 1369/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0433 - r_square: 0.9254\n",
            "Epoch 1369: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0430 - r_square: 0.9291 - val_loss: 0.0784 - val_r_square: 0.9216\n",
            "Epoch 1370/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0431 - r_square: 0.9288\n",
            "Epoch 1370: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0429 - r_square: 0.9291 - val_loss: 0.0788 - val_r_square: 0.9217\n",
            "Epoch 1371/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0428 - r_square: 0.9299\n",
            "Epoch 1371: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0429 - r_square: 0.9291 - val_loss: 0.0785 - val_r_square: 0.9215\n",
            "Epoch 1372/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0429 - r_square: 0.9295\n",
            "Epoch 1372: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0429 - r_square: 0.9295 - val_loss: 0.0785 - val_r_square: 0.9212\n",
            "Epoch 1373/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0424 - r_square: 0.9282\n",
            "Epoch 1373: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0431 - r_square: 0.9289 - val_loss: 0.0786 - val_r_square: 0.9215\n",
            "Epoch 1374/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0427 - r_square: 0.9295\n",
            "Epoch 1374: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0427 - r_square: 0.9295 - val_loss: 0.0785 - val_r_square: 0.9217\n",
            "Epoch 1375/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0426 - r_square: 0.9349\n",
            "Epoch 1375: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0429 - r_square: 0.9293 - val_loss: 0.0785 - val_r_square: 0.9215\n",
            "Epoch 1376/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0426 - r_square: 0.9272\n",
            "Epoch 1376: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0428 - r_square: 0.9292 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1377/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0428 - r_square: 0.9281\n",
            "Epoch 1377: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0429 - r_square: 0.9291 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1378/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0424 - r_square: 0.9325\n",
            "Epoch 1378: val_loss did not improve from 0.07833\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0429 - r_square: 0.9293 - val_loss: 0.0792 - val_r_square: 0.9215\n",
            "Epoch 1379/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0418 - r_square: 0.9280\n",
            "Epoch 1379: val_loss improved from 0.07833 to 0.07826, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0428 - r_square: 0.9295 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1380/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0428 - r_square: 0.9319\n",
            "Epoch 1380: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0430 - r_square: 0.9287 - val_loss: 0.0784 - val_r_square: 0.9216\n",
            "Epoch 1381/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0431 - r_square: 0.9271\n",
            "Epoch 1381: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0430 - r_square: 0.9292 - val_loss: 0.0785 - val_r_square: 0.9215\n",
            "Epoch 1382/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0447 - r_square: 0.9295\n",
            "Epoch 1382: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0430 - r_square: 0.9289 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1383/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0433 - r_square: 0.9288\n",
            "Epoch 1383: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0430 - r_square: 0.9296 - val_loss: 0.0783 - val_r_square: 0.9217\n",
            "Epoch 1384/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0430 - r_square: 0.9259\n",
            "Epoch 1384: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 1s 9ms/step - loss: 0.0427 - r_square: 0.9293 - val_loss: 0.0786 - val_r_square: 0.9214\n",
            "Epoch 1385/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0426 - r_square: 0.9298\n",
            "Epoch 1385: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0426 - r_square: 0.9296 - val_loss: 0.0791 - val_r_square: 0.9217\n",
            "Epoch 1386/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0422 - r_square: 0.9290\n",
            "Epoch 1386: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0427 - r_square: 0.9296 - val_loss: 0.0789 - val_r_square: 0.9215\n",
            "Epoch 1387/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0428 - r_square: 0.9296\n",
            "Epoch 1387: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0428 - r_square: 0.9296 - val_loss: 0.0788 - val_r_square: 0.9214\n",
            "Epoch 1388/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0428 - r_square: 0.9266\n",
            "Epoch 1388: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0427 - r_square: 0.9290 - val_loss: 0.0785 - val_r_square: 0.9216\n",
            "Epoch 1389/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0418 - r_square: 0.9331\n",
            "Epoch 1389: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0427 - r_square: 0.9294 - val_loss: 0.0785 - val_r_square: 0.9216\n",
            "Epoch 1390/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0429 - r_square: 0.9293\n",
            "Epoch 1390: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 1s 8ms/step - loss: 0.0427 - r_square: 0.9295 - val_loss: 0.0788 - val_r_square: 0.9217\n",
            "Epoch 1391/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0423 - r_square: 0.9321\n",
            "Epoch 1391: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0428 - r_square: 0.9295 - val_loss: 0.0789 - val_r_square: 0.9214\n",
            "Epoch 1392/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0428 - r_square: 0.9272\n",
            "Epoch 1392: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0428 - r_square: 0.9296 - val_loss: 0.0784 - val_r_square: 0.9216\n",
            "Epoch 1393/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0434 - r_square: 0.9329\n",
            "Epoch 1393: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0426 - r_square: 0.9294 - val_loss: 0.0784 - val_r_square: 0.9216\n",
            "Epoch 1394/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0423 - r_square: 0.9303\n",
            "Epoch 1394: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0427 - r_square: 0.9294 - val_loss: 0.0787 - val_r_square: 0.9216\n",
            "Epoch 1395/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0432 - r_square: 0.9296\n",
            "Epoch 1395: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0427 - r_square: 0.9297 - val_loss: 0.0785 - val_r_square: 0.9216\n",
            "Epoch 1396/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0431 - r_square: 0.9271\n",
            "Epoch 1396: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0427 - r_square: 0.9291 - val_loss: 0.0787 - val_r_square: 0.9215\n",
            "Epoch 1397/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0428 - r_square: 0.9367\n",
            "Epoch 1397: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0429 - r_square: 0.9294 - val_loss: 0.0783 - val_r_square: 0.9218\n",
            "Epoch 1398/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0434 - r_square: 0.9272\n",
            "Epoch 1398: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0427 - r_square: 0.9293 - val_loss: 0.0784 - val_r_square: 0.9216\n",
            "Epoch 1399/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0430 - r_square: 0.9301\n",
            "Epoch 1399: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0427 - r_square: 0.9292 - val_loss: 0.0786 - val_r_square: 0.9216\n",
            "Epoch 1400/1500\n",
            "56/67 [========================>.....] - ETA: 0s - loss: 0.0433 - r_square: 0.9279\n",
            "Epoch 1400: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0426 - r_square: 0.9293 - val_loss: 0.0787 - val_r_square: 0.9218\n",
            "Epoch 1401/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0418 - r_square: 0.9291\n",
            "Epoch 1401: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0426 - r_square: 0.9294 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1402/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0425 - r_square: 0.9283\n",
            "Epoch 1402: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0427 - r_square: 0.9294 - val_loss: 0.0786 - val_r_square: 0.9214\n",
            "Epoch 1403/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0431 - r_square: 0.9288\n",
            "Epoch 1403: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0426 - r_square: 0.9298 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1404/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0424 - r_square: 0.9304\n",
            "Epoch 1404: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0426 - r_square: 0.9293 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1405/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0420 - r_square: 0.9280\n",
            "Epoch 1405: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0425 - r_square: 0.9294 - val_loss: 0.0788 - val_r_square: 0.9214\n",
            "Epoch 1406/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0435 - r_square: 0.9250\n",
            "Epoch 1406: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0426 - r_square: 0.9297 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1407/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0424 - r_square: 0.9297\n",
            "Epoch 1407: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0424 - r_square: 0.9297 - val_loss: 0.0802 - val_r_square: 0.9208\n",
            "Epoch 1408/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0411 - r_square: 0.9352\n",
            "Epoch 1408: val_loss did not improve from 0.07826\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0426 - r_square: 0.9294 - val_loss: 0.0786 - val_r_square: 0.9215\n",
            "Epoch 1409/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0433 - r_square: 0.9250\n",
            "Epoch 1409: val_loss improved from 0.07826 to 0.07815, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0425 - r_square: 0.9300 - val_loss: 0.0781 - val_r_square: 0.9215\n",
            "Epoch 1410/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0426 - r_square: 0.9288\n",
            "Epoch 1410: val_loss did not improve from 0.07815\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0428 - r_square: 0.9292 - val_loss: 0.0788 - val_r_square: 0.9213\n",
            "Epoch 1411/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0423 - r_square: 0.9285\n",
            "Epoch 1411: val_loss did not improve from 0.07815\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0429 - r_square: 0.9294 - val_loss: 0.0805 - val_r_square: 0.9205\n",
            "Epoch 1412/1500\n",
            "49/67 [====================>.........] - ETA: 0s - loss: 0.0439 - r_square: 0.9201\n",
            "Epoch 1412: val_loss did not improve from 0.07815\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0428 - r_square: 0.9293 - val_loss: 0.0790 - val_r_square: 0.9211\n",
            "Epoch 1413/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0422 - r_square: 0.9292\n",
            "Epoch 1413: val_loss did not improve from 0.07815\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0423 - r_square: 0.9297 - val_loss: 0.0785 - val_r_square: 0.9215\n",
            "Epoch 1414/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0419 - r_square: 0.9331\n",
            "Epoch 1414: val_loss improved from 0.07815 to 0.07805, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0426 - r_square: 0.9298 - val_loss: 0.0781 - val_r_square: 0.9216\n",
            "Epoch 1415/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0425 - r_square: 0.9292\n",
            "Epoch 1415: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0422 - r_square: 0.9296 - val_loss: 0.0788 - val_r_square: 0.9215\n",
            "Epoch 1416/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0426 - r_square: 0.9292\n",
            "Epoch 1416: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0425 - r_square: 0.9298 - val_loss: 0.0781 - val_r_square: 0.9216\n",
            "Epoch 1417/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0427 - r_square: 0.9280\n",
            "Epoch 1417: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0425 - r_square: 0.9294 - val_loss: 0.0786 - val_r_square: 0.9216\n",
            "Epoch 1418/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0419 - r_square: 0.9346\n",
            "Epoch 1418: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0424 - r_square: 0.9297 - val_loss: 0.0782 - val_r_square: 0.9215\n",
            "Epoch 1419/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0420 - r_square: 0.9290\n",
            "Epoch 1419: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0424 - r_square: 0.9297 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1420/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0424 - r_square: 0.9286\n",
            "Epoch 1420: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0424 - r_square: 0.9291 - val_loss: 0.0790 - val_r_square: 0.9214\n",
            "Epoch 1421/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0423 - r_square: 0.9298\n",
            "Epoch 1421: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0423 - r_square: 0.9298 - val_loss: 0.0786 - val_r_square: 0.9217\n",
            "Epoch 1422/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0425 - r_square: 0.9298\n",
            "Epoch 1422: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0425 - r_square: 0.9300 - val_loss: 0.0785 - val_r_square: 0.9215\n",
            "Epoch 1423/1500\n",
            "50/67 [=====================>........] - ETA: 0s - loss: 0.0414 - r_square: 0.9305\n",
            "Epoch 1423: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0424 - r_square: 0.9298 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1424/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0422 - r_square: 0.9323\n",
            "Epoch 1424: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0422 - r_square: 0.9301 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1425/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0421 - r_square: 0.9289\n",
            "Epoch 1425: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0423 - r_square: 0.9297 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1426/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0417 - r_square: 0.9312\n",
            "Epoch 1426: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0422 - r_square: 0.9299 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1427/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0422 - r_square: 0.9287\n",
            "Epoch 1427: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0421 - r_square: 0.9301 - val_loss: 0.0787 - val_r_square: 0.9211\n",
            "Epoch 1428/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0414 - r_square: 0.9296\n",
            "Epoch 1428: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0422 - r_square: 0.9302 - val_loss: 0.0786 - val_r_square: 0.9216\n",
            "Epoch 1429/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0424 - r_square: 0.9271\n",
            "Epoch 1429: val_loss did not improve from 0.07805\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0422 - r_square: 0.9300 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1430/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0424 - r_square: 0.9277\n",
            "Epoch 1430: val_loss improved from 0.07805 to 0.07804, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0422 - r_square: 0.9299 - val_loss: 0.0780 - val_r_square: 0.9217\n",
            "Epoch 1431/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0418 - r_square: 0.9296\n",
            "Epoch 1431: val_loss did not improve from 0.07804\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0421 - r_square: 0.9300 - val_loss: 0.0784 - val_r_square: 0.9216\n",
            "Epoch 1432/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0426 - r_square: 0.9295\n",
            "Epoch 1432: val_loss did not improve from 0.07804\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0421 - r_square: 0.9300 - val_loss: 0.0786 - val_r_square: 0.9212\n",
            "Epoch 1433/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0424 - r_square: 0.9294\n",
            "Epoch 1433: val_loss did not improve from 0.07804\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0424 - r_square: 0.9295 - val_loss: 0.0783 - val_r_square: 0.9214\n",
            "Epoch 1434/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0423 - r_square: 0.9303\n",
            "Epoch 1434: val_loss did not improve from 0.07804\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0424 - r_square: 0.9300 - val_loss: 0.0782 - val_r_square: 0.9217\n",
            "Epoch 1435/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0423 - r_square: 0.9295\n",
            "Epoch 1435: val_loss improved from 0.07804 to 0.07800, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0422 - r_square: 0.9300 - val_loss: 0.0780 - val_r_square: 0.9216\n",
            "Epoch 1436/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0415 - r_square: 0.9298\n",
            "Epoch 1436: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0422 - r_square: 0.9297 - val_loss: 0.0781 - val_r_square: 0.9216\n",
            "Epoch 1437/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0427 - r_square: 0.9297\n",
            "Epoch 1437: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0420 - r_square: 0.9301 - val_loss: 0.0782 - val_r_square: 0.9214\n",
            "Epoch 1438/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0414 - r_square: 0.9298\n",
            "Epoch 1438: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0421 - r_square: 0.9305 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1439/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0419 - r_square: 0.9297\n",
            "Epoch 1439: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0421 - r_square: 0.9299 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1440/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0423 - r_square: 0.9286\n",
            "Epoch 1440: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0420 - r_square: 0.9303 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1441/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0423 - r_square: 0.9294\n",
            "Epoch 1441: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0421 - r_square: 0.9304 - val_loss: 0.0782 - val_r_square: 0.9215\n",
            "Epoch 1442/1500\n",
            "57/67 [========================>.....] - ETA: 0s - loss: 0.0420 - r_square: 0.9351\n",
            "Epoch 1442: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0421 - r_square: 0.9298 - val_loss: 0.0782 - val_r_square: 0.9216\n",
            "Epoch 1443/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0428 - r_square: 0.9276\n",
            "Epoch 1443: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0419 - r_square: 0.9301 - val_loss: 0.0781 - val_r_square: 0.9216\n",
            "Epoch 1444/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0421 - r_square: 0.9300\n",
            "Epoch 1444: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0420 - r_square: 0.9300 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1445/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0425 - r_square: 0.9297\n",
            "Epoch 1445: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0420 - r_square: 0.9299 - val_loss: 0.0786 - val_r_square: 0.9217\n",
            "Epoch 1446/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0421 - r_square: 0.9273\n",
            "Epoch 1446: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0421 - r_square: 0.9299 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1447/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0419 - r_square: 0.9301\n",
            "Epoch 1447: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0419 - r_square: 0.9301 - val_loss: 0.0780 - val_r_square: 0.9215\n",
            "Epoch 1448/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0415 - r_square: 0.9315\n",
            "Epoch 1448: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0420 - r_square: 0.9302 - val_loss: 0.0785 - val_r_square: 0.9216\n",
            "Epoch 1449/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0420 - r_square: 0.9303\n",
            "Epoch 1449: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0421 - r_square: 0.9306 - val_loss: 0.0793 - val_r_square: 0.9213\n",
            "Epoch 1450/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0415 - r_square: 0.9233\n",
            "Epoch 1450: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0420 - r_square: 0.9299 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1451/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0423 - r_square: 0.9291\n",
            "Epoch 1451: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0421 - r_square: 0.9298 - val_loss: 0.0787 - val_r_square: 0.9214\n",
            "Epoch 1452/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0424 - r_square: 0.9270\n",
            "Epoch 1452: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0421 - r_square: 0.9295 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1453/1500\n",
            "62/67 [==========================>...] - ETA: 0s - loss: 0.0416 - r_square: 0.9304\n",
            "Epoch 1453: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0422 - r_square: 0.9302 - val_loss: 0.0783 - val_r_square: 0.9217\n",
            "Epoch 1454/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0418 - r_square: 0.9289\n",
            "Epoch 1454: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0419 - r_square: 0.9305 - val_loss: 0.0795 - val_r_square: 0.9211\n",
            "Epoch 1455/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0416 - r_square: 0.9303\n",
            "Epoch 1455: val_loss did not improve from 0.07800\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0420 - r_square: 0.9299 - val_loss: 0.0785 - val_r_square: 0.9215\n",
            "Epoch 1456/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0421 - r_square: 0.9306\n",
            "Epoch 1456: val_loss improved from 0.07800 to 0.07791, saving model to best_model.h5\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0420 - r_square: 0.9302 - val_loss: 0.0779 - val_r_square: 0.9217\n",
            "Epoch 1457/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0420 - r_square: 0.9298\n",
            "Epoch 1457: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0420 - r_square: 0.9298 - val_loss: 0.0790 - val_r_square: 0.9212\n",
            "Epoch 1458/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0426 - r_square: 0.9272\n",
            "Epoch 1458: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0421 - r_square: 0.9303 - val_loss: 0.0782 - val_r_square: 0.9216\n",
            "Epoch 1459/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0433 - r_square: 0.9466\n",
            "Epoch 1459: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0422 - r_square: 0.9308 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1460/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0421 - r_square: 0.9223\n",
            "Epoch 1460: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0419 - r_square: 0.9302 - val_loss: 0.0785 - val_r_square: 0.9217\n",
            "Epoch 1461/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0416 - r_square: 0.9307\n",
            "Epoch 1461: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0418 - r_square: 0.9304 - val_loss: 0.0782 - val_r_square: 0.9217\n",
            "Epoch 1462/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0420 - r_square: 0.9302\n",
            "Epoch 1462: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0420 - r_square: 0.9302 - val_loss: 0.0785 - val_r_square: 0.9215\n",
            "Epoch 1463/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0443 - r_square: 0.9303\n",
            "Epoch 1463: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0422 - r_square: 0.9303 - val_loss: 0.0790 - val_r_square: 0.9216\n",
            "Epoch 1464/1500\n",
            "58/67 [========================>.....] - ETA: 0s - loss: 0.0416 - r_square: 0.9334\n",
            "Epoch 1464: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0420 - r_square: 0.9301 - val_loss: 0.0794 - val_r_square: 0.9210\n",
            "Epoch 1465/1500\n",
            "55/67 [=======================>......] - ETA: 0s - loss: 0.0421 - r_square: 0.9252\n",
            "Epoch 1465: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0418 - r_square: 0.9304 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1466/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0413 - r_square: 0.9412\n",
            "Epoch 1466: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0418 - r_square: 0.9303 - val_loss: 0.0782 - val_r_square: 0.9216\n",
            "Epoch 1467/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0413 - r_square: 0.9307\n",
            "Epoch 1467: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0417 - r_square: 0.9305 - val_loss: 0.0782 - val_r_square: 0.9216\n",
            "Epoch 1468/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0422 - r_square: 0.9334\n",
            "Epoch 1468: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0420 - r_square: 0.9301 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1469/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0419 - r_square: 0.9310\n",
            "Epoch 1469: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0420 - r_square: 0.9305 - val_loss: 0.0781 - val_r_square: 0.9215\n",
            "Epoch 1470/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0429 - r_square: 0.9338\n",
            "Epoch 1470: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0418 - r_square: 0.9303 - val_loss: 0.0785 - val_r_square: 0.9214\n",
            "Epoch 1471/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0419 - r_square: 0.9351\n",
            "Epoch 1471: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0419 - r_square: 0.9306 - val_loss: 0.0779 - val_r_square: 0.9217\n",
            "Epoch 1472/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0422 - r_square: 0.9286\n",
            "Epoch 1472: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0416 - r_square: 0.9305 - val_loss: 0.0782 - val_r_square: 0.9217\n",
            "Epoch 1473/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0402 - r_square: 0.9321\n",
            "Epoch 1473: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0416 - r_square: 0.9308 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1474/1500\n",
            "51/67 [=====================>........] - ETA: 0s - loss: 0.0421 - r_square: 0.9340\n",
            "Epoch 1474: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0418 - r_square: 0.9304 - val_loss: 0.0786 - val_r_square: 0.9215\n",
            "Epoch 1475/1500\n",
            "64/67 [===========================>..] - ETA: 0s - loss: 0.0413 - r_square: 0.9277\n",
            "Epoch 1475: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0418 - r_square: 0.9305 - val_loss: 0.0787 - val_r_square: 0.9213\n",
            "Epoch 1476/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0413 - r_square: 0.9291\n",
            "Epoch 1476: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0416 - r_square: 0.9305 - val_loss: 0.0782 - val_r_square: 0.9217\n",
            "Epoch 1477/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0418 - r_square: 0.9303\n",
            "Epoch 1477: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0417 - r_square: 0.9306 - val_loss: 0.0785 - val_r_square: 0.9213\n",
            "Epoch 1478/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0417 - r_square: 0.9305\n",
            "Epoch 1478: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0417 - r_square: 0.9305 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1479/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0417 - r_square: 0.9304\n",
            "Epoch 1479: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0417 - r_square: 0.9304 - val_loss: 0.0779 - val_r_square: 0.9215\n",
            "Epoch 1480/1500\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.0416 - r_square: 0.9305\n",
            "Epoch 1480: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0416 - r_square: 0.9305 - val_loss: 0.0790 - val_r_square: 0.9211\n",
            "Epoch 1481/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0413 - r_square: 0.9292\n",
            "Epoch 1481: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0417 - r_square: 0.9304 - val_loss: 0.0785 - val_r_square: 0.9214\n",
            "Epoch 1482/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0418 - r_square: 0.9304\n",
            "Epoch 1482: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0416 - r_square: 0.9304 - val_loss: 0.0783 - val_r_square: 0.9215\n",
            "Epoch 1483/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0419 - r_square: 0.9290\n",
            "Epoch 1483: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0416 - r_square: 0.9304 - val_loss: 0.0781 - val_r_square: 0.9218\n",
            "Epoch 1484/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0417 - r_square: 0.9304\n",
            "Epoch 1484: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0415 - r_square: 0.9307 - val_loss: 0.0789 - val_r_square: 0.9214\n",
            "Epoch 1485/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0414 - r_square: 0.9288\n",
            "Epoch 1485: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0415 - r_square: 0.9306 - val_loss: 0.0783 - val_r_square: 0.9217\n",
            "Epoch 1486/1500\n",
            "63/67 [===========================>..] - ETA: 0s - loss: 0.0418 - r_square: 0.9357\n",
            "Epoch 1486: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 7ms/step - loss: 0.0416 - r_square: 0.9309 - val_loss: 0.0781 - val_r_square: 0.9215\n",
            "Epoch 1487/1500\n",
            "60/67 [=========================>....] - ETA: 0s - loss: 0.0414 - r_square: 0.9307\n",
            "Epoch 1487: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0418 - r_square: 0.9305 - val_loss: 0.0785 - val_r_square: 0.9214\n",
            "Epoch 1488/1500\n",
            "61/67 [==========================>...] - ETA: 0s - loss: 0.0417 - r_square: 0.9307\n",
            "Epoch 1488: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0421 - r_square: 0.9303 - val_loss: 0.0780 - val_r_square: 0.9216\n",
            "Epoch 1489/1500\n",
            "52/67 [======================>.......] - ETA: 0s - loss: 0.0413 - r_square: 0.9325\n",
            "Epoch 1489: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0416 - r_square: 0.9308 - val_loss: 0.0781 - val_r_square: 0.9217\n",
            "Epoch 1490/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0417 - r_square: 0.9297\n",
            "Epoch 1490: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0416 - r_square: 0.9306 - val_loss: 0.0783 - val_r_square: 0.9217\n",
            "Epoch 1491/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0418 - r_square: 0.9305\n",
            "Epoch 1491: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0417 - r_square: 0.9309 - val_loss: 0.0781 - val_r_square: 0.9216\n",
            "Epoch 1492/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0418 - r_square: 0.9307\n",
            "Epoch 1492: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 6ms/step - loss: 0.0416 - r_square: 0.9310 - val_loss: 0.0798 - val_r_square: 0.9212\n",
            "Epoch 1493/1500\n",
            "53/67 [======================>.......] - ETA: 0s - loss: 0.0411 - r_square: 0.9302\n",
            "Epoch 1493: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0416 - r_square: 0.9307 - val_loss: 0.0790 - val_r_square: 0.9215\n",
            "Epoch 1494/1500\n",
            "59/67 [=========================>....] - ETA: 0s - loss: 0.0426 - r_square: 0.9328\n",
            "Epoch 1494: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0414 - r_square: 0.9308 - val_loss: 0.0788 - val_r_square: 0.9218\n",
            "Epoch 1495/1500\n",
            "65/67 [============================>.] - ETA: 0s - loss: 0.0416 - r_square: 0.9313\n",
            "Epoch 1495: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0416 - r_square: 0.9306 - val_loss: 0.0787 - val_r_square: 0.9217\n",
            "Epoch 1496/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0413 - r_square: 0.9345\n",
            "Epoch 1496: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0414 - r_square: 0.9308 - val_loss: 0.0783 - val_r_square: 0.9216\n",
            "Epoch 1497/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0406 - r_square: 0.9287\n",
            "Epoch 1497: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0416 - r_square: 0.9309 - val_loss: 0.0781 - val_r_square: 0.9216\n",
            "Epoch 1498/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0414 - r_square: 0.9307\n",
            "Epoch 1498: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0415 - r_square: 0.9307 - val_loss: 0.0781 - val_r_square: 0.9218\n",
            "Epoch 1499/1500\n",
            "66/67 [============================>.] - ETA: 0s - loss: 0.0411 - r_square: 0.9323\n",
            "Epoch 1499: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0414 - r_square: 0.9311 - val_loss: 0.0784 - val_r_square: 0.9215\n",
            "Epoch 1500/1500\n",
            "54/67 [=======================>......] - ETA: 0s - loss: 0.0414 - r_square: 0.9394\n",
            "Epoch 1500: val_loss did not improve from 0.07791\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0415 - r_square: 0.9308 - val_loss: 0.0786 - val_r_square: 0.9216\n"
          ]
        }
      ],
      "source": [
        "# fit the model!\n",
        "# attach it to a new variable called 'history' in case\n",
        "# to look at the learning curves\n",
        "history = model.fit(x_train, y_train,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[es,mc],\n",
        "                    epochs=1500,\n",
        "                    batch_size=64,\n",
        "                    verbose=1,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# let's see the training and validation accuracy by epoch\n",
        "history_dict = history.history\n",
        "loss_values = history_dict['loss'] # you can change this\n",
        "val_loss_values = history_dict['val_loss'] # you can also change this\n",
        "epochs = range(1, len(loss_values) + 1) # range of X (no. of epochs)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'orange', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# let's see the training and validation accuracy by epoch\n",
        "history_dict = history.history\n",
        "acc_values = history_dict['r_square'] # you can change this\n",
        "val_acc_values = history_dict['val_r_square'] # you can also change this\n",
        "epochs = range(1, len(acc_values) + 1) # range of X (no. of epochs)\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training r_square')\n",
        "plt.plot(epochs, val_acc_values, 'orange', label='Validation r_square')\n",
        "plt.title('Training and validation r_square')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('r_square')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "134/134 [==============================] - 0s 1ms/step\n",
            "58/58 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# scatterplot of actual vs. pred\n",
        "# specify the dimensions \n",
        "fig, axes = plt.subplots(1,2) # 1 row, 2 columns\n",
        "\n",
        "# this makes the individual subplots\n",
        "# Training Results\n",
        "axes[0].scatter(x=y_train, y=model.predict(x_train)) #first row, first entry (left top)\n",
        "axes[0].set_xlabel(\"Actual\", fontsize=10)\n",
        "axes[0].set_ylabel(\"Predicted\",  fontsize=10)\n",
        "axes[0].set_title(\"Training\")\n",
        "# add 45 deg line\n",
        "x = np.linspace(*axes[0].get_xlim())\n",
        "axes[0].plot(x, x, color='red')\n",
        "# Validation Results\n",
        "axes[1].scatter(x=y_test, y=model.predict(x_test)) # first row, second entry (right top)\n",
        "axes[1].set_xlabel(\"Actual\", fontsize=10)\n",
        "axes[1].set_ylabel(\"Predicted\",  fontsize=10)\n",
        "axes[1].set_title(\"Validation\")\n",
        "# add 45 deg line\n",
        "x = np.linspace(*axes[1].get_xlim())\n",
        "axes[1].plot(x, x, color='red')\n",
        "\n",
        "# tight layout\n",
        "fig.tight_layout()\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58/58 [==============================] - 0s 1ms/step\n",
            "134/134 [==============================] - 0s 1ms/step\n",
            "Training accuracy: 93.11338054425248%\n",
            "Test accuracy:     92.15892263951304%\n"
          ]
        }
      ],
      "source": [
        "# metrics\n",
        "pred = model.predict(x_test)\n",
        "trainpreds = model.predict(x_train)\n",
        "#score R2\n",
        "from sklearn.metrics import r2_score\n",
        "print(\"Training accuracy: \"+str(r2_score(y_train, trainpreds)*100)+\"%\") # train\n",
        "print(\"Test accuracy:     \"+str(r2_score(y_test, pred)*100)+\"%\") # test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
